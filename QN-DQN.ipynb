{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Q-Learning-Variables\" data-toc-modified-id=\"Q-Learning-Variables-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Q-Learning Variables</a></span><ul class=\"toc-item\"><li><span><a href=\"#Learning-Rate-(lr)---New-information-vs-old-knowledge?\" data-toc-modified-id=\"Learning-Rate-(lr)---New-information-vs-old-knowledge?-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Learning Rate (lr) - New information vs old knowledge?</a></span></li><li><span><a href=\"#Discount-Factor-(gamma)---long-term-vs.-short-term-gains\" data-toc-modified-id=\"Discount-Factor-(gamma)---long-term-vs.-short-term-gains-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Discount Factor (gamma) - long-term vs. short-term gains</a></span></li><li><span><a href=\"#Epsilon-(epsilon)\" data-toc-modified-id=\"Epsilon-(epsilon)-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Epsilon (epsilon)</a></span></li><li><span><a href=\"#np.argmax()\" data-toc-modified-id=\"np.argmax()-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>np.argmax()</a></span></li><li><span><a href=\"#Train-Agent\" data-toc-modified-id=\"Train-Agent-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Train Agent</a></span></li></ul></li><li><span><a href=\"#QLearningAgent-(Bare)\" data-toc-modified-id=\"QLearningAgent-(Bare)-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>QLearningAgent (Bare)</a></span></li><li><span><a href=\"#QLearningAgent-(Verbose)\" data-toc-modified-id=\"QLearningAgent-(Verbose)-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>QLearningAgent (Verbose)</a></span></li><li><span><a href=\"#QLearningAgent(Table-Verbose)\" data-toc-modified-id=\"QLearningAgent(Table-Verbose)-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>QLearningAgent(Table Verbose)</a></span></li><li><span><a href=\"#Key-Differences-QN-DQN\" data-toc-modified-id=\"Key-Differences-QN-DQN-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Key Differences QN-DQN</a></span><ul class=\"toc-item\"><li><span><a href=\"#Data-Storage\" data-toc-modified-id=\"Data-Storage-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Data Storage</a></span></li><li><span><a href=\"#Getting-Q-Values\" data-toc-modified-id=\"Getting-Q-Values-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Getting Q-Values</a></span></li><li><span><a href=\"#Learning/Updates\" data-toc-modified-id=\"Learning/Updates-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Learning/Updates</a></span></li><li><span><a href=\"#Memory-Requirements\" data-toc-modified-id=\"Memory-Requirements-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;</span>Memory Requirements</a></span></li><li><span><a href=\"#Training-Loop-Comparison\" data-toc-modified-id=\"Training-Loop-Comparison-5.5\"><span class=\"toc-item-num\">5.5&nbsp;&nbsp;</span>Training Loop Comparison</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/vigneshpalanivelr/MeachineLearningAI/blob/master/QN-DQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dvsu_QkOJ5Mw"
   },
   "source": [
    "# Q-Learning Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YAiGYhcExLI2"
   },
   "source": [
    "## Learning Rate (lr) - New information vs old knowledge?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6C4QLMgfvWzL"
   },
   "source": [
    "1.   **What it controls:** How much we update our Q-values with each new experience.\n",
    "2.   **Think of it as:** How much do you trust new information vs. old knowledge?\n",
    "\n",
    "**Examples:**\n",
    "\n",
    "0.   **lr = 0.01:** \"I'll barely change my belief, I trust my old knowledge more\"\n",
    "1.   **lr = 0.1:** \"I'll update my belief by 10% based on this new experience\"\n",
    "2.   **lr = 1.0:** \"I'll completely replace my old belief with this new experience\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T07:17:46.456274Z",
     "start_time": "2025-08-03T07:17:46.453335Z"
    },
    "id": "XMxUEF8_vS_d"
   },
   "outputs": [],
   "source": [
    "# Current Q-value for \"go right\" = 5.0\n",
    "# New experience suggests it should be 8.0\n",
    "# Target = 8.0, Current = 5.0, Difference = 3.0\n",
    "\n",
    "# With lr = 0.1:\n",
    "#new_q_value = 5.0 + 0.1 * (8.0 - 5.0) = 5.0 + 0.3 = 5.3\n",
    "\n",
    "# With lr = 0.5:\n",
    "#new_q_value = 5.0 + 0.5 * (8.0 - 5.0) = 5.0 + 1.5 = 6.5\n",
    "\n",
    "# With lr = 1.0:\n",
    "#new_q_value = 5.0 + 1.0 * (8.0 - 5.0) = 5.0 + 3.0 = 8.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qv_XiITZw8y3"
   },
   "source": [
    "## Discount Factor (gamma) - long-term vs. short-term gains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MqsPy2ynxToc"
   },
   "source": [
    "\n",
    "\n",
    "1.   **What it controls:** How much we value future rewards compared to immediate rewards.\n",
    "2.   **Think of it as:** How much do you care about long-term vs. short-term gains?\n",
    "3.   **Real-world analogy:** Would you rather have \\$10 now or $100  next year? Gamma represents your **\"patience level.\"**\n",
    "\n",
    "**Examples:**\n",
    "1.   **gamma = 0.95:** \"Future rewards are worth 95% of immediate rewards\"\n",
    "2.   **gamma = 0.0:** \"I only care about immediate rewards\" (greedy)\n",
    "3.   **gamma = 1.0:** \"Future rewards are just as valuable as immediate rewards\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T07:17:46.493343Z",
     "start_time": "2025-08-03T07:17:46.489178Z"
    },
    "id": "BZZ1mPV5zwe6"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "cannot assign to expression (2232520577.py, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[2], line 5\u001b[0;36m\u001b[0m\n\u001b[0;31m    target = 10 + 0.0 * 100 = 10  # Only immediate reward matters\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m cannot assign to expression\n"
     ]
    }
   ],
   "source": [
    "# Immediate reward = 10\n",
    "# Expected future reward = 100\n",
    "\n",
    "# With gamma = 0.0:\n",
    "target = 10 + 0.0 * 100 = 10  # Only immediate reward matters\n",
    "\n",
    "# With gamma = 0.5:\n",
    "target = 10 + 0.5 * 100 = 60  # Future reward discounted by 50%\n",
    "\n",
    "# With gamma = 0.95:\n",
    "target = 10 + 0.95 * 100 = 105  # Future reward almost fully valued"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J6aOzxzV2Hpi"
   },
   "source": [
    "## Epsilon (epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uHSwxUCV1cid"
   },
   "source": [
    "1.   **What it controls:** The exploration vs. exploitation trade-off during action selection.\n",
    "2.   **Think of it as:** How often should I try something new vs. stick with what I know works?\n",
    "\n",
    "**Examples:**\n",
    "1.   **epsilon = 0.1:** \"90% of the time, choose the best action; 10% of the time, explore randomly\"\n",
    "2.   **epsilon = 0.5:** \"50% exploration, 50% exploitation\"\n",
    "3.   **epsilon = 0.0:** \"Always choose the best known action\" (pure exploitation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T07:17:46.522143Z",
     "start_time": "2025-08-03T07:17:46.520412Z"
    },
    "id": "53RE2noF24S9"
   },
   "outputs": [],
   "source": [
    "# Current Q-values: [2.1, 8.5, 1.2, 0.9] for actions [Up, Right, Down, Left]\n",
    "# Best action is \"Right\" (index 1) with Q-value 8.5\n",
    "\n",
    "# With epsilon = 0.1:\n",
    "# 90% chance: Choose \"Right\"\n",
    "# 10% chance: Choose random action (Up, Right, Down, or Left)\n",
    "\n",
    "# With epsilon = 0.0:\n",
    "# 100% chance: Always choose \"Right\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LakIqoWdJn3w"
   },
   "source": [
    "## np.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T07:17:46.661638Z",
     "start_time": "2025-08-03T07:17:46.538124Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fzRKJBXw6VwC",
    "outputId": "c0744bda-d8c4-4eb4-88f9-802f330676c0",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üîç UNDERSTANDING np.argmax()\n",
      "============================================================\n",
      "\n",
      "üìç Example 1: Basic Array\n",
      "------------------------------\n",
      "Array:           [1.2, 3.8, 2.1, 0.5]\n",
      "Indices:         [0, 1, 2, 3]\n",
      "np.argmax():     1\n",
      "Max value:       3.8\n",
      "Explanation:     Index 1 contains the largest value (3.8)\n",
      "\n",
      "üìç Example 2: Q-Learning Actions\n",
      "------------------------------\n",
      "Q-values by action:\n",
      "  Index 0:    Up =   2.1\n",
      "  Index 1:  Down =  -1.0\n",
      "  Index 2:  Left =   0.5\n",
      "  Index 3: Right =   3.5 üëà BEST!\n",
      "\n",
      "np.argmax(q_values): 3\n",
      "Best action:         Right\n",
      "Best Q-value:        3.5\n",
      "\n",
      "üìç Example 3: Edge Cases\n",
      "------------------------------\n",
      "All same values: [2.0, 2.0, 2.0, 2.0]\n",
      "np.argmax():     0 (returns first occurrence)\n",
      "All negative:    [-5.0, -1.0, -3.0, -2.0]\n",
      "np.argmax():     1 (index of least negative)\n",
      "Single value:    [42.0]\n",
      "np.argmax():     0 (only option)\n",
      "\n",
      "üìç Example 4: Related Functions Comparison\n",
      "------------------------------\n",
      "Array:               [1.5, 4.2, 2.8, 0.9]\n",
      "np.argmax():         1 (index of max)\n",
      "np.max():            4.2 (actual max value)\n",
      "np.argmin():         3 (index of min)\n",
      "np.min():            0.9 (actual min value)\n",
      "\n",
      "============================================================\n",
      "üéÆ Q-LEARNING ACTION SELECTION WITH ARGMAX\n",
      "============================================================\n",
      "\n",
      "üè† State 0: Clear best choice\n",
      "----------------------------------------\n",
      "Q-values:\n",
      "  Action 0 (   Up):   0.1\n",
      "  Action 1 ( Down):   0.3\n",
      "  Action 2 ( Left):   0.2\n",
      "  Action 3 (Right):   0.8\n",
      "\n",
      "Action Selection:\n",
      "  np.argmax(q_values) = 3\n",
      "  ‚Üí Choose: Action 3 (Right)\n",
      "  ‚Üí Q-value: 0.8\n",
      "\n",
      "üè† State 1: Tie between actions\n",
      "----------------------------------------\n",
      "Q-values:\n",
      "  Action 0 (   Up):   2.1\n",
      "  Action 1 ( Down):   2.1\n",
      "  Action 2 ( Left):   1.5\n",
      "  Action 3 (Right):   2.1\n",
      "\n",
      "Action Selection:\n",
      "  np.argmax(q_values) = 0\n",
      "  ‚Üí Choose: Action 0 (Up)\n",
      "  ‚Üí Q-value: 2.1\n",
      "  ‚ö†Ô∏è  Note: Tied with actions [0, 1, 3] - argmax picks first\n",
      "\n",
      "üè† State 2: All negative Q-values\n",
      "----------------------------------------\n",
      "Q-values:\n",
      "  Action 0 (   Up):  -1.0\n",
      "  Action 1 ( Down):  -0.5\n",
      "  Action 2 ( Left):  -2.0\n",
      "  Action 3 (Right):  -0.3\n",
      "\n",
      "Action Selection:\n",
      "  np.argmax(q_values) = 3\n",
      "  ‚Üí Choose: Action 3 (Right)\n",
      "  ‚Üí Q-value: -0.3\n",
      "\n",
      "üè† State 3: All zeros (untrained)\n",
      "----------------------------------------\n",
      "Q-values:\n",
      "  Action 0 (   Up):   0.0\n",
      "  Action 1 ( Down):   0.0\n",
      "  Action 2 ( Left):   0.0\n",
      "  Action 3 (Right):   0.0\n",
      "\n",
      "Action Selection:\n",
      "  np.argmax(q_values) = 0\n",
      "  ‚Üí Choose: Action 0 (Up)\n",
      "  ‚Üí Q-value: 0.0\n",
      "  ‚ö†Ô∏è  Note: Tied with actions [0, 1, 2, 3] - argmax picks first\n",
      "\n",
      "============================================================\n",
      "üîß MANUAL vs np.argmax() COMPARISON\n",
      "============================================================\n",
      "Q-values: [1.2, 3.8, 2.1, 0.5]\n",
      "\n",
      "üî® Manual Method:\n",
      "  Step 1: Check index 0, value = 1.2\n",
      "           Not larger than current max (1.2)\n",
      "  Step 2: Check index 1, value = 3.8\n",
      "           New maximum! Update max_index to 1\n",
      "  Step 3: Check index 2, value = 2.1\n",
      "           Not larger than current max (3.8)\n",
      "  Step 4: Check index 3, value = 0.5\n",
      "           Not larger than current max (3.8)\n",
      "  Final result: max_index = 1, max_value = 3.8\n",
      "\n",
      "‚ö° Using np.argmax():\n",
      "  np.argmax(q_values) = 1\n",
      "  Same result: True ‚úÖ\n",
      "\n",
      "============================================================\n",
      "üìù SUMMARY\n",
      "============================================================\n",
      "‚Ä¢ np.argmax(array) returns the INDEX of the largest value\n",
      "‚Ä¢ In Q-learning: argmax selects the action with highest Q-value\n",
      "‚Ä¢ If multiple values tie for max, argmax returns the first index\n",
      "‚Ä¢ This is how agents choose the 'best' action (exploitation)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def demonstrate_argmax():\n",
    "    \"\"\"Demonstrate how np.argmax works in different scenarios\"\"\"\n",
    "\n",
    "    print(\"=\"*60)\n",
    "    print(\"üîç UNDERSTANDING np.argmax()\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # Example 1: Basic usage\n",
    "    print(\"\\nüìç Example 1: Basic Array\")\n",
    "    print(\"-\"*30)\n",
    "    values = [1.2, 3.8, 2.1, 0.5]\n",
    "    max_index = np.argmax(values)\n",
    "    max_value = values[max_index]\n",
    "\n",
    "    print(f\"Array:           {values}\")\n",
    "    print(f\"Indices:         [0, 1, 2, 3]\")\n",
    "    print(f\"np.argmax():     {max_index}\")\n",
    "    print(f\"Max value:       {max_value}\")\n",
    "    print(f\"Explanation:     Index {max_index} contains the largest value ({max_value})\")\n",
    "\n",
    "    # Example 2: Q-learning context\n",
    "    print(\"\\nüìç Example 2: Q-Learning Actions\")\n",
    "    print(\"-\"*30)\n",
    "    q_values = np.array([2.1, -1.0, 0.5, 3.5])\n",
    "    actions = [\"Up\", \"Down\", \"Left\", \"Right\"]\n",
    "    best_action_index = np.argmax(q_values)\n",
    "    best_action_name = actions[best_action_index]\n",
    "    best_q_value = q_values[best_action_index]\n",
    "\n",
    "    print(\"Q-values by action:\")\n",
    "    for i, (action, q_val) in enumerate(zip(actions, q_values)):\n",
    "        marker = \" üëà BEST!\" if i == best_action_index else \"\"\n",
    "        print(f\"  Index {i}: {action:>5} = {q_val:5.1f}{marker}\")\n",
    "\n",
    "    print(f\"\\nnp.argmax(q_values): {best_action_index}\")\n",
    "    print(f\"Best action:         {best_action_name}\")\n",
    "    print(f\"Best Q-value:        {best_q_value}\")\n",
    "\n",
    "    # Example 3: Edge cases\n",
    "    print(\"\\nüìç Example 3: Edge Cases\")\n",
    "    print(\"-\"*30)\n",
    "\n",
    "    # All same values\n",
    "    same_values = [2.0, 2.0, 2.0, 2.0]\n",
    "    print(f\"All same values: {same_values}\")\n",
    "    print(f\"np.argmax():     {np.argmax(same_values)} (returns first occurrence)\")\n",
    "\n",
    "    # Negative values\n",
    "    negative_values = [-5.0, -1.0, -3.0, -2.0]\n",
    "    print(f\"All negative:    {negative_values}\")\n",
    "    print(f\"np.argmax():     {np.argmax(negative_values)} (index of least negative)\")\n",
    "\n",
    "    # Single value\n",
    "    single_value = [42.0]\n",
    "    print(f\"Single value:    {single_value}\")\n",
    "    print(f\"np.argmax():     {np.argmax(single_value)} (only option)\")\n",
    "\n",
    "    # Example 4: Compare with related functions\n",
    "    print(\"\\nüìç Example 4: Related Functions Comparison\")\n",
    "    print(\"-\"*30)\n",
    "    test_array = [1.5, 4.2, 2.8, 0.9]\n",
    "\n",
    "    print(f\"Array:               {test_array}\")\n",
    "    print(f\"np.argmax():         {np.argmax(test_array)} (index of max)\")\n",
    "    print(f\"np.max():            {np.max(test_array)} (actual max value)\")\n",
    "    print(f\"np.argmin():         {np.argmin(test_array)} (index of min)\")\n",
    "    print(f\"np.min():            {np.min(test_array)} (actual min value)\")\n",
    "\n",
    "def q_learning_action_selection_demo():\n",
    "    \"\"\"Show how argmax is used in Q-learning action selection\"\"\"\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üéÆ Q-LEARNING ACTION SELECTION WITH ARGMAX\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # Simulate different states with different Q-values\n",
    "    states_info = [\n",
    "        {\"state\": 0, \"q_values\": [0.1, 0.3, 0.2, 0.8], \"description\": \"Clear best choice\"},\n",
    "        {\"state\": 1, \"q_values\": [2.1, 2.1, 1.5, 2.1], \"description\": \"Tie between actions\"},\n",
    "        {\"state\": 2, \"q_values\": [-1.0, -0.5, -2.0, -0.3], \"description\": \"All negative Q-values\"},\n",
    "        {\"state\": 3, \"q_values\": [0.0, 0.0, 0.0, 0.0], \"description\": \"All zeros (untrained)\"}\n",
    "    ]\n",
    "\n",
    "    actions = [\"Up\", \"Down\", \"Left\", \"Right\"]\n",
    "\n",
    "    for state_info in states_info:\n",
    "        state = state_info[\"state\"]\n",
    "        q_values = np.array(state_info[\"q_values\"])\n",
    "        description = state_info[\"description\"]\n",
    "\n",
    "        print(f\"\\nüè† State {state}: {description}\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "        # Show Q-values\n",
    "        print(\"Q-values:\")\n",
    "        for i, (action, q_val) in enumerate(zip(actions, q_values)):\n",
    "            print(f\"  Action {i} ({action:>5}): {q_val:5.1f}\")\n",
    "\n",
    "        # Apply argmax\n",
    "        best_action_idx = np.argmax(q_values)\n",
    "        best_action_name = actions[best_action_idx]\n",
    "        best_q_value = q_values[best_action_idx]\n",
    "\n",
    "        print(f\"\\nAction Selection:\")\n",
    "        print(f\"  np.argmax(q_values) = {best_action_idx}\")\n",
    "        print(f\"  ‚Üí Choose: Action {best_action_idx} ({best_action_name})\")\n",
    "        print(f\"  ‚Üí Q-value: {best_q_value}\")\n",
    "\n",
    "        # Show what happens with ties\n",
    "        if len(np.where(q_values == best_q_value)[0]) > 1:\n",
    "            tied_indices = np.where(q_values == best_q_value)[0]\n",
    "            print(f\"  ‚ö†Ô∏è  Note: Tied with actions {list(tied_indices)} - argmax picks first\")\n",
    "\n",
    "def manual_vs_argmax_comparison():\n",
    "    \"\"\"Compare manual max finding vs np.argmax\"\"\"\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üîß MANUAL vs np.argmax() COMPARISON\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    q_values = [1.2, 3.8, 2.1, 0.5]\n",
    "\n",
    "    print(f\"Q-values: {q_values}\")\n",
    "    print()\n",
    "\n",
    "    # Manual way (what argmax does internally)\n",
    "    print(\"üî® Manual Method:\")\n",
    "    max_value = q_values[0]\n",
    "    max_index = 0\n",
    "\n",
    "    for i in range(len(q_values)):\n",
    "        print(f\"  Step {i+1}: Check index {i}, value = {q_values[i]}\")\n",
    "        if q_values[i] > max_value:\n",
    "            max_value = q_values[i]\n",
    "            max_index = i\n",
    "            print(f\"           New maximum! Update max_index to {i}\")\n",
    "        else:\n",
    "            print(f\"           Not larger than current max ({max_value})\")\n",
    "\n",
    "    print(f\"  Final result: max_index = {max_index}, max_value = {max_value}\")\n",
    "\n",
    "    # Using argmax\n",
    "    print(f\"\\n‚ö° Using np.argmax():\")\n",
    "    argmax_result = np.argmax(q_values)\n",
    "    print(f\"  np.argmax(q_values) = {argmax_result}\")\n",
    "    print(f\"  Same result: {max_index == argmax_result} ‚úÖ\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demonstrate_argmax()\n",
    "    q_learning_action_selection_demo()\n",
    "    manual_vs_argmax_comparison()\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üìù SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"‚Ä¢ np.argmax(array) returns the INDEX of the largest value\")\n",
    "    print(\"‚Ä¢ In Q-learning: argmax selects the action with highest Q-value\")\n",
    "    print(\"‚Ä¢ If multiple values tie for max, argmax returns the first index\")\n",
    "    print(\"‚Ä¢ This is how agents choose the 'best' action (exploitation)\")\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T07:17:46.526281Z",
     "start_time": "2025-08-03T07:17:46.523271Z"
    },
    "id": "gBCU3DSY3M_W"
   },
   "outputs": [],
   "source": [
    "def train_agent():\n",
    "    state = env.reset()\n",
    "\n",
    "    while not done:\n",
    "        # Epsilon controls exploration\n",
    "        if random.random() < epsilon:\n",
    "            action = random.choice(actions)  # Explore\n",
    "        else:\n",
    "            action = argmax(q_table[state])  # Exploit\n",
    "\n",
    "        next_state, reward, done = env.step(action)\n",
    "\n",
    "        # Gamma affects how we value future rewards\n",
    "        if done:\n",
    "            target = reward\n",
    "        else:\n",
    "            target = reward + gamma * max(q_table[next_state])\n",
    "\n",
    "        # Learning rate controls how much we update\n",
    "        q_table[state, action] += lr * (target - q_table[state, action])\n",
    "\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting values\n",
    "state = 1\n",
    "epsilon = 0.3\n",
    "q_table[1] = [2.0, 3.5]  # Q-values for state 1\n",
    "lr = 0.1\n",
    "gamma = 0.9\n",
    "\n",
    "# Step 1: Choose action\n",
    "random_val = 0.75  # > 0.3, so exploit\n",
    "action = argmax([2.0, 3.5]) = 1  # Choose action 1 (RIGHT)\n",
    "\n",
    "# Step 2: Take action\n",
    "next_state, reward, done = env.step(1)\n",
    "# Returns: next_state=2, reward=-1, done=False\n",
    "\n",
    "# Step 3: Calculate target\n",
    "target = -1 + 0.9 * max(q_table[2])\n",
    "# Assume q_table[2] = [1.0, 2.0]\n",
    "target = -1 + 0.9 * 2.0 = -1 + 1.8 = 0.8\n",
    "\n",
    "# Step 4: Update Q-table\n",
    "old_q = q_table[1, 1] = 3.5\n",
    "difference = 0.8 - 3.5 = -2.7\n",
    "update = 0.1 * (-2.7) = -0.27\n",
    "q_table[1, 1] = 3.5 + (-0.27) = 3.23\n",
    "\n",
    "# Step 5: Move to next state\n",
    "state = 2  # Ready for next iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hps9Vj4q7FqQ"
   },
   "source": [
    "# QLearningAgent (Bare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T07:17:46.669589Z",
     "start_time": "2025-08-03T07:17:46.664004Z"
    },
    "id": "7Fx5y7dWtqnL"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class QLearningAgent:\n",
    "    def __init__(self, num_states, num_actions, lr=0.1, gamma=0.95, epsilon=0.1):\n",
    "        # Initialize Q-table with zeros\n",
    "        self.q_table = np.zeros((num_states, num_actions))\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def get_action(self, state):\n",
    "        # Epsilon-greedy action selection\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randint(0, len(self.q_table[state]) - 1)\n",
    "        else:\n",
    "            return np.argmax(self.q_table[state])  # Direct table lookup\n",
    "\n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        # Direct Q-table update using Bellman equation\n",
    "        if done:\n",
    "            target = reward\n",
    "        else:\n",
    "            target = reward + self.gamma * np.max(self.q_table[next_state])\n",
    "\n",
    "        # Update specific table entry\n",
    "        self.q_table[state, action] += self.lr * (target - self.q_table[state, action])\n",
    "\n",
    "# Usage\n",
    "agent = QLearningAgent(num_states=100, num_actions=4)\n",
    "state = 5\n",
    "action = agent.get_action(state)  # Just lookup q_table[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hquyBYYn7iGd"
   },
   "source": [
    "# QLearningAgent (Verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T07:17:46.723138Z",
     "start_time": "2025-08-03T07:17:46.672996Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I1x29bq63ZVM",
    "outputId": "1eb0612c-bc78-4f7c-b210-05249ed43f73",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Q-Learning Agent Initialized:\n",
      "   üìä Q-table shape   : (5, 2)\n",
      "   üìà Learning rate (lr)     : 0.1\n",
      "   üí∞ Discount fact (gamma)  : 0.9\n",
      "   üéØ Explortn rate (epsilon): 0.3\n",
      "   üß† Initial Q-table (all zeros):\n",
      "[[0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]]\n",
      "\n",
      "üöÄ Starting Training!\n",
      "\n",
      "================================================================================\n",
      "\n",
      "üé¨ EPISODE 1/3\n",
      "========================================\n",
      "üèÅ Starting at state 0\n",
      "\n",
      "üìç Step 1:\n",
      "--------------------\n",
      "üéÆ Action Selection for State 0:\n",
      "   üìã Current Q-values: [0. 0.]\n",
      "   ‚≠ê Best action would be: 0 (Q-value: 0.000)\n",
      "   üé≤ EXPLORING! Random prob 0.115 < epsilon 0.3\n",
      "   ‚û°Ô∏è  Chose random action: 0\n",
      "   üìä Exploration count: 1\n",
      "   üîÑ Explore/Exploit ratio: 1/0\n",
      "\n",
      "üåç Environment Response:\n",
      "   üé¨ Action taken: 0 (left)\n",
      "   üìç State transition: 0 ‚Üí 0\n",
      "   üí∞ Reward received: -5\n",
      "   üìä Episode reward so far: -5\n",
      "üìö Q-Learning Update #1:\n",
      "   üèÅ State: 0 ‚Üí Action: 0 ‚Üí Reward: -5 ‚Üí Next State: 0\n",
      "   ‚ö° Episode done: False\n",
      "   üéØ Target calculation (episode continues):\n",
      "      Next state Q-values: [0. 0.]\n",
      "      Best next action: 0 (Q-value: 0.000)\n",
      "      Target = reward + gamma * max_next_Q\n",
      "      Target = -5 + 0.9 * 0.000 = -5.000\n",
      "   üîÑ Q-value Update:\n",
      "      Old Q-value: 0.000\n",
      "      TD Error: target - old = -5.000 - 0.000 = -5.000\n",
      "      Learning step: lr * TD_error = 0.1 * -5.000 = -0.500\n",
      "      New Q-value: old + learning_step = 0.000 + -0.500 = -0.500\n",
      "      üìà Change: -0.500\n",
      "\n",
      "üìç Step 2:\n",
      "--------------------\n",
      "üéÆ Action Selection for State 0:\n",
      "   üìã Current Q-values: [-0.5  0. ]\n",
      "   ‚≠ê Best action would be: 1 (Q-value: 0.000)\n",
      "   üéØ EXPLOITING! Random prob 0.765 >= epsilon 0.3\n",
      "   ‚û°Ô∏è  Chose best action: 1\n",
      "   üìä Exploitation count: 1\n",
      "   üîÑ Explore/Exploit ratio: 1/1\n",
      "\n",
      "üåç Environment Response:\n",
      "   üé¨ Action taken: 1 (right)\n",
      "   üìç State transition: 0 ‚Üí 1\n",
      "   üí∞ Reward received: -1\n",
      "   üìä Episode reward so far: -6\n",
      "üìö Q-Learning Update #2:\n",
      "   üèÅ State: 0 ‚Üí Action: 1 ‚Üí Reward: -1 ‚Üí Next State: 1\n",
      "   ‚ö° Episode done: False\n",
      "   üéØ Target calculation (episode continues):\n",
      "      Next state Q-values: [0. 0.]\n",
      "      Best next action: 0 (Q-value: 0.000)\n",
      "      Target = reward + gamma * max_next_Q\n",
      "      Target = -1 + 0.9 * 0.000 = -1.000\n",
      "   üîÑ Q-value Update:\n",
      "      Old Q-value: 0.000\n",
      "      TD Error: target - old = -1.000 - 0.000 = -1.000\n",
      "      Learning step: lr * TD_error = 0.1 * -1.000 = -0.100\n",
      "      New Q-value: old + learning_step = 0.000 + -0.100 = -0.100\n",
      "      üìà Change: -0.100\n",
      "\n",
      "üìç Step 3:\n",
      "--------------------\n",
      "üéÆ Action Selection for State 1:\n",
      "   üìã Current Q-values: [0. 0.]\n",
      "   ‚≠ê Best action would be: 0 (Q-value: 0.000)\n",
      "   üéØ EXPLOITING! Random prob 0.577 >= epsilon 0.3\n",
      "   ‚û°Ô∏è  Chose best action: 0\n",
      "   üìä Exploitation count: 2\n",
      "   üîÑ Explore/Exploit ratio: 1/2\n",
      "\n",
      "üåç Environment Response:\n",
      "   üé¨ Action taken: 0 (left)\n",
      "   üìç State transition: 1 ‚Üí 0\n",
      "   üí∞ Reward received: -1\n",
      "   üìä Episode reward so far: -7\n",
      "üìö Q-Learning Update #3:\n",
      "   üèÅ State: 1 ‚Üí Action: 0 ‚Üí Reward: -1 ‚Üí Next State: 0\n",
      "   ‚ö° Episode done: False\n",
      "   üéØ Target calculation (episode continues):\n",
      "      Next state Q-values: [-0.5 -0.1]\n",
      "      Best next action: 1 (Q-value: -0.100)\n",
      "      Target = reward + gamma * max_next_Q\n",
      "      Target = -1 + 0.9 * -0.100 = -1.090\n",
      "   üîÑ Q-value Update:\n",
      "      Old Q-value: 0.000\n",
      "      TD Error: target - old = -1.090 - 0.000 = -1.090\n",
      "      Learning step: lr * TD_error = 0.1 * -1.090 = -0.109\n",
      "      New Q-value: old + learning_step = 0.000 + -0.109 = -0.109\n",
      "      üìà Change: -0.109\n",
      "\n",
      "üìç Step 4:\n",
      "--------------------\n",
      "üéÆ Action Selection for State 0:\n",
      "   üìã Current Q-values: [-0.5 -0.1]\n",
      "   ‚≠ê Best action would be: 1 (Q-value: -0.100)\n",
      "   üé≤ EXPLORING! Random prob 0.109 < epsilon 0.3\n",
      "   ‚û°Ô∏è  Chose random action: 0\n",
      "   üìä Exploration count: 2\n",
      "   üîÑ Explore/Exploit ratio: 2/2\n",
      "\n",
      "üåç Environment Response:\n",
      "   üé¨ Action taken: 0 (left)\n",
      "   üìç State transition: 0 ‚Üí 0\n",
      "   üí∞ Reward received: -5\n",
      "   üìä Episode reward so far: -12\n",
      "üìö Q-Learning Update #4:\n",
      "   üèÅ State: 0 ‚Üí Action: 0 ‚Üí Reward: -5 ‚Üí Next State: 0\n",
      "   ‚ö° Episode done: False\n",
      "   üéØ Target calculation (episode continues):\n",
      "      Next state Q-values: [-0.5 -0.1]\n",
      "      Best next action: 1 (Q-value: -0.100)\n",
      "      Target = reward + gamma * max_next_Q\n",
      "      Target = -5 + 0.9 * -0.100 = -5.090\n",
      "   üîÑ Q-value Update:\n",
      "      Old Q-value: -0.500\n",
      "      TD Error: target - old = -5.090 - -0.500 = -4.590\n",
      "      Learning step: lr * TD_error = 0.1 * -4.590 = -0.459\n",
      "      New Q-value: old + learning_step = -0.500 + -0.459 = -0.959\n",
      "      üìà Change: -0.459\n",
      "\n",
      "üìç Step 5:\n",
      "--------------------\n",
      "üéÆ Action Selection for State 0:\n",
      "   üìã Current Q-values: [-0.959 -0.1  ]\n",
      "   ‚≠ê Best action would be: 1 (Q-value: -0.100)\n",
      "   üéØ EXPLOITING! Random prob 0.340 >= epsilon 0.3\n",
      "   ‚û°Ô∏è  Chose best action: 1\n",
      "   üìä Exploitation count: 3\n",
      "   üîÑ Explore/Exploit ratio: 2/3\n",
      "\n",
      "üåç Environment Response:\n",
      "   üé¨ Action taken: 1 (right)\n",
      "   üìç State transition: 0 ‚Üí 1\n",
      "   üí∞ Reward received: -1\n",
      "   üìä Episode reward so far: -13\n",
      "üìö Q-Learning Update #5:\n",
      "   üèÅ State: 0 ‚Üí Action: 1 ‚Üí Reward: -1 ‚Üí Next State: 1\n",
      "   ‚ö° Episode done: False\n",
      "   üéØ Target calculation (episode continues):\n",
      "      Next state Q-values: [-0.109  0.   ]\n",
      "      Best next action: 1 (Q-value: 0.000)\n",
      "      Target = reward + gamma * max_next_Q\n",
      "      Target = -1 + 0.9 * 0.000 = -1.000\n",
      "   üîÑ Q-value Update:\n",
      "      Old Q-value: -0.100\n",
      "      TD Error: target - old = -1.000 - -0.100 = -0.900\n",
      "      Learning step: lr * TD_error = 0.1 * -0.900 = -0.090\n",
      "      New Q-value: old + learning_step = -0.100 + -0.090 = -0.190\n",
      "      üìà Change: -0.090\n",
      "\n",
      "üìç Step 6:\n",
      "--------------------\n",
      "üéÆ Action Selection for State 1:\n",
      "   üìã Current Q-values: [-0.109  0.   ]\n",
      "   ‚≠ê Best action would be: 1 (Q-value: 0.000)\n",
      "   üéØ EXPLOITING! Random prob 0.880 >= epsilon 0.3\n",
      "   ‚û°Ô∏è  Chose best action: 1\n",
      "   üìä Exploitation count: 4\n",
      "   üîÑ Explore/Exploit ratio: 2/4\n",
      "\n",
      "üåç Environment Response:\n",
      "   üé¨ Action taken: 1 (right)\n",
      "   üìç State transition: 1 ‚Üí 2\n",
      "   üí∞ Reward received: -1\n",
      "   üìä Episode reward so far: -14\n",
      "üìö Q-Learning Update #6:\n",
      "   üèÅ State: 1 ‚Üí Action: 1 ‚Üí Reward: -1 ‚Üí Next State: 2\n",
      "   ‚ö° Episode done: False\n",
      "   üéØ Target calculation (episode continues):\n",
      "      Next state Q-values: [0. 0.]\n",
      "      Best next action: 0 (Q-value: 0.000)\n",
      "      Target = reward + gamma * max_next_Q\n",
      "      Target = -1 + 0.9 * 0.000 = -1.000\n",
      "   üîÑ Q-value Update:\n",
      "      Old Q-value: 0.000\n",
      "      TD Error: target - old = -1.000 - 0.000 = -1.000\n",
      "      Learning step: lr * TD_error = 0.1 * -1.000 = -0.100\n",
      "      New Q-value: old + learning_step = 0.000 + -0.100 = -0.100\n",
      "      üìà Change: -0.100\n",
      "\n",
      "üìç Step 7:\n",
      "--------------------\n",
      "üéÆ Action Selection for State 2:\n",
      "   üìã Current Q-values: [0. 0.]\n",
      "   ‚≠ê Best action would be: 0 (Q-value: 0.000)\n",
      "   üéØ EXPLOITING! Random prob 0.912 >= epsilon 0.3\n",
      "   ‚û°Ô∏è  Chose best action: 0\n",
      "   üìä Exploitation count: 5\n",
      "   üîÑ Explore/Exploit ratio: 2/5\n",
      "\n",
      "üåç Environment Response:\n",
      "   üé¨ Action taken: 0 (left)\n",
      "   üìç State transition: 2 ‚Üí 1\n",
      "   üí∞ Reward received: -1\n",
      "   üìä Episode reward so far: -15\n",
      "üìö Q-Learning Update #7:\n",
      "   üèÅ State: 2 ‚Üí Action: 0 ‚Üí Reward: -1 ‚Üí Next State: 1\n",
      "   ‚ö° Episode done: False\n",
      "   üéØ Target calculation (episode continues):\n",
      "      Next state Q-values: [-0.109 -0.1  ]\n",
      "      Best next action: 1 (Q-value: -0.100)\n",
      "      Target = reward + gamma * max_next_Q\n",
      "      Target = -1 + 0.9 * -0.100 = -1.090\n",
      "   üîÑ Q-value Update:\n",
      "      Old Q-value: 0.000\n",
      "      TD Error: target - old = -1.090 - 0.000 = -1.090\n",
      "      Learning step: lr * TD_error = 0.1 * -1.090 = -0.109\n",
      "      New Q-value: old + learning_step = 0.000 + -0.109 = -0.109\n",
      "      üìà Change: -0.109\n",
      "\n",
      "üìç Step 8:\n",
      "--------------------\n",
      "üéÆ Action Selection for State 1:\n",
      "   üìã Current Q-values: [-0.109 -0.1  ]\n",
      "   ‚≠ê Best action would be: 1 (Q-value: -0.100)\n",
      "   üéØ EXPLOITING! Random prob 0.324 >= epsilon 0.3\n",
      "   ‚û°Ô∏è  Chose best action: 1\n",
      "   üìä Exploitation count: 6\n",
      "   üîÑ Explore/Exploit ratio: 2/6\n",
      "\n",
      "üåç Environment Response:\n",
      "   üé¨ Action taken: 1 (right)\n",
      "   üìç State transition: 1 ‚Üí 2\n",
      "   üí∞ Reward received: -1\n",
      "   üìä Episode reward so far: -16\n",
      "üìö Q-Learning Update #8:\n",
      "   üèÅ State: 1 ‚Üí Action: 1 ‚Üí Reward: -1 ‚Üí Next State: 2\n",
      "   ‚ö° Episode done: False\n",
      "   üéØ Target calculation (episode continues):\n",
      "      Next state Q-values: [-0.109  0.   ]\n",
      "      Best next action: 1 (Q-value: 0.000)\n",
      "      Target = reward + gamma * max_next_Q\n",
      "      Target = -1 + 0.9 * 0.000 = -1.000\n",
      "   üîÑ Q-value Update:\n",
      "      Old Q-value: -0.100\n",
      "      TD Error: target - old = -1.000 - -0.100 = -0.900\n",
      "      Learning step: lr * TD_error = 0.1 * -0.900 = -0.090\n",
      "      New Q-value: old + learning_step = -0.100 + -0.090 = -0.190\n",
      "      üìà Change: -0.090\n",
      "‚è∞ Episode ended: Maximum steps (8) reached\n",
      "\n",
      "üìà Episode 1 Summary:\n",
      "   Total Reward: -16\n",
      "   Steps Taken: 8\n",
      "   Goal Reached: No\n",
      "\n",
      "üìä Q-Table after Episode 1:\n",
      "State\\Action     Action0     Action1\n",
      "------------------------------------\n",
      "State  0     -0.959      -0.190    \n",
      "State  1     -0.109      -0.190    \n",
      "State  2     -0.109       0.000    \n",
      "State  3      0.000       0.000    \n",
      "State  4      0.000       0.000    \n",
      "\n",
      "üéØ Current Policy (Best Action per State):\n",
      "   State 0: Action 1 (Q-value: -0.190)\n",
      "   State 1: Action 0 (Q-value: -0.109)\n",
      "   State 2: Action 1 (Q-value: 0.000)\n",
      "   State 3: Action 0 (Q-value: 0.000)\n",
      "   State 4: Action 0 (Q-value: 0.000)\n",
      "\n",
      "================================================================================\n",
      "\n",
      "üé¨ EPISODE 2/3\n",
      "========================================\n",
      "üèÅ Starting at state 0\n",
      "\n",
      "üìç Step 1:\n",
      "--------------------\n",
      "üéÆ Action Selection for State 0:\n",
      "   üìã Current Q-values: [-0.959 -0.19 ]\n",
      "   ‚≠ê Best action would be: 1 (Q-value: -0.190)\n",
      "   üéØ EXPLOITING! Random prob 0.448 >= epsilon 0.3\n",
      "   ‚û°Ô∏è  Chose best action: 1\n",
      "   üìä Exploitation count: 7\n",
      "   üîÑ Explore/Exploit ratio: 2/7\n",
      "\n",
      "üåç Environment Response:\n",
      "   üé¨ Action taken: 1 (right)\n",
      "   üìç State transition: 0 ‚Üí 1\n",
      "   üí∞ Reward received: -1\n",
      "   üìä Episode reward so far: -1\n",
      "üìö Q-Learning Update #9:\n",
      "   üèÅ State: 0 ‚Üí Action: 1 ‚Üí Reward: -1 ‚Üí Next State: 1\n",
      "   ‚ö° Episode done: False\n",
      "   üéØ Target calculation (episode continues):\n",
      "      Next state Q-values: [-0.109 -0.19 ]\n",
      "      Best next action: 0 (Q-value: -0.109)\n",
      "      Target = reward + gamma * max_next_Q\n",
      "      Target = -1 + 0.9 * -0.109 = -1.098\n",
      "   üîÑ Q-value Update:\n",
      "      Old Q-value: -0.190\n",
      "      TD Error: target - old = -1.098 - -0.190 = -0.908\n",
      "      Learning step: lr * TD_error = 0.1 * -0.908 = -0.091\n",
      "      New Q-value: old + learning_step = -0.190 + -0.091 = -0.281\n",
      "      üìà Change: -0.091\n",
      "\n",
      "üìç Step 2:\n",
      "--------------------\n",
      "üéÆ Action Selection for State 1:\n",
      "   üìã Current Q-values: [-0.109 -0.19 ]\n",
      "   ‚≠ê Best action would be: 0 (Q-value: -0.109)\n",
      "   üéØ EXPLOITING! Random prob 0.766 >= epsilon 0.3\n",
      "   ‚û°Ô∏è  Chose best action: 0\n",
      "   üìä Exploitation count: 8\n",
      "   üîÑ Explore/Exploit ratio: 2/8\n",
      "\n",
      "üåç Environment Response:\n",
      "   üé¨ Action taken: 0 (left)\n",
      "   üìç State transition: 1 ‚Üí 0\n",
      "   üí∞ Reward received: -1\n",
      "   üìä Episode reward so far: -2\n",
      "üìö Q-Learning Update #10:\n",
      "   üèÅ State: 1 ‚Üí Action: 0 ‚Üí Reward: -1 ‚Üí Next State: 0\n",
      "   ‚ö° Episode done: False\n",
      "   üéØ Target calculation (episode continues):\n",
      "      Next state Q-values: [-0.959   -0.28081]\n",
      "      Best next action: 1 (Q-value: -0.281)\n",
      "      Target = reward + gamma * max_next_Q\n",
      "      Target = -1 + 0.9 * -0.281 = -1.253\n",
      "   üîÑ Q-value Update:\n",
      "      Old Q-value: -0.109\n",
      "      TD Error: target - old = -1.253 - -0.109 = -1.144\n",
      "      Learning step: lr * TD_error = 0.1 * -1.144 = -0.114\n",
      "      New Q-value: old + learning_step = -0.109 + -0.114 = -0.223\n",
      "      üìà Change: -0.114\n",
      "\n",
      "üìç Step 3:\n",
      "--------------------\n",
      "üéÆ Action Selection for State 0:\n",
      "   üìã Current Q-values: [-0.959   -0.28081]\n",
      "   ‚≠ê Best action would be: 1 (Q-value: -0.281)\n",
      "   üéØ EXPLOITING! Random prob 0.813 >= epsilon 0.3\n",
      "   ‚û°Ô∏è  Chose best action: 1\n",
      "   üìä Exploitation count: 9\n",
      "   üîÑ Explore/Exploit ratio: 2/9\n",
      "\n",
      "üåç Environment Response:\n",
      "   üé¨ Action taken: 1 (right)\n",
      "   üìç State transition: 0 ‚Üí 1\n",
      "   üí∞ Reward received: -1\n",
      "   üìä Episode reward so far: -3\n",
      "üìö Q-Learning Update #11:\n",
      "   üèÅ State: 0 ‚Üí Action: 1 ‚Üí Reward: -1 ‚Üí Next State: 1\n",
      "   ‚ö° Episode done: False\n",
      "   üéØ Target calculation (episode continues):\n",
      "      Next state Q-values: [-0.2233729 -0.19     ]\n",
      "      Best next action: 1 (Q-value: -0.190)\n",
      "      Target = reward + gamma * max_next_Q\n",
      "      Target = -1 + 0.9 * -0.190 = -1.171\n",
      "   üîÑ Q-value Update:\n",
      "      Old Q-value: -0.281\n",
      "      TD Error: target - old = -1.171 - -0.281 = -0.890\n",
      "      Learning step: lr * TD_error = 0.1 * -0.890 = -0.089\n",
      "      New Q-value: old + learning_step = -0.281 + -0.089 = -0.370\n",
      "      üìà Change: -0.089\n",
      "\n",
      "üìç Step 4:\n",
      "--------------------\n",
      "üéÆ Action Selection for State 1:\n",
      "   üìã Current Q-values: [-0.2233729 -0.19     ]\n",
      "   ‚≠ê Best action would be: 1 (Q-value: -0.190)\n",
      "   üé≤ EXPLORING! Random prob 0.111 < epsilon 0.3\n",
      "   ‚û°Ô∏è  Chose random action: 0\n",
      "   üìä Exploration count: 3\n",
      "   üîÑ Explore/Exploit ratio: 3/9\n",
      "\n",
      "üåç Environment Response:\n",
      "   üé¨ Action taken: 0 (left)\n",
      "   üìç State transition: 1 ‚Üí 0\n",
      "   üí∞ Reward received: -1\n",
      "   üìä Episode reward so far: -4\n",
      "üìö Q-Learning Update #12:\n",
      "   üèÅ State: 1 ‚Üí Action: 0 ‚Üí Reward: -1 ‚Üí Next State: 0\n",
      "   ‚ö° Episode done: False\n",
      "   üéØ Target calculation (episode continues):\n",
      "      Next state Q-values: [-0.959    -0.369829]\n",
      "      Best next action: 1 (Q-value: -0.370)\n",
      "      Target = reward + gamma * max_next_Q\n",
      "      Target = -1 + 0.9 * -0.370 = -1.333\n",
      "   üîÑ Q-value Update:\n",
      "      Old Q-value: -0.223\n",
      "      TD Error: target - old = -1.333 - -0.223 = -1.109\n",
      "      Learning step: lr * TD_error = 0.1 * -1.109 = -0.111\n",
      "      New Q-value: old + learning_step = -0.223 + -0.111 = -0.334\n",
      "      üìà Change: -0.111\n",
      "\n",
      "üìç Step 5:\n",
      "--------------------\n",
      "üéÆ Action Selection for State 0:\n",
      "   üìã Current Q-values: [-0.959    -0.369829]\n",
      "   ‚≠ê Best action would be: 1 (Q-value: -0.370)\n",
      "   üéØ EXPLOITING! Random prob 0.365 >= epsilon 0.3\n",
      "   ‚û°Ô∏è  Chose best action: 1\n",
      "   üìä Exploitation count: 10\n",
      "   üîÑ Explore/Exploit ratio: 3/10\n",
      "\n",
      "üåç Environment Response:\n",
      "   üé¨ Action taken: 1 (right)\n",
      "   üìç State transition: 0 ‚Üí 1\n",
      "   üí∞ Reward received: -1\n",
      "   üìä Episode reward so far: -5\n",
      "üìö Q-Learning Update #13:\n",
      "   üèÅ State: 0 ‚Üí Action: 1 ‚Üí Reward: -1 ‚Üí Next State: 1\n",
      "   ‚ö° Episode done: False\n",
      "   üéØ Target calculation (episode continues):\n",
      "      Next state Q-values: [-0.33432022 -0.19      ]\n",
      "      Best next action: 1 (Q-value: -0.190)\n",
      "      Target = reward + gamma * max_next_Q\n",
      "      Target = -1 + 0.9 * -0.190 = -1.171\n",
      "   üîÑ Q-value Update:\n",
      "      Old Q-value: -0.370\n",
      "      TD Error: target - old = -1.171 - -0.370 = -0.801\n",
      "      Learning step: lr * TD_error = 0.1 * -0.801 = -0.080\n",
      "      New Q-value: old + learning_step = -0.370 + -0.080 = -0.450\n",
      "      üìà Change: -0.080\n",
      "\n",
      "üìç Step 6:\n",
      "--------------------\n",
      "üéÆ Action Selection for State 1:\n",
      "   üìã Current Q-values: [-0.33432022 -0.19      ]\n",
      "   ‚≠ê Best action would be: 1 (Q-value: -0.190)\n",
      "   üéØ EXPLOITING! Random prob 0.683 >= epsilon 0.3\n",
      "   ‚û°Ô∏è  Chose best action: 1\n",
      "   üìä Exploitation count: 11\n",
      "   üîÑ Explore/Exploit ratio: 3/11\n",
      "\n",
      "üåç Environment Response:\n",
      "   üé¨ Action taken: 1 (right)\n",
      "   üìç State transition: 1 ‚Üí 2\n",
      "   üí∞ Reward received: -1\n",
      "   üìä Episode reward so far: -6\n",
      "üìö Q-Learning Update #14:\n",
      "   üèÅ State: 1 ‚Üí Action: 1 ‚Üí Reward: -1 ‚Üí Next State: 2\n",
      "   ‚ö° Episode done: False\n",
      "   üéØ Target calculation (episode continues):\n",
      "      Next state Q-values: [-0.109  0.   ]\n",
      "      Best next action: 1 (Q-value: 0.000)\n",
      "      Target = reward + gamma * max_next_Q\n",
      "      Target = -1 + 0.9 * 0.000 = -1.000\n",
      "   üîÑ Q-value Update:\n",
      "      Old Q-value: -0.190\n",
      "      TD Error: target - old = -1.000 - -0.190 = -0.810\n",
      "      Learning step: lr * TD_error = 0.1 * -0.810 = -0.081\n",
      "      New Q-value: old + learning_step = -0.190 + -0.081 = -0.271\n",
      "      üìà Change: -0.081\n",
      "\n",
      "üìç Step 7:\n",
      "--------------------\n",
      "üéÆ Action Selection for State 2:\n",
      "   üìã Current Q-values: [-0.109  0.   ]\n",
      "   ‚≠ê Best action would be: 1 (Q-value: 0.000)\n",
      "   üé≤ EXPLORING! Random prob 0.016 < epsilon 0.3\n",
      "   ‚û°Ô∏è  Chose random action: 1\n",
      "   üìä Exploration count: 4\n",
      "   üîÑ Explore/Exploit ratio: 4/11\n",
      "\n",
      "üåç Environment Response:\n",
      "   üé¨ Action taken: 1 (right)\n",
      "   üìç State transition: 2 ‚Üí 3\n",
      "   üí∞ Reward received: -1\n",
      "   üìä Episode reward so far: -7\n",
      "üìö Q-Learning Update #15:\n",
      "   üèÅ State: 2 ‚Üí Action: 1 ‚Üí Reward: -1 ‚Üí Next State: 3\n",
      "   ‚ö° Episode done: False\n",
      "   üéØ Target calculation (episode continues):\n",
      "      Next state Q-values: [0. 0.]\n",
      "      Best next action: 0 (Q-value: 0.000)\n",
      "      Target = reward + gamma * max_next_Q\n",
      "      Target = -1 + 0.9 * 0.000 = -1.000\n",
      "   üîÑ Q-value Update:\n",
      "      Old Q-value: 0.000\n",
      "      TD Error: target - old = -1.000 - 0.000 = -1.000\n",
      "      Learning step: lr * TD_error = 0.1 * -1.000 = -0.100\n",
      "      New Q-value: old + learning_step = 0.000 + -0.100 = -0.100\n",
      "      üìà Change: -0.100\n",
      "\n",
      "üìç Step 8:\n",
      "--------------------\n",
      "üéÆ Action Selection for State 3:\n",
      "   üìã Current Q-values: [0. 0.]\n",
      "   ‚≠ê Best action would be: 0 (Q-value: 0.000)\n",
      "   üéØ EXPLOITING! Random prob 0.664 >= epsilon 0.3\n",
      "   ‚û°Ô∏è  Chose best action: 0\n",
      "   üìä Exploitation count: 12\n",
      "   üîÑ Explore/Exploit ratio: 4/12\n",
      "\n",
      "üåç Environment Response:\n",
      "   üé¨ Action taken: 0 (left)\n",
      "   üìç State transition: 3 ‚Üí 2\n",
      "   üí∞ Reward received: -1\n",
      "   üìä Episode reward so far: -8\n",
      "üìö Q-Learning Update #16:\n",
      "   üèÅ State: 3 ‚Üí Action: 0 ‚Üí Reward: -1 ‚Üí Next State: 2\n",
      "   ‚ö° Episode done: False\n",
      "   üéØ Target calculation (episode continues):\n",
      "      Next state Q-values: [-0.109 -0.1  ]\n",
      "      Best next action: 1 (Q-value: -0.100)\n",
      "      Target = reward + gamma * max_next_Q\n",
      "      Target = -1 + 0.9 * -0.100 = -1.090\n",
      "   üîÑ Q-value Update:\n",
      "      Old Q-value: 0.000\n",
      "      TD Error: target - old = -1.090 - 0.000 = -1.090\n",
      "      Learning step: lr * TD_error = 0.1 * -1.090 = -0.109\n",
      "      New Q-value: old + learning_step = 0.000 + -0.109 = -0.109\n",
      "      üìà Change: -0.109\n",
      "‚è∞ Episode ended: Maximum steps (8) reached\n",
      "\n",
      "üìà Episode 2 Summary:\n",
      "   Total Reward: -8\n",
      "   Steps Taken: 8\n",
      "   Goal Reached: No\n",
      "\n",
      "üìä Q-Table after Episode 2:\n",
      "State\\Action     Action0     Action1\n",
      "------------------------------------\n",
      "State  0     -0.959      -0.450    \n",
      "State  1     -0.334      -0.271    \n",
      "State  2     -0.109      -0.100    \n",
      "State  3     -0.109       0.000    \n",
      "State  4      0.000       0.000    \n",
      "\n",
      "üéØ Current Policy (Best Action per State):\n",
      "   State 0: Action 1 (Q-value: -0.450)\n",
      "   State 1: Action 1 (Q-value: -0.271)\n",
      "   State 2: Action 1 (Q-value: -0.100)\n",
      "   State 3: Action 1 (Q-value: 0.000)\n",
      "   State 4: Action 0 (Q-value: 0.000)\n",
      "\n",
      "================================================================================\n",
      "\n",
      "üé¨ EPISODE 3/3\n",
      "========================================\n",
      "üèÅ Starting at state 0\n",
      "\n",
      "üìç Step 1:\n",
      "--------------------\n",
      "üéÆ Action Selection for State 0:\n",
      "   üìã Current Q-values: [-0.959     -0.4499461]\n",
      "   ‚≠ê Best action would be: 1 (Q-value: -0.450)\n",
      "   üéØ EXPLOITING! Random prob 0.771 >= epsilon 0.3\n",
      "   ‚û°Ô∏è  Chose best action: 1\n",
      "   üìä Exploitation count: 13\n",
      "   üîÑ Explore/Exploit ratio: 4/13\n",
      "\n",
      "üåç Environment Response:\n",
      "   üé¨ Action taken: 1 (right)\n",
      "   üìç State transition: 0 ‚Üí 1\n",
      "   üí∞ Reward received: -1\n",
      "   üìä Episode reward so far: -1\n",
      "üìö Q-Learning Update #17:\n",
      "   üèÅ State: 0 ‚Üí Action: 1 ‚Üí Reward: -1 ‚Üí Next State: 1\n",
      "   ‚ö° Episode done: False\n",
      "   üéØ Target calculation (episode continues):\n",
      "      Next state Q-values: [-0.33432022 -0.271     ]\n",
      "      Best next action: 1 (Q-value: -0.271)\n",
      "      Target = reward + gamma * max_next_Q\n",
      "      Target = -1 + 0.9 * -0.271 = -1.244\n",
      "   üîÑ Q-value Update:\n",
      "      Old Q-value: -0.450\n",
      "      TD Error: target - old = -1.244 - -0.450 = -0.794\n",
      "      Learning step: lr * TD_error = 0.1 * -0.794 = -0.079\n",
      "      New Q-value: old + learning_step = -0.450 + -0.079 = -0.529\n",
      "      üìà Change: -0.079\n",
      "\n",
      "üìç Step 2:\n",
      "--------------------\n",
      "üéÆ Action Selection for State 1:\n",
      "   üìã Current Q-values: [-0.33432022 -0.271     ]\n",
      "   ‚≠ê Best action would be: 1 (Q-value: -0.271)\n",
      "   üéØ EXPLOITING! Random prob 0.747 >= epsilon 0.3\n",
      "   ‚û°Ô∏è  Chose best action: 1\n",
      "   üìä Exploitation count: 14\n",
      "   üîÑ Explore/Exploit ratio: 4/14\n",
      "\n",
      "üåç Environment Response:\n",
      "   üé¨ Action taken: 1 (right)\n",
      "   üìç State transition: 1 ‚Üí 2\n",
      "   üí∞ Reward received: -1\n",
      "   üìä Episode reward so far: -2\n",
      "üìö Q-Learning Update #18:\n",
      "   üèÅ State: 1 ‚Üí Action: 1 ‚Üí Reward: -1 ‚Üí Next State: 2\n",
      "   ‚ö° Episode done: False\n",
      "   üéØ Target calculation (episode continues):\n",
      "      Next state Q-values: [-0.109 -0.1  ]\n",
      "      Best next action: 1 (Q-value: -0.100)\n",
      "      Target = reward + gamma * max_next_Q\n",
      "      Target = -1 + 0.9 * -0.100 = -1.090\n",
      "   üîÑ Q-value Update:\n",
      "      Old Q-value: -0.271\n",
      "      TD Error: target - old = -1.090 - -0.271 = -0.819\n",
      "      Learning step: lr * TD_error = 0.1 * -0.819 = -0.082\n",
      "      New Q-value: old + learning_step = -0.271 + -0.082 = -0.353\n",
      "      üìà Change: -0.082\n",
      "\n",
      "üìç Step 3:\n",
      "--------------------\n",
      "üéÆ Action Selection for State 2:\n",
      "   üìã Current Q-values: [-0.109 -0.1  ]\n",
      "   ‚≠ê Best action would be: 1 (Q-value: -0.100)\n",
      "   üéØ EXPLOITING! Random prob 0.446 >= epsilon 0.3\n",
      "   ‚û°Ô∏è  Chose best action: 1\n",
      "   üìä Exploitation count: 15\n",
      "   üîÑ Explore/Exploit ratio: 4/15\n",
      "\n",
      "üåç Environment Response:\n",
      "   üé¨ Action taken: 1 (right)\n",
      "   üìç State transition: 2 ‚Üí 3\n",
      "   üí∞ Reward received: -1\n",
      "   üìä Episode reward so far: -3\n",
      "üìö Q-Learning Update #19:\n",
      "   üèÅ State: 2 ‚Üí Action: 1 ‚Üí Reward: -1 ‚Üí Next State: 3\n",
      "   ‚ö° Episode done: False\n",
      "   üéØ Target calculation (episode continues):\n",
      "      Next state Q-values: [-0.109  0.   ]\n",
      "      Best next action: 1 (Q-value: 0.000)\n",
      "      Target = reward + gamma * max_next_Q\n",
      "      Target = -1 + 0.9 * 0.000 = -1.000\n",
      "   üîÑ Q-value Update:\n",
      "      Old Q-value: -0.100\n",
      "      TD Error: target - old = -1.000 - -0.100 = -0.900\n",
      "      Learning step: lr * TD_error = 0.1 * -0.900 = -0.090\n",
      "      New Q-value: old + learning_step = -0.100 + -0.090 = -0.190\n",
      "      üìà Change: -0.090\n",
      "\n",
      "üìç Step 4:\n",
      "--------------------\n",
      "üéÆ Action Selection for State 3:\n",
      "   üìã Current Q-values: [-0.109  0.   ]\n",
      "   ‚≠ê Best action would be: 1 (Q-value: 0.000)\n",
      "   üéØ EXPLOITING! Random prob 0.370 >= epsilon 0.3\n",
      "   ‚û°Ô∏è  Chose best action: 1\n",
      "   üìä Exploitation count: 16\n",
      "   üîÑ Explore/Exploit ratio: 4/16\n",
      "\n",
      "üåç Environment Response:\n",
      "   üé¨ Action taken: 1 (right)\n",
      "   üìç State transition: 3 ‚Üí 4\n",
      "   üí∞ Reward received: 10\n",
      "   üìä Episode reward so far: 7\n",
      "üìö Q-Learning Update #20:\n",
      "   üèÅ State: 3 ‚Üí Action: 1 ‚Üí Reward: 10 ‚Üí Next State: 4\n",
      "   ‚ö° Episode done: True\n",
      "   üéØ Target calculation (episode ended):\n",
      "      Target = reward = 10\n",
      "   üîÑ Q-value Update:\n",
      "      Old Q-value: 0.000\n",
      "      TD Error: target - old = 10.000 - 0.000 = 10.000\n",
      "      Learning step: lr * TD_error = 0.1 * 10.000 = 1.000\n",
      "      New Q-value: old + learning_step = 0.000 + 1.000 = 1.000\n",
      "      üìà Change: +1.000\n",
      "‚úÖ Episode completed! Goal reached in 4 steps!\n",
      "\n",
      "üìà Episode 3 Summary:\n",
      "   Total Reward: 7\n",
      "   Steps Taken: 4\n",
      "   Goal Reached: Yes\n",
      "\n",
      "üìä Q-Table after Episode 3:\n",
      "State\\Action     Action0     Action1\n",
      "------------------------------------\n",
      "State  0     -0.959      -0.529    \n",
      "State  1     -0.334      -0.353    \n",
      "State  2     -0.109      -0.190    \n",
      "State  3     -0.109       1.000    \n",
      "State  4      0.000       0.000    \n",
      "\n",
      "üéØ Current Policy (Best Action per State):\n",
      "   State 0: Action 1 (Q-value: -0.529)\n",
      "   State 1: Action 0 (Q-value: -0.334)\n",
      "   State 2: Action 0 (Q-value: -0.109)\n",
      "   State 3: Action 1 (Q-value: 1.000)\n",
      "   State 4: Action 0 (Q-value: 0.000)\n",
      "\n",
      "================================================================================\n",
      "\n",
      "üéâ Training Complete!\n",
      "üìä Total Episodes: 3\n",
      "üìà Rewards per Episode: [-16, -8, 7]\n",
      "üéØ Average Reward: -5.67\n",
      "üîç Exploration vs Exploitation: 4 vs 16\n",
      "\n",
      "üìä Final Q-Table:\n",
      "State\\Action     Action0     Action1\n",
      "------------------------------------\n",
      "State  0     -0.959      -0.529    \n",
      "State  1     -0.334      -0.353    \n",
      "State  2     -0.109      -0.190    \n",
      "State  3     -0.109       1.000    \n",
      "State  4      0.000       0.000    \n",
      "\n",
      "üéØ Current Policy (Best Action per State):\n",
      "   State 0: Action 1 (Q-value: -0.529)\n",
      "   State 1: Action 0 (Q-value: -0.334)\n",
      "   State 2: Action 0 (Q-value: -0.109)\n",
      "   State 3: Action 1 (Q-value: 1.000)\n",
      "   State 4: Action 0 (Q-value: 0.000)\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üß™ Testing the trained agent (no more learning, just exploitation):\n",
      "================================================================================\n",
      "üèÅ Starting test at state 0\n",
      "üéÆ Action Selection for State 0:\n",
      "   üìã Current Q-values: [-0.959      -0.52934149]\n",
      "   ‚≠ê Best action would be: 1 (Q-value: -0.529)\n",
      "   üéØ EXPLOITING! Random prob 0.445 >= epsilon 0.0\n",
      "   ‚û°Ô∏è  Chose best action: 1\n",
      "   üìä Exploitation count: 17\n",
      "   üîÑ Explore/Exploit ratio: 4/17\n",
      "\n",
      "   üé¨ Took action 1 ‚Üí moved to state 1, got reward -1\n",
      "üéÆ Action Selection for State 1:\n",
      "   üìã Current Q-values: [-0.33432022 -0.3529    ]\n",
      "   ‚≠ê Best action would be: 0 (Q-value: -0.334)\n",
      "   üéØ EXPLOITING! Random prob 0.162 >= epsilon 0.0\n",
      "   ‚û°Ô∏è  Chose best action: 0\n",
      "   üìä Exploitation count: 18\n",
      "   üîÑ Explore/Exploit ratio: 4/18\n",
      "\n",
      "   üé¨ Took action 0 ‚Üí moved to state 0, got reward -1\n",
      "üéÆ Action Selection for State 0:\n",
      "   üìã Current Q-values: [-0.959      -0.52934149]\n",
      "   ‚≠ê Best action would be: 1 (Q-value: -0.529)\n",
      "   üéØ EXPLOITING! Random prob 0.537 >= epsilon 0.0\n",
      "   ‚û°Ô∏è  Chose best action: 1\n",
      "   üìä Exploitation count: 19\n",
      "   üîÑ Explore/Exploit ratio: 4/19\n",
      "\n",
      "   üé¨ Took action 1 ‚Üí moved to state 1, got reward -1\n",
      "üéÆ Action Selection for State 1:\n",
      "   üìã Current Q-values: [-0.33432022 -0.3529    ]\n",
      "   ‚≠ê Best action would be: 0 (Q-value: -0.334)\n",
      "   üéØ EXPLOITING! Random prob 0.569 >= epsilon 0.0\n",
      "   ‚û°Ô∏è  Chose best action: 0\n",
      "   üìä Exploitation count: 20\n",
      "   üîÑ Explore/Exploit ratio: 4/20\n",
      "\n",
      "   üé¨ Took action 0 ‚Üí moved to state 0, got reward -1\n",
      "üéÆ Action Selection for State 0:\n",
      "   üìã Current Q-values: [-0.959      -0.52934149]\n",
      "   ‚≠ê Best action would be: 1 (Q-value: -0.529)\n",
      "   üéØ EXPLOITING! Random prob 0.066 >= epsilon 0.0\n",
      "   ‚û°Ô∏è  Chose best action: 1\n",
      "   üìä Exploitation count: 21\n",
      "   üîÑ Explore/Exploit ratio: 4/21\n",
      "\n",
      "   üé¨ Took action 1 ‚Üí moved to state 1, got reward -1\n",
      "üéÆ Action Selection for State 1:\n",
      "   üìã Current Q-values: [-0.33432022 -0.3529    ]\n",
      "   ‚≠ê Best action would be: 0 (Q-value: -0.334)\n",
      "   üéØ EXPLOITING! Random prob 0.369 >= epsilon 0.0\n",
      "   ‚û°Ô∏è  Chose best action: 0\n",
      "   üìä Exploitation count: 22\n",
      "   üîÑ Explore/Exploit ratio: 4/22\n",
      "\n",
      "   üé¨ Took action 0 ‚Üí moved to state 0, got reward -1\n",
      "üéÆ Action Selection for State 0:\n",
      "   üìã Current Q-values: [-0.959      -0.52934149]\n",
      "   ‚≠ê Best action would be: 1 (Q-value: -0.529)\n",
      "   üéØ EXPLOITING! Random prob 0.807 >= epsilon 0.0\n",
      "   ‚û°Ô∏è  Chose best action: 1\n",
      "   üìä Exploitation count: 23\n",
      "   üîÑ Explore/Exploit ratio: 4/23\n",
      "\n",
      "   üé¨ Took action 1 ‚Üí moved to state 1, got reward -1\n",
      "üéÆ Action Selection for State 1:\n",
      "   üìã Current Q-values: [-0.33432022 -0.3529    ]\n",
      "   ‚≠ê Best action would be: 0 (Q-value: -0.334)\n",
      "   üéØ EXPLOITING! Random prob 0.983 >= epsilon 0.0\n",
      "   ‚û°Ô∏è  Chose best action: 0\n",
      "   üìä Exploitation count: 24\n",
      "   üîÑ Explore/Exploit ratio: 4/24\n",
      "\n",
      "   üé¨ Took action 0 ‚Üí moved to state 0, got reward -1\n",
      "üéÆ Action Selection for State 0:\n",
      "   üìã Current Q-values: [-0.959      -0.52934149]\n",
      "   ‚≠ê Best action would be: 1 (Q-value: -0.529)\n",
      "   üéØ EXPLOITING! Random prob 0.810 >= epsilon 0.0\n",
      "   ‚û°Ô∏è  Chose best action: 1\n",
      "   üìä Exploitation count: 25\n",
      "   üîÑ Explore/Exploit ratio: 4/25\n",
      "\n",
      "   üé¨ Took action 1 ‚Üí moved to state 1, got reward -1\n",
      "üéÆ Action Selection for State 1:\n",
      "   üìã Current Q-values: [-0.33432022 -0.3529    ]\n",
      "   ‚≠ê Best action would be: 0 (Q-value: -0.334)\n",
      "   üéØ EXPLOITING! Random prob 0.106 >= epsilon 0.0\n",
      "   ‚û°Ô∏è  Chose best action: 0\n",
      "   üìä Exploitation count: 26\n",
      "   üîÑ Explore/Exploit ratio: 4/26\n",
      "\n",
      "   üé¨ Took action 0 ‚Üí moved to state 0, got reward -1\n",
      "‚ùå Failed to reach goal in 10 steps\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class QLearningAgent:\n",
    "    def __init__(self, num_states, num_actions, lr=0.1, gamma=0.95, epsilon=0.1):\n",
    "        # Initialize Q-table with zeros\n",
    "        self.q_table = np.zeros((num_states, num_actions))\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.num_states = num_states\n",
    "        self.num_actions = num_actions\n",
    "\n",
    "        # Logging counters\n",
    "        self.exploration_count = 0\n",
    "        self.exploitation_count = 0\n",
    "        self.update_count = 0\n",
    "\n",
    "        print(f\"ü§ñ Q-Learning Agent Initialized:\")\n",
    "        print(f\"   üìä Q-table shape   : {self.q_table.shape}\")\n",
    "        print(f\"   üìà Learning rate (lr)     : {self.lr}\")\n",
    "        print(f\"   üí∞ Discount fact (gamma)  : {self.gamma}\")\n",
    "        print(f\"   üéØ Explortn rate (epsilon): {self.epsilon}\")\n",
    "        print(f\"   üß† Initial Q-table (all zeros):\")\n",
    "        print(f\"{self.q_table}\\n\")\n",
    "\n",
    "    def get_action(self, state, verbose=True):\n",
    "        \"\"\"Get action using epsilon-greedy strategy with detailed logging\"\"\"\n",
    "\n",
    "        # Get current Q-values for this state\n",
    "        current_q_values = self.q_table[state]\n",
    "        best_action = np.argmax(current_q_values)\n",
    "        best_q_value = current_q_values[best_action]\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"üéÆ Action Selection for State {state}:\")\n",
    "            print(f\"   üìã Current Q-values: {current_q_values}\")\n",
    "            print(f\"   ‚≠ê Best action would be: {best_action} (Q-value: {best_q_value:.3f})\")\n",
    "\n",
    "        # Epsilon-greedy decision\n",
    "        random_prob = random.random()\n",
    "        if random_prob < self.epsilon:\n",
    "            # Exploration: choose random action\n",
    "            action = random.randint(0, self.num_actions - 1)\n",
    "            self.exploration_count += 1\n",
    "            if verbose:\n",
    "                print(f\"   üé≤ EXPLORING! Random prob {random_prob:.3f} < epsilon {self.epsilon}\")\n",
    "                print(f\"   ‚û°Ô∏è  Chose random action: {action}\")\n",
    "                print(f\"   üìä Exploration count: {self.exploration_count}\")\n",
    "        else:\n",
    "            # Exploitation: choose best known action\n",
    "            action = best_action\n",
    "            self.exploitation_count += 1\n",
    "            if verbose:\n",
    "                print(f\"   üéØ EXPLOITING! Random prob {random_prob:.3f} >= epsilon {self.epsilon}\")\n",
    "                print(f\"   ‚û°Ô∏è  Chose best action: {action}\")\n",
    "                print(f\"   üìä Exploitation count: {self.exploitation_count}\")\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"   üîÑ Explore/Exploit ratio: {self.exploration_count}/{self.exploitation_count}\\n\")\n",
    "\n",
    "        return action\n",
    "\n",
    "    def update(self, state, action, reward, next_state, done, verbose=True):\n",
    "        \"\"\"Update Q-table with detailed logging of the learning process\"\"\"\n",
    "\n",
    "        self.update_count += 1\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"üìö Q-Learning Update #{self.update_count}:\")\n",
    "            print(f\"   üèÅ State: {state} ‚Üí Action: {action} ‚Üí Reward: {reward} ‚Üí Next State: {next_state}\")\n",
    "            print(f\"   ‚ö° Episode done: {done}\")\n",
    "\n",
    "        # Store old Q-value for comparison\n",
    "        old_q_value = self.q_table[state, action]\n",
    "\n",
    "        # Calculate target Q-value\n",
    "        if done:\n",
    "            target = reward\n",
    "            if verbose:\n",
    "                print(f\"   üéØ Target calculation (episode ended):\")\n",
    "                print(f\"      Target = reward = {reward}\")\n",
    "        else:\n",
    "            next_q_values = self.q_table[next_state]\n",
    "            max_next_q = np.max(next_q_values)\n",
    "            best_next_action = np.argmax(next_q_values)\n",
    "            target = reward + self.gamma * max_next_q\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"   üéØ Target calculation (episode continues):\")\n",
    "                print(f\"      Next state Q-values: {next_q_values}\")\n",
    "                print(f\"      Best next action: {best_next_action} (Q-value: {max_next_q:.3f})\")\n",
    "                print(f\"      Target = reward + gamma * max_next_Q\")\n",
    "                print(f\"      Target = {reward} + {self.gamma} * {max_next_q:.3f} = {target:.3f}\")\n",
    "\n",
    "        # Calculate temporal difference error\n",
    "        td_error = target - old_q_value\n",
    "\n",
    "        # Update Q-value using Q-learning formula\n",
    "        new_q_value = old_q_value + self.lr * td_error\n",
    "        self.q_table[state, action] = new_q_value\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"   üîÑ Q-value Update:\")\n",
    "            print(f\"      Old Q-value: {old_q_value:.3f}\")\n",
    "            print(f\"      TD Error: target - old = {target:.3f} - {old_q_value:.3f} = {td_error:.3f}\")\n",
    "            print(f\"      Learning step: lr * TD_error = {self.lr} * {td_error:.3f} = {self.lr * td_error:.3f}\")\n",
    "            print(f\"      New Q-value: old + learning_step = {old_q_value:.3f} + {self.lr * td_error:.3f} = {new_q_value:.3f}\")\n",
    "            print(f\"      üìà Change: {new_q_value - old_q_value:+.3f}\")\n",
    "\n",
    "        return td_error\n",
    "\n",
    "    def print_q_table(self, title=\"Current Q-Table\"):\n",
    "        \"\"\"Print the current Q-table in a readable format\"\"\"\n",
    "        print(f\"\\nüìä {title}:\")\n",
    "        print(\"State\\\\Action\", end=\"\")\n",
    "        for a in range(self.num_actions):\n",
    "            print(f\"     Action{a}\", end=\"\")\n",
    "        print()\n",
    "        print(\"-\" * (12 + 12 * self.num_actions))\n",
    "\n",
    "        for s in range(self.num_states):\n",
    "            print(f\"State {s:2d}   \", end=\"\")\n",
    "            for a in range(self.num_actions):\n",
    "                print(f\"{self.q_table[s, a]:8.3f}    \", end=\"\")\n",
    "            print()\n",
    "        print()\n",
    "\n",
    "    def print_policy(self):\n",
    "        \"\"\"Print the current policy (best action for each state)\"\"\"\n",
    "        print(\"üéØ Current Policy (Best Action per State):\")\n",
    "        for s in range(self.num_states):\n",
    "            best_action = np.argmax(self.q_table[s])\n",
    "            best_q_value = self.q_table[s, best_action]\n",
    "            print(f\"   State {s}: Action {best_action} (Q-value: {best_q_value:.3f})\")\n",
    "        print()\n",
    "\n",
    "\n",
    "def simple_environment_step(state, action, num_states=5):\n",
    "    \"\"\"\n",
    "    Simple environment for demonstration:\n",
    "    - States: 0, 1, 2, 3, 4\n",
    "    - Actions: 0 (left), 1 (right)\n",
    "    - Goal: Reach state 4 (rightmost)\n",
    "    - Reward: +10 for reaching goal, -1 for each step, -5 for going out of bounds\n",
    "    \"\"\"\n",
    "\n",
    "    if action == 0:  # Move left\n",
    "        next_state = max(0, state - 1)\n",
    "    else:  # Move right (action == 1)\n",
    "        next_state = min(num_states - 1, state + 1)\n",
    "\n",
    "    # Calculate reward\n",
    "    if next_state == num_states - 1:  # Reached goal\n",
    "        reward = 10\n",
    "        done = True\n",
    "    elif (action == 0 and state == 0) or (action == 1 and state == num_states - 1):\n",
    "        # Tried to go out of bounds\n",
    "        reward = -5\n",
    "        done = False\n",
    "    else:\n",
    "        reward = -1  # Normal step cost\n",
    "        done = False\n",
    "\n",
    "    return next_state, reward, done\n",
    "\n",
    "\n",
    "def train_agent(episodes=5, max_steps_per_episode=10, verbose=True):\n",
    "    \"\"\"Train the Q-learning agent with detailed logging\"\"\"\n",
    "\n",
    "    # Initialize agent\n",
    "    agent = QLearningAgent(num_states=5, num_actions=2, lr=0.1, gamma=0.9, epsilon=0.3)\n",
    "\n",
    "    print(\"üöÄ Starting Training!\\n\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    total_rewards = []\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        print(f\"\\nüé¨ EPISODE {episode + 1}/{episodes}\")\n",
    "        print(\"=\" * 40)\n",
    "\n",
    "        # Reset environment\n",
    "        state = 0  # Always start at leftmost state\n",
    "        episode_reward = 0\n",
    "        step_count = 0\n",
    "\n",
    "        print(f\"üèÅ Starting at state {state}\")\n",
    "\n",
    "        for step in range(max_steps_per_episode):\n",
    "            step_count += 1\n",
    "            print(f\"\\nüìç Step {step_count}:\")\n",
    "            print(\"-\" * 20)\n",
    "\n",
    "            # Get action\n",
    "            action = agent.get_action(state, verbose=verbose)\n",
    "\n",
    "            # Take action in environment\n",
    "            next_state, reward, done = simple_environment_step(state, action)\n",
    "            episode_reward += reward\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"üåç Environment Response:\")\n",
    "                print(f\"   üé¨ Action taken: {action} ({'left' if action == 0 else 'right'})\")\n",
    "                print(f\"   üìç State transition: {state} ‚Üí {next_state}\")\n",
    "                print(f\"   üí∞ Reward received: {reward}\")\n",
    "                print(f\"   üìä Episode reward so far: {episode_reward}\")\n",
    "\n",
    "            # Update Q-table\n",
    "            td_error = agent.update(state, action, reward, next_state, done, verbose=verbose)\n",
    "\n",
    "            # Move to next state\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                if verbose:\n",
    "                    print(f\"‚úÖ Episode completed! Goal reached in {step_count} steps!\")\n",
    "                break\n",
    "\n",
    "            if step_count >= max_steps_per_episode:\n",
    "                if verbose:\n",
    "                    print(f\"‚è∞ Episode ended: Maximum steps ({max_steps_per_episode}) reached\")\n",
    "                break\n",
    "\n",
    "        total_rewards.append(episode_reward)\n",
    "\n",
    "        print(f\"\\nüìà Episode {episode + 1} Summary:\")\n",
    "        print(f\"   Total Reward: {episode_reward}\")\n",
    "        print(f\"   Steps Taken: {step_count}\")\n",
    "        print(f\"   Goal Reached: {'Yes' if done else 'No'}\")\n",
    "\n",
    "        # Print Q-table after each episode\n",
    "        agent.print_q_table(f\"Q-Table after Episode {episode + 1}\")\n",
    "        agent.print_policy()\n",
    "\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "    print(f\"\\nüéâ Training Complete!\")\n",
    "    print(f\"üìä Total Episodes: {episodes}\")\n",
    "    print(f\"üìà Rewards per Episode: {total_rewards}\")\n",
    "    print(f\"üéØ Average Reward: {np.mean(total_rewards):.2f}\")\n",
    "    print(f\"üîç Exploration vs Exploitation: {agent.exploration_count} vs {agent.exploitation_count}\")\n",
    "\n",
    "    agent.print_q_table(\"Final Q-Table\")\n",
    "    agent.print_policy()\n",
    "\n",
    "    return agent\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Train with detailed logging\n",
    "    trained_agent = train_agent(episodes=3, max_steps_per_episode=8, verbose=True)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üß™ Testing the trained agent (no more learning, just exploitation):\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # Test the trained agent\n",
    "    trained_agent.epsilon = 0.0  # No more exploration, pure exploitation\n",
    "    state = 0\n",
    "    step = 0\n",
    "\n",
    "    print(f\"üèÅ Starting test at state {state}\")\n",
    "\n",
    "    while step < 10:\n",
    "        step += 1\n",
    "        action = trained_agent.get_action(state, verbose=True)\n",
    "        next_state, reward, done = simple_environment_step(state, action)\n",
    "        print(f\"   üé¨ Took action {action} ‚Üí moved to state {next_state}, got reward {reward}\")\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            print(f\"‚úÖ Goal reached in {step} steps!\")\n",
    "            break\n",
    "\n",
    "    if not done:\n",
    "        print(f\"‚ùå Failed to reach goal in {step} steps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h1jCGyUE74pW"
   },
   "source": [
    "# QLearningAgent(Table Verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T08:13:35.180762Z",
     "start_time": "2025-08-03T08:13:35.140340Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cw_cnRQ54ZDx",
    "outputId": "46d0f9e2-e4ba-4989-a62b-ec560c040b10",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Q-LEARNING TRAINING DEMONSTRATION\n",
      "Environment: 5 states (0,1,2,3,4), 2 actions (left/right), goal=state 4\n",
      "\n",
      "============================================================\n",
      "ü§ñ Q-LEARNING AGENT INITIALIZATION\n",
      "============================================================\n",
      "‚îÇ Q-table Shape       ‚îÇ (5, 2)\n",
      "‚îÇ Learning Rate (Œ±)   ‚îÇ 0.1\n",
      "‚îÇ Discount Factor (Œ≥) ‚îÇ 0.9\n",
      "‚îÇ Exploration Rate (Œµ)‚îÇ 0.3\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Initial Q-Table (all zeros):\n",
      "        A0     A1  \n",
      "S0 ‚îÇ  0.00  0.00 \n",
      "S1 ‚îÇ  0.00  0.00 \n",
      "S2 ‚îÇ  0.00  0.00 \n",
      "S3 ‚îÇ  0.00  0.00 \n",
      "S4 ‚îÇ  0.00  0.00 \n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "üé¨ EPISODE  1/10 - 13:43:35\n",
      "============================================================\n",
      "\n",
      "üìç STEP  1 - Current State: 0\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ACTION SELECTION (State 0) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Q-values: [0.00, 0.00]\n",
      "‚îÇ Best:     Action 0 (Q=0.000)\n",
      "‚îÇ Decision: EXPLOIT üéØ (0.420 ‚â• 0.3)\n",
      "‚îÇ Chosen:   Action 0 (greedy)\n",
      "‚îÇ Stats:    Explore 0/1 (0.0%)\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ENVIRONMENT RESPONSE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Action:     0 (LEFT)\n",
      "‚îÇ Transition: 0 ‚Üí 0\n",
      "‚îÇ Reward:     -5.0\n",
      "‚îÇ Done:       False\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ Q-UPDATE #1 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Transition: S0 --A0--> S0 (R=-5.0)\n",
      "‚îÇ Done:       False\n",
      "‚îÇ Next Q's:   [0.00, 0.00]\n",
      "‚îÇ Target:     -5 + 0.9√ó0.000 = -5.000\n",
      "‚îÇ Q-Update:   0.000 + 0.1√ó-5.000 = -0.500\n",
      "‚îÇ Change:     -0.500\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "üìç STEP  2 - Current State: 0\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ACTION SELECTION (State 0) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Q-values: [-0.50, 0.00]\n",
      "‚îÇ Best:     Action 1 (Q=0.000)\n",
      "‚îÇ Decision: EXPLOIT üéØ (0.533 ‚â• 0.3)\n",
      "‚îÇ Chosen:   Action 1 (greedy)\n",
      "‚îÇ Stats:    Explore 0/2 (0.0%)\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ENVIRONMENT RESPONSE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Action:     1 (RIGHT)\n",
      "‚îÇ Transition: 0 ‚Üí 1\n",
      "‚îÇ Reward:     -1.0\n",
      "‚îÇ Done:       False\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ Q-UPDATE #2 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Transition: S0 --A1--> S1 (R=-1.0)\n",
      "‚îÇ Done:       False\n",
      "‚îÇ Next Q's:   [0.00, 0.00]\n",
      "‚îÇ Target:     -1 + 0.9√ó0.000 = -1.000\n",
      "‚îÇ Q-Update:   0.000 + 0.1√ó-1.000 = -0.100\n",
      "‚îÇ Change:     -0.100\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "üìç STEP  3 - Current State: 1\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ACTION SELECTION (State 1) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Q-values: [0.00, 0.00]\n",
      "‚îÇ Best:     Action 0 (Q=0.000)\n",
      "‚îÇ Decision: EXPLOIT üéØ (0.753 ‚â• 0.3)\n",
      "‚îÇ Chosen:   Action 0 (greedy)\n",
      "‚îÇ Stats:    Explore 0/3 (0.0%)\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ENVIRONMENT RESPONSE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Action:     0 (LEFT)\n",
      "‚îÇ Transition: 1 ‚Üí 0\n",
      "‚îÇ Reward:     -1.0\n",
      "‚îÇ Done:       False\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ Q-UPDATE #3 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Transition: S1 --A0--> S0 (R=-1.0)\n",
      "‚îÇ Done:       False\n",
      "‚îÇ Next Q's:   [-0.50, -0.10]\n",
      "‚îÇ Target:     -1 + 0.9√ó-0.100 = -1.090\n",
      "‚îÇ Q-Update:   0.000 + 0.1√ó-1.090 = -0.109\n",
      "‚îÇ Change:     -0.109\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "üìç STEP  4 - Current State: 0\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ACTION SELECTION (State 0) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Q-values: [-0.50, -0.10]\n",
      "‚îÇ Best:     Action 1 (Q=-0.100)\n",
      "‚îÇ Decision: EXPLOIT üéØ (0.579 ‚â• 0.3)\n",
      "‚îÇ Chosen:   Action 1 (greedy)\n",
      "‚îÇ Stats:    Explore 0/4 (0.0%)\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ENVIRONMENT RESPONSE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Action:     1 (RIGHT)\n",
      "‚îÇ Transition: 0 ‚Üí 1\n",
      "‚îÇ Reward:     -1.0\n",
      "‚îÇ Done:       False\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ Q-UPDATE #4 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Transition: S0 --A1--> S1 (R=-1.0)\n",
      "‚îÇ Done:       False\n",
      "‚îÇ Next Q's:   [-0.11, 0.00]\n",
      "‚îÇ Target:     -1 + 0.9√ó0.000 = -1.000\n",
      "‚îÇ Q-Update:   -0.100 + 0.1√ó-0.900 = -0.190\n",
      "‚îÇ Change:     -0.090\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "üìç STEP  5 - Current State: 1\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ACTION SELECTION (State 1) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Q-values: [-0.11, 0.00]\n",
      "‚îÇ Best:     Action 1 (Q=0.000)\n",
      "‚îÇ Decision: EXPLOIT üéØ (0.629 ‚â• 0.3)\n",
      "‚îÇ Chosen:   Action 1 (greedy)\n",
      "‚îÇ Stats:    Explore 0/5 (0.0%)\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ENVIRONMENT RESPONSE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Action:     1 (RIGHT)\n",
      "‚îÇ Transition: 1 ‚Üí 2\n",
      "‚îÇ Reward:     -1.0\n",
      "‚îÇ Done:       False\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ Q-UPDATE #5 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Transition: S1 --A1--> S2 (R=-1.0)\n",
      "‚îÇ Done:       False\n",
      "‚îÇ Next Q's:   [0.00, 0.00]\n",
      "‚îÇ Target:     -1 + 0.9√ó0.000 = -1.000\n",
      "‚îÇ Q-Update:   0.000 + 0.1√ó-1.000 = -0.100\n",
      "‚îÇ Change:     -0.100\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "üìç STEP  6 - Current State: 2\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ACTION SELECTION (State 2) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Q-values: [0.00, 0.00]\n",
      "‚îÇ Best:     Action 0 (Q=0.000)\n",
      "‚îÇ Decision: EXPLOIT üéØ (0.974 ‚â• 0.3)\n",
      "‚îÇ Chosen:   Action 0 (greedy)\n",
      "‚îÇ Stats:    Explore 0/6 (0.0%)\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ENVIRONMENT RESPONSE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Action:     0 (LEFT)\n",
      "‚îÇ Transition: 2 ‚Üí 1\n",
      "‚îÇ Reward:     -1.0\n",
      "‚îÇ Done:       False\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ Q-UPDATE #6 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Transition: S2 --A0--> S1 (R=-1.0)\n",
      "‚îÇ Done:       False\n",
      "‚îÇ Next Q's:   [-0.11, -0.10]\n",
      "‚îÇ Target:     -1 + 0.9√ó-0.100 = -1.090\n",
      "‚îÇ Q-Update:   0.000 + 0.1√ó-1.090 = -0.109\n",
      "‚îÇ Change:     -0.109\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "üìç STEP  7 - Current State: 1\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ACTION SELECTION (State 1) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Q-values: [-0.11, -0.10]\n",
      "‚îÇ Best:     Action 1 (Q=-0.100)\n",
      "‚îÇ Decision: EXPLOIT üéØ (0.884 ‚â• 0.3)\n",
      "‚îÇ Chosen:   Action 1 (greedy)\n",
      "‚îÇ Stats:    Explore 0/7 (0.0%)\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ENVIRONMENT RESPONSE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Action:     1 (RIGHT)\n",
      "‚îÇ Transition: 1 ‚Üí 2\n",
      "‚îÇ Reward:     -1.0\n",
      "‚îÇ Done:       False\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ Q-UPDATE #7 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Transition: S1 --A1--> S2 (R=-1.0)\n",
      "‚îÇ Done:       False\n",
      "‚îÇ Next Q's:   [-0.11, 0.00]\n",
      "‚îÇ Target:     -1 + 0.9√ó0.000 = -1.000\n",
      "‚îÇ Q-Update:   -0.100 + 0.1√ó-0.900 = -0.190\n",
      "‚îÇ Change:     -0.090\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "üìç STEP  8 - Current State: 2\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ACTION SELECTION (State 2) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Q-values: [-0.11, 0.00]\n",
      "‚îÇ Best:     Action 1 (Q=0.000)\n",
      "‚îÇ Decision: EXPLOIT üéØ (0.522 ‚â• 0.3)\n",
      "‚îÇ Chosen:   Action 1 (greedy)\n",
      "‚îÇ Stats:    Explore 0/8 (0.0%)\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ENVIRONMENT RESPONSE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Action:     1 (RIGHT)\n",
      "‚îÇ Transition: 2 ‚Üí 3\n",
      "‚îÇ Reward:     -1.0\n",
      "‚îÇ Done:       False\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ Q-UPDATE #8 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Transition: S2 --A1--> S3 (R=-1.0)\n",
      "‚îÇ Done:       False\n",
      "‚îÇ Next Q's:   [0.00, 0.00]\n",
      "‚îÇ Target:     -1 + 0.9√ó0.000 = -1.000\n",
      "‚îÇ Q-Update:   0.000 + 0.1√ó-1.000 = -0.100\n",
      "‚îÇ Change:     -0.100\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "üìç STEP  9 - Current State: 3\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ACTION SELECTION (State 3) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Q-values: [0.00, 0.00]\n",
      "‚îÇ Best:     Action 0 (Q=0.000)\n",
      "‚îÇ Decision: EXPLORE üé≤ (0.040 < 0.3)\n",
      "‚îÇ Chosen:   Action 1 (random)\n",
      "‚îÇ Stats:    Explore 1/9 (11.1%)\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ENVIRONMENT RESPONSE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Action:     1 (RIGHT)\n",
      "‚îÇ Transition: 3 ‚Üí 4\n",
      "‚îÇ Reward:     +10.0\n",
      "‚îÇ Done:       True\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ Q-UPDATE #9 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Transition: S3 --A1--> S4 (R=+10.0)\n",
      "‚îÇ Done:       True\n",
      "‚îÇ Target:     10.000 (episode ended, no future)\n",
      "‚îÇ Q-Update:   0.000 + 0.1√ó10.000 = 1.000\n",
      "‚îÇ Change:     +1.000\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "‚îå‚îÄ EPISODE 1 SUMMARY ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Status:     SUCCESS ‚úÖ\n",
      "‚îÇ Steps:      9\n",
      "‚îÇ Reward:     -2.0\n",
      "‚îÇ Goal:       Reached\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "‚îå‚îÄ Q-TABLE AFTER EPISODE 1 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "        A0     A1  \n",
      "S0 ‚îÇ -0.50 -0.19 \n",
      "S1 ‚îÇ -0.11 -0.19 \n",
      "S2 ‚îÇ -0.11 -0.10 \n",
      "S3 ‚îÇ  0.00  1.00 \n",
      "S4 ‚îÇ  0.00  0.00 \n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "‚îå‚îÄ CURRENT POLICY ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ State 0: Action 1 (Q=-0.190)\n",
      "‚îÇ State 1: Action 0 (Q=-0.109)\n",
      "‚îÇ State 2: Action 1 (Q=-0.100)\n",
      "‚îÇ State 3: Action 1 (Q=1.000)\n",
      "‚îÇ State 4: Action 0 (Q=0.000)\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "============================================================\n",
      "üé¨ EPISODE  2/10 - 13:43:35\n",
      "============================================================\n",
      "\n",
      "üìç STEP  1 - Current State: 0\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ACTION SELECTION (State 0) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Q-values: [-0.50, -0.19]\n",
      "‚îÇ Best:     Action 1 (Q=-0.190)\n",
      "‚îÇ Decision: EXPLORE üé≤ (0.299 < 0.3)\n",
      "‚îÇ Chosen:   Action 0 (random)\n",
      "‚îÇ Stats:    Explore 2/10 (20.0%)\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ENVIRONMENT RESPONSE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Action:     0 (LEFT)\n",
      "‚îÇ Transition: 0 ‚Üí 0\n",
      "‚îÇ Reward:     -5.0\n",
      "‚îÇ Done:       False\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ Q-UPDATE #10 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Transition: S0 --A0--> S0 (R=-5.0)\n",
      "‚îÇ Done:       False\n",
      "‚îÇ Next Q's:   [-0.50, -0.19]\n",
      "‚îÇ Target:     -5 + 0.9√ó-0.190 = -5.171\n",
      "‚îÇ Q-Update:   -0.500 + 0.1√ó-4.671 = -0.967\n",
      "‚îÇ Change:     -0.467\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "üìç STEP  2 - Current State: 0\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ACTION SELECTION (State 0) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Q-values: [-0.97, -0.19]\n",
      "‚îÇ Best:     Action 1 (Q=-0.190)\n",
      "‚îÇ Decision: EXPLOIT üéØ (0.479 ‚â• 0.3)\n",
      "‚îÇ Chosen:   Action 1 (greedy)\n",
      "‚îÇ Stats:    Explore 2/11 (18.2%)\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ENVIRONMENT RESPONSE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Action:     1 (RIGHT)\n",
      "‚îÇ Transition: 0 ‚Üí 1\n",
      "‚îÇ Reward:     -1.0\n",
      "‚îÇ Done:       False\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ Q-UPDATE #11 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Transition: S0 --A1--> S1 (R=-1.0)\n",
      "‚îÇ Done:       False\n",
      "‚îÇ Next Q's:   [-0.11, -0.19]\n",
      "‚îÇ Target:     -1 + 0.9√ó-0.109 = -1.098\n",
      "‚îÇ Q-Update:   -0.190 + 0.1√ó-0.908 = -0.281\n",
      "‚îÇ Change:     -0.091\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "üìç STEP  3 - Current State: 1\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ACTION SELECTION (State 1) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Q-values: [-0.11, -0.19]\n",
      "‚îÇ Best:     Action 0 (Q=-0.109)\n",
      "‚îÇ Decision: EXPLOIT üéØ (0.468 ‚â• 0.3)\n",
      "‚îÇ Chosen:   Action 0 (greedy)\n",
      "‚îÇ Stats:    Explore 2/12 (16.7%)\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ENVIRONMENT RESPONSE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Action:     0 (LEFT)\n",
      "‚îÇ Transition: 1 ‚Üí 0\n",
      "‚îÇ Reward:     -1.0\n",
      "‚îÇ Done:       False\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ Q-UPDATE #12 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Transition: S1 --A0--> S0 (R=-1.0)\n",
      "‚îÇ Done:       False\n",
      "‚îÇ Next Q's:   [-0.97, -0.28]\n",
      "‚îÇ Target:     -1 + 0.9√ó-0.281 = -1.253\n",
      "‚îÇ Q-Update:   -0.109 + 0.1√ó-1.144 = -0.223\n",
      "‚îÇ Change:     -0.114\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "üìç STEP  4 - Current State: 0\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ACTION SELECTION (State 0) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Q-values: [-0.97, -0.28]\n",
      "‚îÇ Best:     Action 1 (Q=-0.281)\n",
      "‚îÇ Decision: EXPLOIT üéØ (0.788 ‚â• 0.3)\n",
      "‚îÇ Chosen:   Action 1 (greedy)\n",
      "‚îÇ Stats:    Explore 2/13 (15.4%)\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ENVIRONMENT RESPONSE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Action:     1 (RIGHT)\n",
      "‚îÇ Transition: 0 ‚Üí 1\n",
      "‚îÇ Reward:     -1.0\n",
      "‚îÇ Done:       False\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ Q-UPDATE #13 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Transition: S0 --A1--> S1 (R=-1.0)\n",
      "‚îÇ Done:       False\n",
      "‚îÇ Next Q's:   [-0.22, -0.19]\n",
      "‚îÇ Target:     -1 + 0.9√ó-0.190 = -1.171\n",
      "‚îÇ Q-Update:   -0.281 + 0.1√ó-0.890 = -0.370\n",
      "‚îÇ Change:     -0.089\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "üìç STEP  5 - Current State: 1\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ACTION SELECTION (State 1) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Q-values: [-0.22, -0.19]\n",
      "‚îÇ Best:     Action 1 (Q=-0.190)\n",
      "‚îÇ Decision: EXPLOIT üéØ (0.640 ‚â• 0.3)\n",
      "‚îÇ Chosen:   Action 1 (greedy)\n",
      "‚îÇ Stats:    Explore 2/14 (14.3%)\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ENVIRONMENT RESPONSE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Action:     1 (RIGHT)\n",
      "‚îÇ Transition: 1 ‚Üí 2\n",
      "‚îÇ Reward:     -1.0\n",
      "‚îÇ Done:       False\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ Q-UPDATE #14 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Transition: S1 --A1--> S2 (R=-1.0)\n",
      "‚îÇ Done:       False\n",
      "‚îÇ Next Q's:   [-0.11, -0.10]\n",
      "‚îÇ Target:     -1 + 0.9√ó-0.100 = -1.090\n",
      "‚îÇ Q-Update:   -0.190 + 0.1√ó-0.900 = -0.280\n",
      "‚îÇ Change:     -0.090\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "üìç STEP  6 - Current State: 2\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ACTION SELECTION (State 2) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Q-values: [-0.11, -0.10]\n",
      "‚îÇ Best:     Action 1 (Q=-0.100)\n",
      "‚îÇ Decision: EXPLORE üé≤ (0.048 < 0.3)\n",
      "‚îÇ Chosen:   Action 1 (random)\n",
      "‚îÇ Stats:    Explore 3/15 (20.0%)\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ENVIRONMENT RESPONSE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Action:     1 (RIGHT)\n",
      "‚îÇ Transition: 2 ‚Üí 3\n",
      "‚îÇ Reward:     -1.0\n",
      "‚îÇ Done:       False\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ Q-UPDATE #15 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Transition: S2 --A1--> S3 (R=-1.0)\n",
      "‚îÇ Done:       False\n",
      "‚îÇ Next Q's:   [0.00, 1.00]\n",
      "‚îÇ Target:     -1 + 0.9√ó1.000 = -0.100\n",
      "‚îÇ Q-Update:   -0.100 + 0.1√ó0.000 = -0.100\n",
      "‚îÇ Change:     +0.000\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "üìç STEP  7 - Current State: 3\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ACTION SELECTION (State 3) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Q-values: [0.00, 1.00]\n",
      "‚îÇ Best:     Action 1 (Q=1.000)\n",
      "‚îÇ Decision: EXPLORE üé≤ (0.082 < 0.3)\n",
      "‚îÇ Chosen:   Action 0 (random)\n",
      "‚îÇ Stats:    Explore 4/16 (25.0%)\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ENVIRONMENT RESPONSE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Action:     0 (LEFT)\n",
      "‚îÇ Transition: 3 ‚Üí 2\n",
      "‚îÇ Reward:     -1.0\n",
      "‚îÇ Done:       False\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ Q-UPDATE #16 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Transition: S3 --A0--> S2 (R=-1.0)\n",
      "‚îÇ Done:       False\n",
      "‚îÇ Next Q's:   [-0.11, -0.10]\n",
      "‚îÇ Target:     -1 + 0.9√ó-0.100 = -1.090\n",
      "‚îÇ Q-Update:   0.000 + 0.1√ó-1.090 = -0.109\n",
      "‚îÇ Change:     -0.109\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "üìç STEP  8 - Current State: 2\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ACTION SELECTION (State 2) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Q-values: [-0.11, -0.10]\n",
      "‚îÇ Best:     Action 1 (Q=-0.100)\n",
      "‚îÇ Decision: EXPLORE üé≤ (0.177 < 0.3)\n",
      "‚îÇ Chosen:   Action 0 (random)\n",
      "‚îÇ Stats:    Explore 5/17 (29.4%)\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ENVIRONMENT RESPONSE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Action:     0 (LEFT)\n",
      "‚îÇ Transition: 2 ‚Üí 1\n",
      "‚îÇ Reward:     -1.0\n",
      "‚îÇ Done:       False\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ Q-UPDATE #17 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Transition: S2 --A0--> S1 (R=-1.0)\n",
      "‚îÇ Done:       False\n",
      "‚îÇ Next Q's:   [-0.22, -0.28]\n",
      "‚îÇ Target:     -1 + 0.9√ó-0.223 = -1.201\n",
      "‚îÇ Q-Update:   -0.109 + 0.1√ó-1.092 = -0.218\n",
      "‚îÇ Change:     -0.109\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "üìç STEP  9 - Current State: 1\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ACTION SELECTION (State 1) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Q-values: [-0.22, -0.28]\n",
      "‚îÇ Best:     Action 0 (Q=-0.223)\n",
      "‚îÇ Decision: EXPLOIT üéØ (0.674 ‚â• 0.3)\n",
      "‚îÇ Chosen:   Action 0 (greedy)\n",
      "‚îÇ Stats:    Explore 5/18 (27.8%)\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ENVIRONMENT RESPONSE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Action:     0 (LEFT)\n",
      "‚îÇ Transition: 1 ‚Üí 0\n",
      "‚îÇ Reward:     -1.0\n",
      "‚îÇ Done:       False\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ Q-UPDATE #18 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Transition: S1 --A0--> S0 (R=-1.0)\n",
      "‚îÇ Done:       False\n",
      "‚îÇ Next Q's:   [-0.97, -0.37]\n",
      "‚îÇ Target:     -1 + 0.9√ó-0.370 = -1.333\n",
      "‚îÇ Q-Update:   -0.223 + 0.1√ó-1.109 = -0.334\n",
      "‚îÇ Change:     -0.111\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "üìç STEP 10 - Current State: 0\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ACTION SELECTION (State 0) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Q-values: [-0.97, -0.37]\n",
      "‚îÇ Best:     Action 1 (Q=-0.370)\n",
      "‚îÇ Decision: EXPLOIT üéØ (0.517 ‚â• 0.3)\n",
      "‚îÇ Chosen:   Action 1 (greedy)\n",
      "‚îÇ Stats:    Explore 5/19 (26.3%)\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ENVIRONMENT RESPONSE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Action:     1 (RIGHT)\n",
      "‚îÇ Transition: 0 ‚Üí 1\n",
      "‚îÇ Reward:     -1.0\n",
      "‚îÇ Done:       False\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ Q-UPDATE #19 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Transition: S0 --A1--> S1 (R=-1.0)\n",
      "‚îÇ Done:       False\n",
      "‚îÇ Next Q's:   [-0.33, -0.28]\n",
      "‚îÇ Target:     -1 + 0.9√ó-0.280 = -1.252\n",
      "‚îÇ Q-Update:   -0.370 + 0.1√ó-0.882 = -0.458\n",
      "‚îÇ Change:     -0.088\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "‚îå‚îÄ EPISODE 2 SUMMARY ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Status:     TIMEOUT ‚è∞\n",
      "‚îÇ Steps:      10\n",
      "‚îÇ Reward:     -14.0\n",
      "‚îÇ Goal:       Not reached\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "‚îå‚îÄ Q-TABLE AFTER EPISODE 2 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "        A0     A1  \n",
      "S0 ‚îÇ -0.97 -0.46 \n",
      "S1 ‚îÇ -0.33 -0.28 \n",
      "S2 ‚îÇ -0.22 -0.10 \n",
      "S3 ‚îÇ -0.11  1.00 \n",
      "S4 ‚îÇ  0.00  0.00 \n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "‚îå‚îÄ CURRENT POLICY ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ State 0: Action 1 (Q=-0.458)\n",
      "‚îÇ State 1: Action 1 (Q=-0.280)\n",
      "‚îÇ State 2: Action 1 (Q=-0.100)\n",
      "‚îÇ State 3: Action 1 (Q=1.000)\n",
      "‚îÇ State 4: Action 0 (Q=0.000)\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "============================================================\n",
      "üé¨ EPISODE  3/10 - 13:43:35\n",
      "============================================================\n",
      "\n",
      "üìç STEP  1 - Current State: 0\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ACTION SELECTION (State 0) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Q-values: [-0.97, -0.46]\n",
      "‚îÇ Best:     Action 1 (Q=-0.458)\n",
      "‚îÇ Decision: EXPLOIT üéØ (0.682 ‚â• 0.3)\n",
      "‚îÇ Chosen:   Action 1 (greedy)\n",
      "‚îÇ Stats:    Explore 5/20 (25.0%)\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ENVIRONMENT RESPONSE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Action:     1 (RIGHT)\n",
      "‚îÇ Transition: 0 ‚Üí 1\n",
      "‚îÇ Reward:     -1.0\n",
      "‚îÇ Done:       False\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ Q-UPDATE #20 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Transition: S0 --A1--> S1 (R=-1.0)\n",
      "‚îÇ Done:       False\n",
      "‚îÇ Next Q's:   [-0.33, -0.28]\n",
      "‚îÇ Target:     -1 + 0.9√ó-0.280 = -1.252\n",
      "‚îÇ Q-Update:   -0.458 + 0.1√ó-0.794 = -0.537\n",
      "‚îÇ Change:     -0.079\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "üìç STEP  2 - Current State: 1\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ACTION SELECTION (State 1) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Q-values: [-0.33, -0.28]\n",
      "‚îÇ Best:     Action 1 (Q=-0.280)\n",
      "‚îÇ Decision: EXPLORE üé≤ (0.036 < 0.3)\n",
      "‚îÇ Chosen:   Action 1 (random)\n",
      "‚îÇ Stats:    Explore 6/21 (28.6%)\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ENVIRONMENT RESPONSE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Action:     1 (RIGHT)\n",
      "‚îÇ Transition: 1 ‚Üí 2\n",
      "‚îÇ Reward:     -1.0\n",
      "‚îÇ Done:       False\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ Q-UPDATE #21 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Transition: S1 --A1--> S2 (R=-1.0)\n",
      "‚îÇ Done:       False\n",
      "‚îÇ Next Q's:   [-0.22, -0.10]\n",
      "‚îÇ Target:     -1 + 0.9√ó-0.100 = -1.090\n",
      "‚îÇ Q-Update:   -0.280 + 0.1√ó-0.810 = -0.361\n",
      "‚îÇ Change:     -0.081\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "üìç STEP  3 - Current State: 2\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ACTION SELECTION (State 2) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Q-values: [-0.22, -0.10]\n",
      "‚îÇ Best:     Action 1 (Q=-0.100)\n",
      "‚îÇ Decision: EXPLOIT üéØ (0.567 ‚â• 0.3)\n",
      "‚îÇ Chosen:   Action 1 (greedy)\n",
      "‚îÇ Stats:    Explore 6/22 (27.3%)\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ENVIRONMENT RESPONSE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Action:     1 (RIGHT)\n",
      "‚îÇ Transition: 2 ‚Üí 3\n",
      "‚îÇ Reward:     -1.0\n",
      "‚îÇ Done:       False\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ Q-UPDATE #22 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Transition: S2 --A1--> S3 (R=-1.0)\n",
      "‚îÇ Done:       False\n",
      "‚îÇ Next Q's:   [-0.11, 1.00]\n",
      "‚îÇ Target:     -1 + 0.9√ó1.000 = -0.100\n",
      "‚îÇ Q-Update:   -0.100 + 0.1√ó0.000 = -0.100\n",
      "‚îÇ Change:     +0.000\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "üìç STEP  4 - Current State: 3\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ACTION SELECTION (State 3) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Q-values: [-0.11, 1.00]\n",
      "‚îÇ Best:     Action 1 (Q=1.000)\n",
      "‚îÇ Decision: EXPLORE üé≤ (0.265 < 0.3)\n",
      "‚îÇ Chosen:   Action 0 (random)\n",
      "‚îÇ Stats:    Explore 7/23 (30.4%)\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ENVIRONMENT RESPONSE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Action:     0 (LEFT)\n",
      "‚îÇ Transition: 3 ‚Üí 2\n",
      "‚îÇ Reward:     -1.0\n",
      "‚îÇ Done:       False\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ Q-UPDATE #23 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Transition: S3 --A0--> S2 (R=-1.0)\n",
      "‚îÇ Done:       False\n",
      "‚îÇ Next Q's:   [-0.22, -0.10]\n",
      "‚îÇ Target:     -1 + 0.9√ó-0.100 = -1.090\n",
      "‚îÇ Q-Update:   -0.109 + 0.1√ó-0.981 = -0.207\n",
      "‚îÇ Change:     -0.098\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "üìç STEP  5 - Current State: 2\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ACTION SELECTION (State 2) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Q-values: [-0.22, -0.10]\n",
      "‚îÇ Best:     Action 1 (Q=-0.100)\n",
      "‚îÇ Decision: EXPLOIT üéØ (0.924 ‚â• 0.3)\n",
      "‚îÇ Chosen:   Action 1 (greedy)\n",
      "‚îÇ Stats:    Explore 7/24 (29.2%)\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ENVIRONMENT RESPONSE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Action:     1 (RIGHT)\n",
      "‚îÇ Transition: 2 ‚Üí 3\n",
      "‚îÇ Reward:     -1.0\n",
      "‚îÇ Done:       False\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ Q-UPDATE #24 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Transition: S2 --A1--> S3 (R=-1.0)\n",
      "‚îÇ Done:       False\n",
      "‚îÇ Next Q's:   [-0.21, 1.00]\n",
      "‚îÇ Target:     -1 + 0.9√ó1.000 = -0.100\n",
      "‚îÇ Q-Update:   -0.100 + 0.1√ó0.000 = -0.100\n",
      "‚îÇ Change:     +0.000\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "üìç STEP  6 - Current State: 3\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ACTION SELECTION (State 3) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Q-values: [-0.21, 1.00]\n",
      "‚îÇ Best:     Action 1 (Q=1.000)\n",
      "‚îÇ Decision: EXPLOIT üéØ (0.872 ‚â• 0.3)\n",
      "‚îÇ Chosen:   Action 1 (greedy)\n",
      "‚îÇ Stats:    Explore 7/25 (28.0%)\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ENVIRONMENT RESPONSE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Action:     1 (RIGHT)\n",
      "‚îÇ Transition: 3 ‚Üí 4\n",
      "‚îÇ Reward:     +10.0\n",
      "‚îÇ Done:       True\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ Q-UPDATE #25 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Transition: S3 --A1--> S4 (R=+10.0)\n",
      "‚îÇ Done:       True\n",
      "‚îÇ Target:     10.000 (episode ended, no future)\n",
      "‚îÇ Q-Update:   1.000 + 0.1√ó9.000 = 1.900\n",
      "‚îÇ Change:     +0.900\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "‚îå‚îÄ EPISODE 3 SUMMARY ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Status:     SUCCESS ‚úÖ\n",
      "‚îÇ Steps:      6\n",
      "‚îÇ Reward:     +5.0\n",
      "‚îÇ Goal:       Reached\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "‚îå‚îÄ Q-TABLE AFTER EPISODE 3 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "        A0     A1  \n",
      "S0 ‚îÇ -0.97 -0.54 \n",
      "S1 ‚îÇ -0.33 -0.36 \n",
      "S2 ‚îÇ -0.22 -0.10 \n",
      "S3 ‚îÇ -0.21  1.90 \n",
      "S4 ‚îÇ  0.00  0.00 \n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "‚îå‚îÄ CURRENT POLICY ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ State 0: Action 1 (Q=-0.537)\n",
      "‚îÇ State 1: Action 0 (Q=-0.334)\n",
      "‚îÇ State 2: Action 1 (Q=-0.100)\n",
      "‚îÇ State 3: Action 1 (Q=1.900)\n",
      "‚îÇ State 4: Action 0 (Q=0.000)\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "============================================================\n",
      "üé¨ EPISODE  4/10 - 13:43:35\n",
      "============================================================\n",
      "\n",
      "üìç STEP  1 - Current State: 0\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ACTION SELECTION (State 0) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Q-values: [-0.97, -0.54]\n",
      "‚îÇ Best:     Action 1 (Q=-0.537)\n",
      "‚îÇ Decision: EXPLOIT üéØ (0.747 ‚â• 0.3)\n",
      "‚îÇ Chosen:   Action 1 (greedy)\n",
      "‚îÇ Stats:    Explore 7/26 (26.9%)\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ENVIRONMENT RESPONSE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Action:     1 (RIGHT)\n",
      "‚îÇ Transition: 0 ‚Üí 1\n",
      "‚îÇ Reward:     -1.0\n",
      "‚îÇ Done:       False\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ Q-UPDATE #26 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Transition: S0 --A1--> S1 (R=-1.0)\n",
      "‚îÇ Done:       False\n",
      "‚îÇ Next Q's:   [-0.33, -0.36]\n",
      "‚îÇ Target:     -1 + 0.9√ó-0.334 = -1.301\n",
      "‚îÇ Q-Update:   -0.537 + 0.1√ó-0.763 = -0.614\n",
      "‚îÇ Change:     -0.076\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "üìç STEP  2 - Current State: 1\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ACTION SELECTION (State 1) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Q-values: [-0.33, -0.36]\n",
      "‚îÇ Best:     Action 0 (Q=-0.334)\n",
      "‚îÇ Decision: EXPLOIT üéØ (0.716 ‚â• 0.3)\n",
      "‚îÇ Chosen:   Action 0 (greedy)\n",
      "‚îÇ Stats:    Explore 7/27 (25.9%)\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ENVIRONMENT RESPONSE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Action:     0 (LEFT)\n",
      "‚îÇ Transition: 1 ‚Üí 0\n",
      "‚îÇ Reward:     -1.0\n",
      "‚îÇ Done:       False\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ Q-UPDATE #27 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Transition: S1 --A0--> S0 (R=-1.0)\n",
      "‚îÇ Done:       False\n",
      "‚îÇ Next Q's:   [-0.97, -0.61]\n",
      "‚îÇ Target:     -1 + 0.9√ó-0.614 = -1.552\n",
      "‚îÇ Q-Update:   -0.334 + 0.1√ó-1.218 = -0.456\n",
      "‚îÇ Change:     -0.122\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "üìç STEP  3 - Current State: 0\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ACTION SELECTION (State 0) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Q-values: [-0.97, -0.61]\n",
      "‚îÇ Best:     Action 1 (Q=-0.614)\n",
      "‚îÇ Decision: EXPLOIT üéØ (0.890 ‚â• 0.3)\n",
      "‚îÇ Chosen:   Action 1 (greedy)\n",
      "‚îÇ Stats:    Explore 7/28 (25.0%)\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ENVIRONMENT RESPONSE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Action:     1 (RIGHT)\n",
      "‚îÇ Transition: 0 ‚Üí 1\n",
      "‚îÇ Reward:     -1.0\n",
      "‚îÇ Done:       False\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ Q-UPDATE #28 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Transition: S0 --A1--> S1 (R=-1.0)\n",
      "‚îÇ Done:       False\n",
      "‚îÇ Next Q's:   [-0.46, -0.36]\n",
      "‚îÇ Target:     -1 + 0.9√ó-0.361 = -1.325\n",
      "‚îÇ Q-Update:   -0.614 + 0.1√ó-0.711 = -0.685\n",
      "‚îÇ Change:     -0.071\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "üìç STEP  4 - Current State: 1\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ACTION SELECTION (State 1) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Q-values: [-0.46, -0.36]\n",
      "‚îÇ Best:     Action 1 (Q=-0.361)\n",
      "‚îÇ Decision: EXPLOIT üéØ (0.582 ‚â• 0.3)\n",
      "‚îÇ Chosen:   Action 1 (greedy)\n",
      "‚îÇ Stats:    Explore 7/29 (24.1%)\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ENVIRONMENT RESPONSE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Action:     1 (RIGHT)\n",
      "‚îÇ Transition: 1 ‚Üí 2\n",
      "‚îÇ Reward:     -1.0\n",
      "‚îÇ Done:       False\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ Q-UPDATE #29 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Transition: S1 --A1--> S2 (R=-1.0)\n",
      "‚îÇ Done:       False\n",
      "‚îÇ Next Q's:   [-0.22, -0.10]\n",
      "‚îÇ Target:     -1 + 0.9√ó-0.100 = -1.090\n",
      "‚îÇ Q-Update:   -0.361 + 0.1√ó-0.729 = -0.434\n",
      "‚îÇ Change:     -0.073\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "üìç STEP  5 - Current State: 2\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ACTION SELECTION (State 2) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Q-values: [-0.22, -0.10]\n",
      "‚îÇ Best:     Action 1 (Q=-0.100)\n",
      "‚îÇ Decision: EXPLOIT üéØ (0.496 ‚â• 0.3)\n",
      "‚îÇ Chosen:   Action 1 (greedy)\n",
      "‚îÇ Stats:    Explore 7/30 (23.3%)\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ENVIRONMENT RESPONSE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Action:     1 (RIGHT)\n",
      "‚îÇ Transition: 2 ‚Üí 3\n",
      "‚îÇ Reward:     -1.0\n",
      "‚îÇ Done:       False\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ Q-UPDATE #30 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Transition: S2 --A1--> S3 (R=-1.0)\n",
      "‚îÇ Done:       False\n",
      "‚îÇ Next Q's:   [-0.21, 1.90]\n",
      "‚îÇ Target:     -1 + 0.9√ó1.900 = 0.710\n",
      "‚îÇ Q-Update:   -0.100 + 0.1√ó0.810 = -0.019\n",
      "‚îÇ Change:     +0.081\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "üìç STEP  6 - Current State: 3\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ACTION SELECTION (State 3) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Q-values: [-0.21, 1.90]\n",
      "‚îÇ Best:     Action 1 (Q=1.900)\n",
      "‚îÇ Decision: EXPLOIT üéØ (0.831 ‚â• 0.3)\n",
      "‚îÇ Chosen:   Action 1 (greedy)\n",
      "‚îÇ Stats:    Explore 7/31 (22.6%)\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ENVIRONMENT RESPONSE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Action:     1 (RIGHT)\n",
      "‚îÇ Transition: 3 ‚Üí 4\n",
      "‚îÇ Reward:     +10.0\n",
      "‚îÇ Done:       True\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ Q-UPDATE #31 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Transition: S3 --A1--> S4 (R=+10.0)\n",
      "‚îÇ Done:       True\n",
      "‚îÇ Target:     10.000 (episode ended, no future)\n",
      "‚îÇ Q-Update:   1.900 + 0.1√ó8.100 = 2.710\n",
      "‚îÇ Change:     +0.810\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "‚îå‚îÄ EPISODE 4 SUMMARY ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Status:     SUCCESS ‚úÖ\n",
      "‚îÇ Steps:      6\n",
      "‚îÇ Reward:     +5.0\n",
      "‚îÇ Goal:       Reached\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "‚îå‚îÄ Q-TABLE AFTER EPISODE 4 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "        A0     A1  \n",
      "S0 ‚îÇ -0.97 -0.68 \n",
      "S1 ‚îÇ -0.46 -0.43 \n",
      "S2 ‚îÇ -0.22 -0.02 \n",
      "S3 ‚îÇ -0.21  2.71 \n",
      "S4 ‚îÇ  0.00  0.00 \n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "‚îå‚îÄ CURRENT POLICY ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ State 0: Action 1 (Q=-0.685)\n",
      "‚îÇ State 1: Action 1 (Q=-0.434)\n",
      "‚îÇ State 2: Action 1 (Q=-0.019)\n",
      "‚îÇ State 3: Action 1 (Q=2.710)\n",
      "‚îÇ State 4: Action 0 (Q=0.000)\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "============================================================\n",
      "üé¨ EPISODE  5/10 - 13:43:35\n",
      "============================================================\n",
      "\n",
      "üìç STEP  1 - Current State: 0\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ACTION SELECTION (State 0) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Q-values: [-0.97, -0.68]\n",
      "‚îÇ Best:     Action 1 (Q=-0.685)\n",
      "‚îÇ Decision: EXPLORE üé≤ (0.109 < 0.3)\n",
      "‚îÇ Chosen:   Action 1 (random)\n",
      "‚îÇ Stats:    Explore 8/32 (25.0%)\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ENVIRONMENT RESPONSE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Action:     1 (RIGHT)\n",
      "‚îÇ Transition: 0 ‚Üí 1\n",
      "‚îÇ Reward:     -1.0\n",
      "‚îÇ Done:       False\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ Q-UPDATE #32 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Transition: S0 --A1--> S1 (R=-1.0)\n",
      "‚îÇ Done:       False\n",
      "‚îÇ Next Q's:   [-0.46, -0.43]\n",
      "‚îÇ Target:     -1 + 0.9√ó-0.434 = -1.391\n",
      "‚îÇ Q-Update:   -0.685 + 0.1√ó-0.706 = -0.755\n",
      "‚îÇ Change:     -0.071\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "üìç STEP  2 - Current State: 1\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ACTION SELECTION (State 1) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Q-values: [-0.46, -0.43]\n",
      "‚îÇ Best:     Action 1 (Q=-0.434)\n",
      "‚îÇ Decision: EXPLOIT üéØ (0.512 ‚â• 0.3)\n",
      "‚îÇ Chosen:   Action 1 (greedy)\n",
      "‚îÇ Stats:    Explore 8/33 (24.2%)\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ENVIRONMENT RESPONSE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Action:     1 (RIGHT)\n",
      "‚îÇ Transition: 1 ‚Üí 2\n",
      "‚îÇ Reward:     -1.0\n",
      "‚îÇ Done:       False\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ Q-UPDATE #33 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Transition: S1 --A1--> S2 (R=-1.0)\n",
      "‚îÇ Done:       False\n",
      "‚îÇ Next Q's:   [-0.22, -0.02]\n",
      "‚îÇ Target:     -1 + 0.9√ó-0.019 = -1.017\n",
      "‚îÇ Q-Update:   -0.434 + 0.1√ó-0.583 = -0.492\n",
      "‚îÇ Change:     -0.058\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "üìç STEP  3 - Current State: 2\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ACTION SELECTION (State 2) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Q-values: [-0.22, -0.02]\n",
      "‚îÇ Best:     Action 1 (Q=-0.019)\n",
      "‚îÇ Decision: EXPLORE üé≤ (0.279 < 0.3)\n",
      "‚îÇ Chosen:   Action 0 (random)\n",
      "‚îÇ Stats:    Explore 9/34 (26.5%)\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ENVIRONMENT RESPONSE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Action:     0 (LEFT)\n",
      "‚îÇ Transition: 2 ‚Üí 1\n",
      "‚îÇ Reward:     -1.0\n",
      "‚îÇ Done:       False\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ Q-UPDATE #34 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Transition: S2 --A0--> S1 (R=-1.0)\n",
      "‚îÇ Done:       False\n",
      "‚îÇ Next Q's:   [-0.46, -0.49]\n",
      "‚îÇ Target:     -1 + 0.9√ó-0.456 = -1.411\n",
      "‚îÇ Q-Update:   -0.218 + 0.1√ó-1.192 = -0.337\n",
      "‚îÇ Change:     -0.119\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "üìç STEP  4 - Current State: 1\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ACTION SELECTION (State 1) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Q-values: [-0.46, -0.49]\n",
      "‚îÇ Best:     Action 0 (Q=-0.456)\n",
      "‚îÇ Decision: EXPLORE üé≤ (0.110 < 0.3)\n",
      "‚îÇ Chosen:   Action 0 (random)\n",
      "‚îÇ Stats:    Explore 10/35 (28.6%)\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ENVIRONMENT RESPONSE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Action:     0 (LEFT)\n",
      "‚îÇ Transition: 1 ‚Üí 0\n",
      "‚îÇ Reward:     -1.0\n",
      "‚îÇ Done:       False\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ Q-UPDATE #35 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Transition: S1 --A0--> S0 (R=-1.0)\n",
      "‚îÇ Done:       False\n",
      "‚îÇ Next Q's:   [-0.97, -0.76]\n",
      "‚îÇ Target:     -1 + 0.9√ó-0.755 = -1.680\n",
      "‚îÇ Q-Update:   -0.456 + 0.1√ó-1.224 = -0.579\n",
      "‚îÇ Change:     -0.122\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "üìç STEP  5 - Current State: 0\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ACTION SELECTION (State 0) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Q-values: [-0.97, -0.76]\n",
      "‚îÇ Best:     Action 1 (Q=-0.755)\n",
      "‚îÇ Decision: EXPLOIT üéØ (0.312 ‚â• 0.3)\n",
      "‚îÇ Chosen:   Action 1 (greedy)\n",
      "‚îÇ Stats:    Explore 10/36 (27.8%)\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ENVIRONMENT RESPONSE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Action:     1 (RIGHT)\n",
      "‚îÇ Transition: 0 ‚Üí 1\n",
      "‚îÇ Reward:     -1.0\n",
      "‚îÇ Done:       False\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ Q-UPDATE #36 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Transition: S0 --A1--> S1 (R=-1.0)\n",
      "‚îÇ Done:       False\n",
      "‚îÇ Next Q's:   [-0.58, -0.49]\n",
      "‚îÇ Target:     -1 + 0.9√ó-0.492 = -1.443\n",
      "‚îÇ Q-Update:   -0.755 + 0.1√ó-0.688 = -0.824\n",
      "‚îÇ Change:     -0.069\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "üìç STEP  6 - Current State: 1\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ACTION SELECTION (State 1) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Q-values: [-0.58, -0.49]\n",
      "‚îÇ Best:     Action 1 (Q=-0.492)\n",
      "‚îÇ Decision: EXPLORE üé≤ (0.272 < 0.3)\n",
      "‚îÇ Chosen:   Action 0 (random)\n",
      "‚îÇ Stats:    Explore 11/37 (29.7%)\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ENVIRONMENT RESPONSE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Action:     0 (LEFT)\n",
      "‚îÇ Transition: 1 ‚Üí 0\n",
      "‚îÇ Reward:     -1.0\n",
      "‚îÇ Done:       False\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ Q-UPDATE #37 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Transition: S1 --A0--> S0 (R=-1.0)\n",
      "‚îÇ Done:       False\n",
      "‚îÇ Next Q's:   [-0.97, -0.82]\n",
      "‚îÇ Target:     -1 + 0.9√ó-0.824 = -1.742\n",
      "‚îÇ Q-Update:   -0.579 + 0.1√ó-1.163 = -0.695\n",
      "‚îÇ Change:     -0.116\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "üìç STEP  7 - Current State: 0\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ACTION SELECTION (State 0) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Q-values: [-0.97, -0.82]\n",
      "‚îÇ Best:     Action 1 (Q=-0.824)\n",
      "‚îÇ Decision: EXPLOIT üéØ (0.617 ‚â• 0.3)\n",
      "‚îÇ Chosen:   Action 1 (greedy)\n",
      "‚îÇ Stats:    Explore 11/38 (28.9%)\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ENVIRONMENT RESPONSE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Action:     1 (RIGHT)\n",
      "‚îÇ Transition: 0 ‚Üí 1\n",
      "‚îÇ Reward:     -1.0\n",
      "‚îÇ Done:       False\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ Q-UPDATE #38 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Transition: S0 --A1--> S1 (R=-1.0)\n",
      "‚îÇ Done:       False\n",
      "‚îÇ Next Q's:   [-0.69, -0.49]\n",
      "‚îÇ Target:     -1 + 0.9√ó-0.492 = -1.443\n",
      "‚îÇ Q-Update:   -0.824 + 0.1√ó-0.619 = -0.886\n",
      "‚îÇ Change:     -0.062\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "üìç STEP  8 - Current State: 1\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ACTION SELECTION (State 1) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Q-values: [-0.69, -0.49]\n",
      "‚îÇ Best:     Action 1 (Q=-0.492)\n",
      "‚îÇ Decision: EXPLOIT üéØ (0.762 ‚â• 0.3)\n",
      "‚îÇ Chosen:   Action 1 (greedy)\n",
      "‚îÇ Stats:    Explore 11/39 (28.2%)\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ENVIRONMENT RESPONSE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Action:     1 (RIGHT)\n",
      "‚îÇ Transition: 1 ‚Üí 2\n",
      "‚îÇ Reward:     -1.0\n",
      "‚îÇ Done:       False\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ Q-UPDATE #39 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Transition: S1 --A1--> S2 (R=-1.0)\n",
      "‚îÇ Done:       False\n",
      "‚îÇ Next Q's:   [-0.34, -0.02]\n",
      "‚îÇ Target:     -1 + 0.9√ó-0.019 = -1.017\n",
      "‚îÇ Q-Update:   -0.492 + 0.1√ó-0.525 = -0.545\n",
      "‚îÇ Change:     -0.052\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "üìç STEP  9 - Current State: 2\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ACTION SELECTION (State 2) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Q-values: [-0.34, -0.02]\n",
      "‚îÇ Best:     Action 1 (Q=-0.019)\n",
      "‚îÇ Decision: EXPLOIT üéØ (0.867 ‚â• 0.3)\n",
      "‚îÇ Chosen:   Action 1 (greedy)\n",
      "‚îÇ Stats:    Explore 11/40 (27.5%)\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ENVIRONMENT RESPONSE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Action:     1 (RIGHT)\n",
      "‚îÇ Transition: 2 ‚Üí 3\n",
      "‚îÇ Reward:     -1.0\n",
      "‚îÇ Done:       False\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ Q-UPDATE #40 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Transition: S2 --A1--> S3 (R=-1.0)\n",
      "‚îÇ Done:       False\n",
      "‚îÇ Next Q's:   [-0.21, 2.71]\n",
      "‚îÇ Target:     -1 + 0.9√ó2.710 = 1.439\n",
      "‚îÇ Q-Update:   -0.019 + 0.1√ó1.458 = 0.127\n",
      "‚îÇ Change:     +0.146\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "üìç STEP 10 - Current State: 3\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ACTION SELECTION (State 3) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Q-values: [-0.21, 2.71]\n",
      "‚îÇ Best:     Action 1 (Q=2.710)\n",
      "‚îÇ Decision: EXPLOIT üéØ (0.356 ‚â• 0.3)\n",
      "‚îÇ Chosen:   Action 1 (greedy)\n",
      "‚îÇ Stats:    Explore 11/41 (26.8%)\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ENVIRONMENT RESPONSE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Action:     1 (RIGHT)\n",
      "‚îÇ Transition: 3 ‚Üí 4\n",
      "‚îÇ Reward:     +10.0\n",
      "‚îÇ Done:       True\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ Q-UPDATE #41 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Transition: S3 --A1--> S4 (R=+10.0)\n",
      "‚îÇ Done:       True\n",
      "‚îÇ Target:     10.000 (episode ended, no future)\n",
      "‚îÇ Q-Update:   2.710 + 0.1√ó7.290 = 3.439\n",
      "‚îÇ Change:     +0.729\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "‚îå‚îÄ EPISODE 5 SUMMARY ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Status:     SUCCESS ‚úÖ\n",
      "‚îÇ Steps:      10\n",
      "‚îÇ Reward:     +1.0\n",
      "‚îÇ Goal:       Reached\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "‚îå‚îÄ Q-TABLE AFTER EPISODE 5 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "        A0     A1  \n",
      "S0 ‚îÇ -0.97 -0.89 \n",
      "S1 ‚îÇ -0.69 -0.54 \n",
      "S2 ‚îÇ -0.34  0.13 \n",
      "S3 ‚îÇ -0.21  3.44 \n",
      "S4 ‚îÇ  0.00  0.00 \n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "‚îå‚îÄ CURRENT POLICY ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ State 0: Action 1 (Q=-0.886)\n",
      "‚îÇ State 1: Action 1 (Q=-0.545)\n",
      "‚îÇ State 2: Action 1 (Q=0.127)\n",
      "‚îÇ State 3: Action 1 (Q=3.439)\n",
      "‚îÇ State 4: Action 0 (Q=0.000)\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "============================================================\n",
      "üé¨ EPISODE  6/10 - 13:43:35\n",
      "============================================================\n",
      "\n",
      "üìç STEP  1 - Current State: 0\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ACTION SELECTION (State 0) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Q-values: [-0.97, -0.89]\n",
      "‚îÇ Best:     Action 1 (Q=-0.886)\n",
      "‚îÇ Decision: EXPLOIT üéØ (0.567 ‚â• 0.3)\n",
      "‚îÇ Chosen:   Action 1 (greedy)\n",
      "‚îÇ Stats:    Explore 11/42 (26.2%)\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ENVIRONMENT RESPONSE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Action:     1 (RIGHT)\n",
      "‚îÇ Transition: 0 ‚Üí 1\n",
      "‚îÇ Reward:     -1.0\n",
      "‚îÇ Done:       False\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ Q-UPDATE #42 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Transition: S0 --A1--> S1 (R=-1.0)\n",
      "‚îÇ Done:       False\n",
      "‚îÇ Next Q's:   [-0.69, -0.54]\n",
      "‚îÇ Target:     -1 + 0.9√ó-0.545 = -1.490\n",
      "‚îÇ Q-Update:   -0.886 + 0.1√ó-0.604 = -0.947\n",
      "‚îÇ Change:     -0.060\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "üìç STEP  2 - Current State: 1\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ACTION SELECTION (State 1) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Q-values: [-0.69, -0.54]\n",
      "‚îÇ Best:     Action 1 (Q=-0.545)\n",
      "‚îÇ Decision: EXPLORE üé≤ (0.279 < 0.3)\n",
      "‚îÇ Chosen:   Action 0 (random)\n",
      "‚îÇ Stats:    Explore 12/43 (27.9%)\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ENVIRONMENT RESPONSE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Action:     0 (LEFT)\n",
      "‚îÇ Transition: 1 ‚Üí 0\n",
      "‚îÇ Reward:     -1.0\n",
      "‚îÇ Done:       False\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ Q-UPDATE #43 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Transition: S1 --A0--> S0 (R=-1.0)\n",
      "‚îÇ Done:       False\n",
      "‚îÇ Next Q's:   [-0.97, -0.95]\n",
      "‚îÇ Target:     -1 + 0.9√ó-0.947 = -1.852\n",
      "‚îÇ Q-Update:   -0.695 + 0.1√ó-1.157 = -0.811\n",
      "‚îÇ Change:     -0.116\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "üìç STEP  3 - Current State: 0\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ACTION SELECTION (State 0) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Q-values: [-0.97, -0.95]\n",
      "‚îÇ Best:     Action 1 (Q=-0.947)\n",
      "‚îÇ Decision: EXPLOIT üéØ (0.875 ‚â• 0.3)\n",
      "‚îÇ Chosen:   Action 1 (greedy)\n",
      "‚îÇ Stats:    Explore 12/44 (27.3%)\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ENVIRONMENT RESPONSE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Action:     1 (RIGHT)\n",
      "‚îÇ Transition: 0 ‚Üí 1\n",
      "‚îÇ Reward:     -1.0\n",
      "‚îÇ Done:       False\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ Q-UPDATE #44 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Transition: S0 --A1--> S1 (R=-1.0)\n",
      "‚îÇ Done:       False\n",
      "‚îÇ Next Q's:   [-0.81, -0.54]\n",
      "‚îÇ Target:     -1 + 0.9√ó-0.545 = -1.490\n",
      "‚îÇ Q-Update:   -0.947 + 0.1√ó-0.544 = -1.001\n",
      "‚îÇ Change:     -0.054\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "üìç STEP  4 - Current State: 1\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ACTION SELECTION (State 1) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Q-values: [-0.81, -0.54]\n",
      "‚îÇ Best:     Action 1 (Q=-0.545)\n",
      "‚îÇ Decision: EXPLORE üé≤ (0.275 < 0.3)\n",
      "‚îÇ Chosen:   Action 1 (random)\n",
      "‚îÇ Stats:    Explore 13/45 (28.9%)\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ENVIRONMENT RESPONSE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Action:     1 (RIGHT)\n",
      "‚îÇ Transition: 1 ‚Üí 2\n",
      "‚îÇ Reward:     -1.0\n",
      "‚îÇ Done:       False\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ Q-UPDATE #45 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Transition: S1 --A1--> S2 (R=-1.0)\n",
      "‚îÇ Done:       False\n",
      "‚îÇ Next Q's:   [-0.34, 0.13]\n",
      "‚îÇ Target:     -1 + 0.9√ó0.127 = -0.886\n",
      "‚îÇ Q-Update:   -0.545 + 0.1√ó-0.341 = -0.579\n",
      "‚îÇ Change:     -0.034\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "üìç STEP  5 - Current State: 2\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ACTION SELECTION (State 2) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Q-values: [-0.34, 0.13]\n",
      "‚îÇ Best:     Action 1 (Q=0.127)\n",
      "‚îÇ Decision: EXPLORE üé≤ (0.233 < 0.3)\n",
      "‚îÇ Chosen:   Action 1 (random)\n",
      "‚îÇ Stats:    Explore 14/46 (30.4%)\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ENVIRONMENT RESPONSE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Action:     1 (RIGHT)\n",
      "‚îÇ Transition: 2 ‚Üí 3\n",
      "‚îÇ Reward:     -1.0\n",
      "‚îÇ Done:       False\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ Q-UPDATE #46 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Transition: S2 --A1--> S3 (R=-1.0)\n",
      "‚îÇ Done:       False\n",
      "‚îÇ Next Q's:   [-0.21, 3.44]\n",
      "‚îÇ Target:     -1 + 0.9√ó3.439 = 2.095\n",
      "‚îÇ Q-Update:   0.127 + 0.1√ó1.968 = 0.324\n",
      "‚îÇ Change:     +0.197\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "üìç STEP  6 - Current State: 3\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ACTION SELECTION (State 3) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Q-values: [-0.21, 3.44]\n",
      "‚îÇ Best:     Action 1 (Q=3.439)\n",
      "‚îÇ Decision: EXPLOIT üéØ (0.616 ‚â• 0.3)\n",
      "‚îÇ Chosen:   Action 1 (greedy)\n",
      "‚îÇ Stats:    Explore 14/47 (29.8%)\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ENVIRONMENT RESPONSE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Action:     1 (RIGHT)\n",
      "‚îÇ Transition: 3 ‚Üí 4\n",
      "‚îÇ Reward:     +10.0\n",
      "‚îÇ Done:       True\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ Q-UPDATE #47 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Transition: S3 --A1--> S4 (R=+10.0)\n",
      "‚îÇ Done:       True\n",
      "‚îÇ Target:     10.000 (episode ended, no future)\n",
      "‚îÇ Q-Update:   3.439 + 0.1√ó6.561 = 4.095\n",
      "‚îÇ Change:     +0.656\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "‚îå‚îÄ EPISODE 6 SUMMARY ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Status:     SUCCESS ‚úÖ\n",
      "‚îÇ Steps:      6\n",
      "‚îÇ Reward:     +5.0\n",
      "‚îÇ Goal:       Reached\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "‚îå‚îÄ Q-TABLE AFTER EPISODE 6 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "        A0     A1  \n",
      "S0 ‚îÇ -0.97 -1.00 \n",
      "S1 ‚îÇ -0.81 -0.58 \n",
      "S2 ‚îÇ -0.34  0.32 \n",
      "S3 ‚îÇ -0.21  4.10 \n",
      "S4 ‚îÇ  0.00  0.00 \n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "‚îå‚îÄ CURRENT POLICY ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ State 0: Action 0 (Q=-0.967)\n",
      "‚îÇ State 1: Action 1 (Q=-0.579)\n",
      "‚îÇ State 2: Action 1 (Q=0.324)\n",
      "‚îÇ State 3: Action 1 (Q=4.095)\n",
      "‚îÇ State 4: Action 0 (Q=0.000)\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "============================================================\n",
      "üé¨ EPISODE  7/10 - 13:43:35\n",
      "============================================================\n",
      "\n",
      "üìç STEP  1 - Current State: 0\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ACTION SELECTION (State 0) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Q-values: [-0.97, -1.00]\n",
      "‚îÇ Best:     Action 0 (Q=-0.967)\n",
      "‚îÇ Decision: EXPLORE üé≤ (0.141 < 0.3)\n",
      "‚îÇ Chosen:   Action 0 (random)\n",
      "‚îÇ Stats:    Explore 15/48 (31.2%)\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ENVIRONMENT RESPONSE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Action:     0 (LEFT)\n",
      "‚îÇ Transition: 0 ‚Üí 0\n",
      "‚îÇ Reward:     -5.0\n",
      "‚îÇ Done:       False\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ Q-UPDATE #48 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Transition: S0 --A0--> S0 (R=-5.0)\n",
      "‚îÇ Done:       False\n",
      "‚îÇ Next Q's:   [-0.97, -1.00]\n",
      "‚îÇ Target:     -5 + 0.9√ó-0.967 = -5.870\n",
      "‚îÇ Q-Update:   -0.967 + 0.1√ó-4.903 = -1.457\n",
      "‚îÇ Change:     -0.490\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "üìç STEP  2 - Current State: 0\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ACTION SELECTION (State 0) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Q-values: [-1.46, -1.00]\n",
      "‚îÇ Best:     Action 1 (Q=-1.001)\n",
      "‚îÇ Decision: EXPLOIT üéØ (0.527 ‚â• 0.3)\n",
      "‚îÇ Chosen:   Action 1 (greedy)\n",
      "‚îÇ Stats:    Explore 15/49 (30.6%)\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ENVIRONMENT RESPONSE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Action:     1 (RIGHT)\n",
      "‚îÇ Transition: 0 ‚Üí 1\n",
      "‚îÇ Reward:     -1.0\n",
      "‚îÇ Done:       False\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ Q-UPDATE #49 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Transition: S0 --A1--> S1 (R=-1.0)\n",
      "‚îÇ Done:       False\n",
      "‚îÇ Next Q's:   [-0.81, -0.58]\n",
      "‚îÇ Target:     -1 + 0.9√ó-0.579 = -1.521\n",
      "‚îÇ Q-Update:   -1.001 + 0.1√ó-0.520 = -1.053\n",
      "‚îÇ Change:     -0.052\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "üìç STEP  3 - Current State: 1\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ACTION SELECTION (State 1) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Q-values: [-0.81, -0.58]\n",
      "‚îÇ Best:     Action 1 (Q=-0.579)\n",
      "‚îÇ Decision: EXPLOIT üéØ (0.494 ‚â• 0.3)\n",
      "‚îÇ Chosen:   Action 1 (greedy)\n",
      "‚îÇ Stats:    Explore 15/50 (30.0%)\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ENVIRONMENT RESPONSE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Action:     1 (RIGHT)\n",
      "‚îÇ Transition: 1 ‚Üí 2\n",
      "‚îÇ Reward:     -1.0\n",
      "‚îÇ Done:       False\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ Q-UPDATE #50 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Transition: S1 --A1--> S2 (R=-1.0)\n",
      "‚îÇ Done:       False\n",
      "‚îÇ Next Q's:   [-0.34, 0.32]\n",
      "‚îÇ Target:     -1 + 0.9√ó0.324 = -0.709\n",
      "‚îÇ Q-Update:   -0.579 + 0.1√ó-0.130 = -0.592\n",
      "‚îÇ Change:     -0.013\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "üìç STEP  4 - Current State: 2\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ACTION SELECTION (State 2) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Q-values: [-0.34, 0.32]\n",
      "‚îÇ Best:     Action 1 (Q=0.324)\n",
      "‚îÇ Decision: EXPLOIT üéØ (0.914 ‚â• 0.3)\n",
      "‚îÇ Chosen:   Action 1 (greedy)\n",
      "‚îÇ Stats:    Explore 15/51 (29.4%)\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ENVIRONMENT RESPONSE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Action:     1 (RIGHT)\n",
      "‚îÇ Transition: 2 ‚Üí 3\n",
      "‚îÇ Reward:     -1.0\n",
      "‚îÇ Done:       False\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ Q-UPDATE #51 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Transition: S2 --A1--> S3 (R=-1.0)\n",
      "‚îÇ Done:       False\n",
      "‚îÇ Next Q's:   [-0.21, 4.10]\n",
      "‚îÇ Target:     -1 + 0.9√ó4.095 = 2.686\n",
      "‚îÇ Q-Update:   0.324 + 0.1√ó2.362 = 0.560\n",
      "‚îÇ Change:     +0.236\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "üìç STEP  5 - Current State: 3\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ACTION SELECTION (State 3) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Q-values: [-0.21, 4.10]\n",
      "‚îÇ Best:     Action 1 (Q=4.095)\n",
      "‚îÇ Decision: EXPLORE üé≤ (0.027 < 0.3)\n",
      "‚îÇ Chosen:   Action 0 (random)\n",
      "‚îÇ Stats:    Explore 16/52 (30.8%)\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ENVIRONMENT RESPONSE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Action:     0 (LEFT)\n",
      "‚îÇ Transition: 3 ‚Üí 2\n",
      "‚îÇ Reward:     -1.0\n",
      "‚îÇ Done:       False\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ Q-UPDATE #52 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Transition: S3 --A0--> S2 (R=-1.0)\n",
      "‚îÇ Done:       False\n",
      "‚îÇ Next Q's:   [-0.34, 0.56]\n",
      "‚îÇ Target:     -1 + 0.9√ó0.560 = -0.496\n",
      "‚îÇ Q-Update:   -0.207 + 0.1√ó-0.289 = -0.236\n",
      "‚îÇ Change:     -0.029\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "üìç STEP  6 - Current State: 2\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ACTION SELECTION (State 2) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Q-values: [-0.34, 0.56]\n",
      "‚îÇ Best:     Action 1 (Q=0.560)\n",
      "‚îÇ Decision: EXPLOIT üéØ (0.419 ‚â• 0.3)\n",
      "‚îÇ Chosen:   Action 1 (greedy)\n",
      "‚îÇ Stats:    Explore 16/53 (30.2%)\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ENVIRONMENT RESPONSE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Action:     1 (RIGHT)\n",
      "‚îÇ Transition: 2 ‚Üí 3\n",
      "‚îÇ Reward:     -1.0\n",
      "‚îÇ Done:       False\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ Q-UPDATE #53 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Transition: S2 --A1--> S3 (R=-1.0)\n",
      "‚îÇ Done:       False\n",
      "‚îÇ Next Q's:   [-0.24, 4.10]\n",
      "‚îÇ Target:     -1 + 0.9√ó4.095 = 2.686\n",
      "‚îÇ Q-Update:   0.560 + 0.1√ó2.126 = 0.772\n",
      "‚îÇ Change:     +0.213\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "üìç STEP  7 - Current State: 3\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ACTION SELECTION (State 3) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Q-values: [-0.24, 4.10]\n",
      "‚îÇ Best:     Action 1 (Q=4.095)\n",
      "‚îÇ Decision: EXPLOIT üéØ (0.591 ‚â• 0.3)\n",
      "‚îÇ Chosen:   Action 1 (greedy)\n",
      "‚îÇ Stats:    Explore 16/54 (29.6%)\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ENVIRONMENT RESPONSE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Action:     1 (RIGHT)\n",
      "‚îÇ Transition: 3 ‚Üí 4\n",
      "‚îÇ Reward:     +10.0\n",
      "‚îÇ Done:       True\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ Q-UPDATE #54 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Transition: S3 --A1--> S4 (R=+10.0)\n",
      "‚îÇ Done:       True\n",
      "‚îÇ Target:     10.000 (episode ended, no future)\n",
      "‚îÇ Q-Update:   4.095 + 0.1√ó5.905 = 4.686\n",
      "‚îÇ Change:     +0.590\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "‚îå‚îÄ EPISODE 7 SUMMARY ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Status:     SUCCESS ‚úÖ\n",
      "‚îÇ Steps:      7\n",
      "‚îÇ Reward:     +0.0\n",
      "‚îÇ Goal:       Reached\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "‚îå‚îÄ Q-TABLE AFTER EPISODE 7 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "        A0     A1  \n",
      "S0 ‚îÇ -1.46 -1.05 \n",
      "S1 ‚îÇ -0.81 -0.59 \n",
      "S2 ‚îÇ -0.34  0.77 \n",
      "S3 ‚îÇ -0.24  4.69 \n",
      "S4 ‚îÇ  0.00  0.00 \n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "‚îå‚îÄ CURRENT POLICY ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ State 0: Action 1 (Q=-1.053)\n",
      "‚îÇ State 1: Action 1 (Q=-0.592)\n",
      "‚îÇ State 2: Action 1 (Q=0.772)\n",
      "‚îÇ State 3: Action 1 (Q=4.686)\n",
      "‚îÇ State 4: Action 0 (Q=0.000)\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "============================================================\n",
      "üé¨ EPISODE  8/10 - 13:43:35\n",
      "============================================================\n",
      "\n",
      "üìç STEP  1 - Current State: 0\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ACTION SELECTION (State 0) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Q-values: [-1.46, -1.05]\n",
      "‚îÇ Best:     Action 1 (Q=-1.053)\n",
      "‚îÇ Decision: EXPLOIT üéØ (0.464 ‚â• 0.3)\n",
      "‚îÇ Chosen:   Action 1 (greedy)\n",
      "‚îÇ Stats:    Explore 16/55 (29.1%)\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ENVIRONMENT RESPONSE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Action:     1 (RIGHT)\n",
      "‚îÇ Transition: 0 ‚Üí 1\n",
      "‚îÇ Reward:     -1.0\n",
      "‚îÇ Done:       False\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ Q-UPDATE #55 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Transition: S0 --A1--> S1 (R=-1.0)\n",
      "‚îÇ Done:       False\n",
      "‚îÇ Next Q's:   [-0.81, -0.59]\n",
      "‚îÇ Target:     -1 + 0.9√ó-0.592 = -1.533\n",
      "‚îÇ Q-Update:   -1.053 + 0.1√ó-0.480 = -1.101\n",
      "‚îÇ Change:     -0.048\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "üìç STEP  2 - Current State: 1\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ACTION SELECTION (State 1) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Q-values: [-0.81, -0.59]\n",
      "‚îÇ Best:     Action 1 (Q=-0.592)\n",
      "‚îÇ Decision: EXPLOIT üéØ (0.782 ‚â• 0.3)\n",
      "‚îÇ Chosen:   Action 1 (greedy)\n",
      "‚îÇ Stats:    Explore 16/56 (28.6%)\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ENVIRONMENT RESPONSE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Action:     1 (RIGHT)\n",
      "‚îÇ Transition: 1 ‚Üí 2\n",
      "‚îÇ Reward:     -1.0\n",
      "‚îÇ Done:       False\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ Q-UPDATE #56 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Transition: S1 --A1--> S2 (R=-1.0)\n",
      "‚îÇ Done:       False\n",
      "‚îÇ Next Q's:   [-0.34, 0.77]\n",
      "‚îÇ Target:     -1 + 0.9√ó0.772 = -0.305\n",
      "‚îÇ Q-Update:   -0.592 + 0.1√ó0.287 = -0.563\n",
      "‚îÇ Change:     +0.029\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "üìç STEP  3 - Current State: 2\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ACTION SELECTION (State 2) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Q-values: [-0.34, 0.77]\n",
      "‚îÇ Best:     Action 1 (Q=0.772)\n",
      "‚îÇ Decision: EXPLORE üé≤ (0.012 < 0.3)\n",
      "‚îÇ Chosen:   Action 0 (random)\n",
      "‚îÇ Stats:    Explore 17/57 (29.8%)\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ENVIRONMENT RESPONSE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Action:     0 (LEFT)\n",
      "‚îÇ Transition: 2 ‚Üí 1\n",
      "‚îÇ Reward:     -1.0\n",
      "‚îÇ Done:       False\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ Q-UPDATE #57 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Transition: S2 --A0--> S1 (R=-1.0)\n",
      "‚îÇ Done:       False\n",
      "‚îÇ Next Q's:   [-0.81, -0.56]\n",
      "‚îÇ Target:     -1 + 0.9√ó-0.563 = -1.507\n",
      "‚îÇ Q-Update:   -0.337 + 0.1√ó-1.169 = -0.454\n",
      "‚îÇ Change:     -0.117\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "üìç STEP  4 - Current State: 1\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ACTION SELECTION (State 1) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Q-values: [-0.81, -0.56]\n",
      "‚îÇ Best:     Action 1 (Q=-0.563)\n",
      "‚îÇ Decision: EXPLOIT üéØ (0.785 ‚â• 0.3)\n",
      "‚îÇ Chosen:   Action 1 (greedy)\n",
      "‚îÇ Stats:    Explore 17/58 (29.3%)\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ENVIRONMENT RESPONSE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Action:     1 (RIGHT)\n",
      "‚îÇ Transition: 1 ‚Üí 2\n",
      "‚îÇ Reward:     -1.0\n",
      "‚îÇ Done:       False\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ Q-UPDATE #58 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Transition: S1 --A1--> S2 (R=-1.0)\n",
      "‚îÇ Done:       False\n",
      "‚îÇ Next Q's:   [-0.45, 0.77]\n",
      "‚îÇ Target:     -1 + 0.9√ó0.772 = -0.305\n",
      "‚îÇ Q-Update:   -0.563 + 0.1√ó0.258 = -0.537\n",
      "‚îÇ Change:     +0.026\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "üìç STEP  5 - Current State: 2\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ACTION SELECTION (State 2) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Q-values: [-0.45, 0.77]\n",
      "‚îÇ Best:     Action 1 (Q=0.772)\n",
      "‚îÇ Decision: EXPLOIT üéØ (0.852 ‚â• 0.3)\n",
      "‚îÇ Chosen:   Action 1 (greedy)\n",
      "‚îÇ Stats:    Explore 17/59 (28.8%)\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ENVIRONMENT RESPONSE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Action:     1 (RIGHT)\n",
      "‚îÇ Transition: 2 ‚Üí 3\n",
      "‚îÇ Reward:     -1.0\n",
      "‚îÇ Done:       False\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ Q-UPDATE #59 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Transition: S2 --A1--> S3 (R=-1.0)\n",
      "‚îÇ Done:       False\n",
      "‚îÇ Next Q's:   [-0.24, 4.69]\n",
      "‚îÇ Target:     -1 + 0.9√ó4.686 = 3.217\n",
      "‚îÇ Q-Update:   0.772 + 0.1√ó2.445 = 1.017\n",
      "‚îÇ Change:     +0.244\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "üìç STEP  6 - Current State: 3\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ACTION SELECTION (State 3) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Q-values: [-0.24, 4.69]\n",
      "‚îÇ Best:     Action 1 (Q=4.686)\n",
      "‚îÇ Decision: EXPLORE üé≤ (0.204 < 0.3)\n",
      "‚îÇ Chosen:   Action 1 (random)\n",
      "‚îÇ Stats:    Explore 18/60 (30.0%)\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ENVIRONMENT RESPONSE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Action:     1 (RIGHT)\n",
      "‚îÇ Transition: 3 ‚Üí 4\n",
      "‚îÇ Reward:     +10.0\n",
      "‚îÇ Done:       True\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ Q-UPDATE #60 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Transition: S3 --A1--> S4 (R=+10.0)\n",
      "‚îÇ Done:       True\n",
      "‚îÇ Target:     10.000 (episode ended, no future)\n",
      "‚îÇ Q-Update:   4.686 + 0.1√ó5.314 = 5.217\n",
      "‚îÇ Change:     +0.531\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "‚îå‚îÄ EPISODE 8 SUMMARY ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Status:     SUCCESS ‚úÖ\n",
      "‚îÇ Steps:      6\n",
      "‚îÇ Reward:     +5.0\n",
      "‚îÇ Goal:       Reached\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "‚îå‚îÄ Q-TABLE AFTER EPISODE 8 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "        A0     A1  \n",
      "S0 ‚îÇ -1.46 -1.10 \n",
      "S1 ‚îÇ -0.81 -0.54 \n",
      "S2 ‚îÇ -0.45  1.02 \n",
      "S3 ‚îÇ -0.24  5.22 \n",
      "S4 ‚îÇ  0.00  0.00 \n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "‚îå‚îÄ CURRENT POLICY ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ State 0: Action 1 (Q=-1.101)\n",
      "‚îÇ State 1: Action 1 (Q=-0.537)\n",
      "‚îÇ State 2: Action 1 (Q=1.017)\n",
      "‚îÇ State 3: Action 1 (Q=5.217)\n",
      "‚îÇ State 4: Action 0 (Q=0.000)\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "============================================================\n",
      "üé¨ EPISODE  9/10 - 13:43:35\n",
      "============================================================\n",
      "\n",
      "üìç STEP  1 - Current State: 0\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ACTION SELECTION (State 0) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Q-values: [-1.46, -1.10]\n",
      "‚îÇ Best:     Action 1 (Q=-1.101)\n",
      "‚îÇ Decision: EXPLOIT üéØ (0.515 ‚â• 0.3)\n",
      "‚îÇ Chosen:   Action 1 (greedy)\n",
      "‚îÇ Stats:    Explore 18/61 (29.5%)\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ENVIRONMENT RESPONSE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Action:     1 (RIGHT)\n",
      "‚îÇ Transition: 0 ‚Üí 1\n",
      "‚îÇ Reward:     -1.0\n",
      "‚îÇ Done:       False\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ Q-UPDATE #61 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Transition: S0 --A1--> S1 (R=-1.0)\n",
      "‚îÇ Done:       False\n",
      "‚îÇ Next Q's:   [-0.81, -0.54]\n",
      "‚îÇ Target:     -1 + 0.9√ó-0.537 = -1.484\n",
      "‚îÇ Q-Update:   -1.101 + 0.1√ó-0.383 = -1.139\n",
      "‚îÇ Change:     -0.038\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "üìç STEP  2 - Current State: 1\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ACTION SELECTION (State 1) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Q-values: [-0.81, -0.54]\n",
      "‚îÇ Best:     Action 1 (Q=-0.537)\n",
      "‚îÇ Decision: EXPLOIT üéØ (0.751 ‚â• 0.3)\n",
      "‚îÇ Chosen:   Action 1 (greedy)\n",
      "‚îÇ Stats:    Explore 18/62 (29.0%)\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ENVIRONMENT RESPONSE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Action:     1 (RIGHT)\n",
      "‚îÇ Transition: 1 ‚Üí 2\n",
      "‚îÇ Reward:     -1.0\n",
      "‚îÇ Done:       False\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ Q-UPDATE #62 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Transition: S1 --A1--> S2 (R=-1.0)\n",
      "‚îÇ Done:       False\n",
      "‚îÇ Next Q's:   [-0.45, 1.02]\n",
      "‚îÇ Target:     -1 + 0.9√ó1.017 = -0.085\n",
      "‚îÇ Q-Update:   -0.537 + 0.1√ó0.452 = -0.492\n",
      "‚îÇ Change:     +0.045\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "üìç STEP  3 - Current State: 2\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ACTION SELECTION (State 2) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Q-values: [-0.45, 1.02]\n",
      "‚îÇ Best:     Action 1 (Q=1.017)\n",
      "‚îÇ Decision: EXPLORE üé≤ (0.103 < 0.3)\n",
      "‚îÇ Chosen:   Action 0 (random)\n",
      "‚îÇ Stats:    Explore 19/63 (30.2%)\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ENVIRONMENT RESPONSE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Action:     0 (LEFT)\n",
      "‚îÇ Transition: 2 ‚Üí 1\n",
      "‚îÇ Reward:     -1.0\n",
      "‚îÇ Done:       False\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ Q-UPDATE #63 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Transition: S2 --A0--> S1 (R=-1.0)\n",
      "‚îÇ Done:       False\n",
      "‚îÇ Next Q's:   [-0.81, -0.49]\n",
      "‚îÇ Target:     -1 + 0.9√ó-0.492 = -1.443\n",
      "‚îÇ Q-Update:   -0.454 + 0.1√ó-0.988 = -0.553\n",
      "‚îÇ Change:     -0.099\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "üìç STEP  4 - Current State: 1\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ACTION SELECTION (State 1) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Q-values: [-0.81, -0.49]\n",
      "‚îÇ Best:     Action 1 (Q=-0.492)\n",
      "‚îÇ Decision: EXPLOIT üéØ (0.391 ‚â• 0.3)\n",
      "‚îÇ Chosen:   Action 1 (greedy)\n",
      "‚îÇ Stats:    Explore 19/64 (29.7%)\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ENVIRONMENT RESPONSE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Action:     1 (RIGHT)\n",
      "‚îÇ Transition: 1 ‚Üí 2\n",
      "‚îÇ Reward:     -1.0\n",
      "‚îÇ Done:       False\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ Q-UPDATE #64 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Transition: S1 --A1--> S2 (R=-1.0)\n",
      "‚îÇ Done:       False\n",
      "‚îÇ Next Q's:   [-0.55, 1.02]\n",
      "‚îÇ Target:     -1 + 0.9√ó1.017 = -0.085\n",
      "‚îÇ Q-Update:   -0.492 + 0.1√ó0.407 = -0.451\n",
      "‚îÇ Change:     +0.041\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "üìç STEP  5 - Current State: 2\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ACTION SELECTION (State 2) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Q-values: [-0.55, 1.02]\n",
      "‚îÇ Best:     Action 1 (Q=1.017)\n",
      "‚îÇ Decision: EXPLORE üé≤ (0.188 < 0.3)\n",
      "‚îÇ Chosen:   Action 1 (random)\n",
      "‚îÇ Stats:    Explore 20/65 (30.8%)\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ENVIRONMENT RESPONSE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Action:     1 (RIGHT)\n",
      "‚îÇ Transition: 2 ‚Üí 3\n",
      "‚îÇ Reward:     -1.0\n",
      "‚îÇ Done:       False\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ Q-UPDATE #65 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Transition: S2 --A1--> S3 (R=-1.0)\n",
      "‚îÇ Done:       False\n",
      "‚îÇ Next Q's:   [-0.24, 5.22]\n",
      "‚îÇ Target:     -1 + 0.9√ó5.217 = 3.695\n",
      "‚îÇ Q-Update:   1.017 + 0.1√ó2.678 = 1.285\n",
      "‚îÇ Change:     +0.268\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "üìç STEP  6 - Current State: 3\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ACTION SELECTION (State 3) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Q-values: [-0.24, 5.22]\n",
      "‚îÇ Best:     Action 1 (Q=5.217)\n",
      "‚îÇ Decision: EXPLOIT üéØ (0.331 ‚â• 0.3)\n",
      "‚îÇ Chosen:   Action 1 (greedy)\n",
      "‚îÇ Stats:    Explore 20/66 (30.3%)\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ENVIRONMENT RESPONSE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Action:     1 (RIGHT)\n",
      "‚îÇ Transition: 3 ‚Üí 4\n",
      "‚îÇ Reward:     +10.0\n",
      "‚îÇ Done:       True\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ Q-UPDATE #66 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Transition: S3 --A1--> S4 (R=+10.0)\n",
      "‚îÇ Done:       True\n",
      "‚îÇ Target:     10.000 (episode ended, no future)\n",
      "‚îÇ Q-Update:   5.217 + 0.1√ó4.783 = 5.695\n",
      "‚îÇ Change:     +0.478\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "‚îå‚îÄ EPISODE 9 SUMMARY ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Status:     SUCCESS ‚úÖ\n",
      "‚îÇ Steps:      6\n",
      "‚îÇ Reward:     +5.0\n",
      "‚îÇ Goal:       Reached\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "‚îå‚îÄ Q-TABLE AFTER EPISODE 9 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "        A0     A1  \n",
      "S0 ‚îÇ -1.46 -1.14 \n",
      "S1 ‚îÇ -0.81 -0.45 \n",
      "S2 ‚îÇ -0.55  1.28 \n",
      "S3 ‚îÇ -0.24  5.70 \n",
      "S4 ‚îÇ  0.00  0.00 \n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "‚îå‚îÄ CURRENT POLICY ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ State 0: Action 1 (Q=-1.139)\n",
      "‚îÇ State 1: Action 1 (Q=-0.451)\n",
      "‚îÇ State 2: Action 1 (Q=1.285)\n",
      "‚îÇ State 3: Action 1 (Q=5.695)\n",
      "‚îÇ State 4: Action 0 (Q=0.000)\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "============================================================\n",
      "üé¨ EPISODE 10/10 - 13:43:35\n",
      "============================================================\n",
      "\n",
      "üìç STEP  1 - Current State: 0\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ACTION SELECTION (State 0) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Q-values: [-1.46, -1.14]\n",
      "‚îÇ Best:     Action 1 (Q=-1.139)\n",
      "‚îÇ Decision: EXPLORE üé≤ (0.171 < 0.3)\n",
      "‚îÇ Chosen:   Action 1 (random)\n",
      "‚îÇ Stats:    Explore 21/67 (31.3%)\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ENVIRONMENT RESPONSE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Action:     1 (RIGHT)\n",
      "‚îÇ Transition: 0 ‚Üí 1\n",
      "‚îÇ Reward:     -1.0\n",
      "‚îÇ Done:       False\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ Q-UPDATE #67 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Transition: S0 --A1--> S1 (R=-1.0)\n",
      "‚îÇ Done:       False\n",
      "‚îÇ Next Q's:   [-0.81, -0.45]\n",
      "‚îÇ Target:     -1 + 0.9√ó-0.451 = -1.406\n",
      "‚îÇ Q-Update:   -1.139 + 0.1√ó-0.267 = -1.166\n",
      "‚îÇ Change:     -0.027\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "üìç STEP  2 - Current State: 1\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ACTION SELECTION (State 1) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Q-values: [-0.81, -0.45]\n",
      "‚îÇ Best:     Action 1 (Q=-0.451)\n",
      "‚îÇ Decision: EXPLOIT üéØ (0.565 ‚â• 0.3)\n",
      "‚îÇ Chosen:   Action 1 (greedy)\n",
      "‚îÇ Stats:    Explore 21/68 (30.9%)\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ENVIRONMENT RESPONSE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Action:     1 (RIGHT)\n",
      "‚îÇ Transition: 1 ‚Üí 2\n",
      "‚îÇ Reward:     -1.0\n",
      "‚îÇ Done:       False\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ Q-UPDATE #68 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Transition: S1 --A1--> S2 (R=-1.0)\n",
      "‚îÇ Done:       False\n",
      "‚îÇ Next Q's:   [-0.55, 1.28]\n",
      "‚îÇ Target:     -1 + 0.9√ó1.285 = 0.156\n",
      "‚îÇ Q-Update:   -0.451 + 0.1√ó0.608 = -0.391\n",
      "‚îÇ Change:     +0.061\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "üìç STEP  3 - Current State: 2\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ACTION SELECTION (State 2) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Q-values: [-0.55, 1.28]\n",
      "‚îÇ Best:     Action 1 (Q=1.285)\n",
      "‚îÇ Decision: EXPLORE üé≤ (0.054 < 0.3)\n",
      "‚îÇ Chosen:   Action 0 (random)\n",
      "‚îÇ Stats:    Explore 22/69 (31.9%)\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ENVIRONMENT RESPONSE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Action:     0 (LEFT)\n",
      "‚îÇ Transition: 2 ‚Üí 1\n",
      "‚îÇ Reward:     -1.0\n",
      "‚îÇ Done:       False\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ Q-UPDATE #69 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Transition: S2 --A0--> S1 (R=-1.0)\n",
      "‚îÇ Done:       False\n",
      "‚îÇ Next Q's:   [-0.81, -0.39]\n",
      "‚îÇ Target:     -1 + 0.9√ó-0.391 = -1.352\n",
      "‚îÇ Q-Update:   -0.553 + 0.1√ó-0.798 = -0.633\n",
      "‚îÇ Change:     -0.080\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "üìç STEP  4 - Current State: 1\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ACTION SELECTION (State 1) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Q-values: [-0.81, -0.39]\n",
      "‚îÇ Best:     Action 1 (Q=-0.391)\n",
      "‚îÇ Decision: EXPLOIT üéØ (0.458 ‚â• 0.3)\n",
      "‚îÇ Chosen:   Action 1 (greedy)\n",
      "‚îÇ Stats:    Explore 22/70 (31.4%)\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ENVIRONMENT RESPONSE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Action:     1 (RIGHT)\n",
      "‚îÇ Transition: 1 ‚Üí 2\n",
      "‚îÇ Reward:     -1.0\n",
      "‚îÇ Done:       False\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ Q-UPDATE #70 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Transition: S1 --A1--> S2 (R=-1.0)\n",
      "‚îÇ Done:       False\n",
      "‚îÇ Next Q's:   [-0.63, 1.28]\n",
      "‚îÇ Target:     -1 + 0.9√ó1.285 = 0.156\n",
      "‚îÇ Q-Update:   -0.391 + 0.1√ó0.547 = -0.336\n",
      "‚îÇ Change:     +0.055\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "üìç STEP  5 - Current State: 2\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ACTION SELECTION (State 2) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Q-values: [-0.63, 1.28]\n",
      "‚îÇ Best:     Action 1 (Q=1.285)\n",
      "‚îÇ Decision: EXPLOIT üéØ (0.875 ‚â• 0.3)\n",
      "‚îÇ Chosen:   Action 1 (greedy)\n",
      "‚îÇ Stats:    Explore 22/71 (31.0%)\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ENVIRONMENT RESPONSE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Action:     1 (RIGHT)\n",
      "‚îÇ Transition: 2 ‚Üí 3\n",
      "‚îÇ Reward:     -1.0\n",
      "‚îÇ Done:       False\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ Q-UPDATE #71 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Transition: S2 --A1--> S3 (R=-1.0)\n",
      "‚îÇ Done:       False\n",
      "‚îÇ Next Q's:   [-0.24, 5.70]\n",
      "‚îÇ Target:     -1 + 0.9√ó5.695 = 4.126\n",
      "‚îÇ Q-Update:   1.285 + 0.1√ó2.841 = 1.569\n",
      "‚îÇ Change:     +0.284\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "üìç STEP  6 - Current State: 3\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ACTION SELECTION (State 3) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Q-values: [-0.24, 5.70]\n",
      "‚îÇ Best:     Action 1 (Q=5.695)\n",
      "‚îÇ Decision: EXPLOIT üéØ (0.410 ‚â• 0.3)\n",
      "‚îÇ Chosen:   Action 1 (greedy)\n",
      "‚îÇ Stats:    Explore 22/72 (30.6%)\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ ENVIRONMENT RESPONSE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Action:     1 (RIGHT)\n",
      "‚îÇ Transition: 3 ‚Üí 4\n",
      "‚îÇ Reward:     +10.0\n",
      "‚îÇ Done:       True\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îå‚îÄ Q-UPDATE #72 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Transition: S3 --A1--> S4 (R=+10.0)\n",
      "‚îÇ Done:       True\n",
      "‚îÇ Target:     10.000 (episode ended, no future)\n",
      "‚îÇ Q-Update:   5.695 + 0.1√ó4.305 = 6.126\n",
      "‚îÇ Change:     +0.430\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "‚îå‚îÄ EPISODE 10 SUMMARY ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ Status:     SUCCESS ‚úÖ\n",
      "‚îÇ Steps:      6\n",
      "‚îÇ Reward:     +5.0\n",
      "‚îÇ Goal:       Reached\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "‚îå‚îÄ Q-TABLE AFTER EPISODE 10 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "        A0     A1  \n",
      "S0 ‚îÇ -1.46 -1.17 \n",
      "S1 ‚îÇ -0.81 -0.34 \n",
      "S2 ‚îÇ -0.63  1.57 \n",
      "S3 ‚îÇ -0.24  6.13 \n",
      "S4 ‚îÇ  0.00  0.00 \n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "‚îå‚îÄ CURRENT POLICY ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ State 0: Action 1 (Q=-1.166)\n",
      "‚îÇ State 1: Action 1 (Q=-0.336)\n",
      "‚îÇ State 2: Action 1 (Q=1.569)\n",
      "‚îÇ State 3: Action 1 (Q=6.126)\n",
      "‚îÇ State 4: Action 0 (Q=0.000)\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "============================================================\n",
      "üéâ TRAINING COMPLETE\n",
      "============================================================\n",
      "‚îÇ Episodes:      10\n",
      "‚îÇ Success Rate:  9/10 (90.0%)\n",
      "‚îÇ Avg Reward:    +1.50\n",
      "‚îÇ Avg Steps:     7.2\n",
      "‚îÇ Exploration:   22/72 (30.6%)\n",
      "‚îÇ Duration:      0.0s\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "EPISODE BREAKDOWN:\n",
      "Ep# ‚îÇ Steps ‚îÇ Reward ‚îÇ Status\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      " 1  ‚îÇ   9   ‚îÇ   -2.0 ‚îÇ   ‚úÖ\n",
      " 2  ‚îÇ  10   ‚îÇ  -14.0 ‚îÇ   ‚è∞\n",
      " 3  ‚îÇ   6   ‚îÇ   +5.0 ‚îÇ   ‚úÖ\n",
      " 4  ‚îÇ   6   ‚îÇ   +5.0 ‚îÇ   ‚úÖ\n",
      " 5  ‚îÇ  10   ‚îÇ   +1.0 ‚îÇ   ‚úÖ\n",
      " 6  ‚îÇ   6   ‚îÇ   +5.0 ‚îÇ   ‚úÖ\n",
      " 7  ‚îÇ   7   ‚îÇ   +0.0 ‚îÇ   ‚úÖ\n",
      " 8  ‚îÇ   6   ‚îÇ   +5.0 ‚îÇ   ‚úÖ\n",
      " 9  ‚îÇ   6   ‚îÇ   +5.0 ‚îÇ   ‚úÖ\n",
      "10  ‚îÇ   6   ‚îÇ   +5.0 ‚îÇ   ‚úÖ\n",
      "============================================================\n",
      "\n",
      "‚îå‚îÄ FINAL Q-TABLE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "        A0     A1  \n",
      "S0 ‚îÇ -1.46 -1.17 \n",
      "S1 ‚îÇ -0.81 -0.34 \n",
      "S2 ‚îÇ -0.63  1.57 \n",
      "S3 ‚îÇ -0.24  6.13 \n",
      "S4 ‚îÇ  0.00  0.00 \n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "‚îå‚îÄ CURRENT POLICY ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ State 0: Action 1 (Q=-1.166)\n",
      "‚îÇ State 1: Action 1 (Q=-0.336)\n",
      "‚îÇ State 2: Action 1 (Q=1.569)\n",
      "‚îÇ State 3: Action 1 (Q=6.126)\n",
      "‚îÇ State 4: Action 0 (Q=0.000)\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "============================================================\n",
      "üß™ TESTING TRAINED AGENT (Pure Exploitation)\n",
      "============================================================\n",
      "Starting at state 0\n",
      "\n",
      "Step 1:\n",
      "  Action: 1 (RIGHT) ‚Üí State 0 ‚Üí 1 (Reward: -1.0)\n",
      "Step 2:\n",
      "  Action: 1 (RIGHT) ‚Üí State 1 ‚Üí 2 (Reward: -1.0)\n",
      "Step 3:\n",
      "  Action: 1 (RIGHT) ‚Üí State 2 ‚Üí 3 (Reward: -1.0)\n",
      "Step 4:\n",
      "  Action: 1 (RIGHT) ‚Üí State 3 ‚Üí 4 (Reward: +10.0)\n",
      "‚úÖ SUCCESS! Goal reached in 4 steps!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "class QLearningAgent:\n",
    "    def __init__(self, num_states, num_actions, lr=0.1, gamma=0.95, epsilon=0.1):\n",
    "        # Initialize Q-table with zeros\n",
    "        self.q_table = np.zeros((num_states, num_actions))\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.num_states = num_states\n",
    "        self.num_actions = num_actions\n",
    "\n",
    "        # Logging counters\n",
    "        self.exploration_count = 0\n",
    "        self.exploitation_count = 0\n",
    "        self.update_count = 0\n",
    "\n",
    "        self._print_initialization()\n",
    "\n",
    "    def _print_initialization(self):\n",
    "        \"\"\"Print agent initialization in organized format\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ü§ñ Q-LEARNING AGENT INITIALIZATION\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"‚îÇ Q-table Shape       ‚îÇ {self.q_table.shape}\")\n",
    "        print(f\"‚îÇ Learning Rate (Œ±)   ‚îÇ {self.lr}\")\n",
    "        print(f\"‚îÇ Discount Factor (Œ≥) ‚îÇ {self.gamma}\")\n",
    "        print(f\"‚îÇ Exploration Rate (Œµ)‚îÇ {self.epsilon}\")\n",
    "        print(\"‚îÄ\"*60)\n",
    "        print(\"Initial Q-Table (all zeros):\")\n",
    "        self._print_q_table_compact()\n",
    "        print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "    def get_action(self, state, verbose=True):\n",
    "        \"\"\"Get action using epsilon-greedy strategy\"\"\"\n",
    "        current_q_values = self.q_table[state]\n",
    "        best_action = np.argmax(current_q_values)\n",
    "        best_q_value = current_q_values[best_action]\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"‚îå‚îÄ ACTION SELECTION (State {state}) \" + \"‚îÄ\"*25)\n",
    "            print(f\"‚îÇ Q-values: {self._format_array(current_q_values)}\")\n",
    "            print(f\"‚îÇ Best:     Action {best_action} (Q={best_q_value:.3f})\")\n",
    "\n",
    "        # Epsilon-greedy decision\n",
    "        random_prob = random.random()\n",
    "        if random_prob < self.epsilon:\n",
    "            action = random.randint(0, self.num_actions - 1)\n",
    "            self.exploration_count += 1\n",
    "            decision_type = \"EXPLORE üé≤\"\n",
    "            if verbose:\n",
    "                print(f\"‚îÇ Decision: {decision_type} ({random_prob:.3f} < {self.epsilon})\")\n",
    "                print(f\"‚îÇ Chosen:   Action {action} (random)\")\n",
    "        # Exploit decision\n",
    "        else:\n",
    "            action = best_action\n",
    "            self.exploitation_count += 1\n",
    "            decision_type = \"EXPLOIT üéØ\"\n",
    "            if verbose:\n",
    "                print(f\"‚îÇ Decision: {decision_type} ({random_prob:.3f} ‚â• {self.epsilon})\")\n",
    "                print(f\"‚îÇ Chosen:   Action {action} (greedy)\")\n",
    "\n",
    "        if verbose:\n",
    "            total_decisions = self.exploration_count + self.exploitation_count\n",
    "            explore_pct = (self.exploration_count / total_decisions * 100) if total_decisions > 0 else 0\n",
    "            print(f\"‚îÇ Stats:    Explore {self.exploration_count}/{total_decisions} ({explore_pct:.1f}%)\")\n",
    "            print(\"‚îî\" + \"‚îÄ\"*50)\n",
    "\n",
    "        return action\n",
    "\n",
    "    def update(self, state, action, reward, next_state, done, verbose=True):\n",
    "        \"\"\"Update Q-table with organized logging\"\"\"\n",
    "        self.update_count += 1\n",
    "        old_q_value = self.q_table[state, action]\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"‚îå‚îÄ Q-UPDATE #{self.update_count} \" + \"‚îÄ\"*35)\n",
    "            print(f\"‚îÇ Transition: S{state} --A{action}--> S{next_state} (R={reward:+.1f})\")\n",
    "            print(f\"‚îÇ Done:       {done}\")\n",
    "\n",
    "        # Calculate target\n",
    "        if done:\n",
    "            target = reward\n",
    "            if verbose:\n",
    "                print(f\"‚îÇ Target:     {target:.3f} (episode ended, no future)\")\n",
    "        else:\n",
    "            next_q_values = self.q_table[next_state]\n",
    "            max_next_q = np.max(next_q_values)\n",
    "            target = reward + self.gamma * max_next_q\n",
    "            if verbose:\n",
    "                print(f\"‚îÇ Next Q's:   {self._format_array(next_q_values)}\")\n",
    "                print(f\"‚îÇ Target:     {reward} + {self.gamma}√ó{max_next_q:.3f} = {target:.3f}\")\n",
    "\n",
    "        # Update Q-value\n",
    "        td_error = target - old_q_value\n",
    "        new_q_value = old_q_value + self.lr * td_error\n",
    "        self.q_table[state, action] = new_q_value\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"‚îÇ Q-Update:   {old_q_value:.3f} + {self.lr}√ó{td_error:.3f} = {new_q_value:.3f}\")\n",
    "            print(f\"‚îÇ Change:     {new_q_value - old_q_value:+.3f}\")\n",
    "            print(\"‚îî\" + \"‚îÄ\"*50)\n",
    "\n",
    "        return td_error\n",
    "\n",
    "    def _format_array(self, arr, decimals=2):\n",
    "        \"\"\"Format numpy array for clean display\"\"\"\n",
    "        return \"[\" + \", \".join([f\"{x:.{decimals}f}\" for x in arr]) + \"]\"\n",
    "\n",
    "    def _print_q_table_compact(self):\n",
    "        \"\"\"Print Q-table in compact format\"\"\"\n",
    "        print(\"     \", end=\"\")\n",
    "        for a in range(self.num_actions):\n",
    "            print(f\"   A{a}  \", end=\"\")\n",
    "        print()\n",
    "\n",
    "        for s in range(self.num_states):\n",
    "            print(f\"S{s} ‚îÇ \", end=\"\")\n",
    "            for a in range(self.num_actions):\n",
    "                print(f\"{self.q_table[s, a]:5.2f}\", end=\" \")\n",
    "            print()\n",
    "\n",
    "    def print_q_table(self, title=\"Q-TABLE\"):\n",
    "        \"\"\"Print Q-table with header\"\"\"\n",
    "        print(f\"\\n‚îå‚îÄ {title} \" + \"‚îÄ\"*(50-len(title)))\n",
    "        self._print_q_table_compact()\n",
    "        print(\"‚îî\" + \"‚îÄ\"*50)\n",
    "\n",
    "    def print_policy(self):\n",
    "        \"\"\"Print current policy\"\"\"\n",
    "        print(\"\\n‚îå‚îÄ CURRENT POLICY \" + \"‚îÄ\"*33)\n",
    "        for s in range(self.num_states):\n",
    "            best_action = np.argmax(self.q_table[s])\n",
    "            best_q_value = self.q_table[s, best_action]\n",
    "            print(f\"‚îÇ State {s}: Action {best_action} (Q={best_q_value:.3f})\")\n",
    "        print(\"‚îî\" + \"‚îÄ\"*50)\n",
    "\n",
    "\n",
    "class TrainingLogger:\n",
    "    \"\"\"Separate class to handle training-level logging\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.episode_data = []\n",
    "        self.start_time = datetime.now()\n",
    "\n",
    "    def log_episode_start(self, episode, total_episodes):\n",
    "        \"\"\"Log episode start\"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"üé¨ EPISODE {episode + 1:2d}/{total_episodes} - {datetime.now().strftime('%H:%M:%S')}\")\n",
    "        print(f\"{'='*60}\")\n",
    "\n",
    "    def log_step_start(self, step, state):\n",
    "        \"\"\"Log step start\"\"\"\n",
    "        print(f\"\\nüìç STEP {step:2d} - Current State: {state}\")\n",
    "        print(\"‚îÄ\"*30)\n",
    "\n",
    "    def log_environment_response(self, action, state, next_state, reward, done):\n",
    "        \"\"\"Log environment response\"\"\"\n",
    "        action_name = \"LEFT\" if action == 0 else \"RIGHT\"\n",
    "        print(f\"‚îå‚îÄ ENVIRONMENT RESPONSE \" + \"‚îÄ\"*27)\n",
    "        print(f\"‚îÇ Action:     {action} ({action_name})\")\n",
    "        print(f\"‚îÇ Transition: {state} ‚Üí {next_state}\")\n",
    "        print(f\"‚îÇ Reward:     {reward:+.1f}\")\n",
    "        print(f\"‚îÇ Done:       {done}\")\n",
    "        print(\"‚îî\" + \"‚îÄ\"*50)\n",
    "\n",
    "    def log_episode_summary(self, episode, steps, reward, done, goal_state=4):\n",
    "        \"\"\"Log episode summary\"\"\"\n",
    "        status = \"SUCCESS ‚úÖ\" if done and steps > 0 else \"TIMEOUT ‚è∞\"\n",
    "        self.episode_data.append({\n",
    "            'episode': episode + 1,\n",
    "            'steps': steps,\n",
    "            'reward': reward,\n",
    "            'success': done\n",
    "        })\n",
    "\n",
    "        print(f\"\\n‚îå‚îÄ EPISODE {episode + 1} SUMMARY \" + \"‚îÄ\"*28)\n",
    "        print(f\"‚îÇ Status:     {status}\")\n",
    "        print(f\"‚îÇ Steps:      {steps}\")\n",
    "        print(f\"‚îÇ Reward:     {reward:+.1f}\")\n",
    "        print(f\"‚îÇ Goal:       {'Reached' if done else 'Not reached'}\")\n",
    "        print(\"‚îî\" + \"‚îÄ\"*50)\n",
    "\n",
    "    def print_training_summary(self, agent):\n",
    "        \"\"\"Print final training summary\"\"\"\n",
    "        total_episodes = len(self.episode_data)\n",
    "        successful_episodes = sum(1 for ep in self.episode_data if ep['success'])\n",
    "        avg_reward = np.mean([ep['reward'] for ep in self.episode_data])\n",
    "        avg_steps = np.mean([ep['steps'] for ep in self.episode_data])\n",
    "\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"üéâ TRAINING COMPLETE\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"‚îÇ Episodes:      {total_episodes}\")\n",
    "        print(f\"‚îÇ Success Rate:  {successful_episodes}/{total_episodes} ({successful_episodes/total_episodes*100:.1f}%)\")\n",
    "        print(f\"‚îÇ Avg Reward:    {avg_reward:+.2f}\")\n",
    "        print(f\"‚îÇ Avg Steps:     {avg_steps:.1f}\")\n",
    "        print(f\"‚îÇ Exploration:   {agent.exploration_count}/{agent.exploration_count + agent.exploitation_count} ({agent.exploration_count/(agent.exploration_count + agent.exploitation_count)*100:.1f}%)\")\n",
    "        print(f\"‚îÇ Duration:      {(datetime.now() - self.start_time).total_seconds():.1f}s\")\n",
    "        print(\"‚îÄ\"*60)\n",
    "\n",
    "        # Episode-by-episode breakdown\n",
    "        print(\"EPISODE BREAKDOWN:\")\n",
    "        print(\"Ep# ‚îÇ Steps ‚îÇ Reward ‚îÇ Status\")\n",
    "        print(\"‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\")\n",
    "        for ep in self.episode_data:\n",
    "            status = \"‚úÖ\" if ep['success'] else \"‚è∞\"\n",
    "            print(f\"{ep['episode']:2d}  ‚îÇ  {ep['steps']:2d}   ‚îÇ {ep['reward']:+6.1f} ‚îÇ   {status}\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "\n",
    "def simple_environment_step(state, action, num_states=5):\n",
    "    \"\"\"Simple environment: move left/right, goal is rightmost state\"\"\"\n",
    "    if action == 0:  # Move left\n",
    "        next_state = max(0, state - 1)\n",
    "    else:  # Move right\n",
    "        next_state = min(num_states - 1, state + 1)\n",
    "\n",
    "    # Rewards\n",
    "    if next_state == num_states - 1:  # Goal reached\n",
    "        reward = 10\n",
    "        done = True\n",
    "    elif (action == 0 and state == 0) or (action == 1 and state == num_states - 1):\n",
    "        reward = -5  # Hit boundary\n",
    "        done = False\n",
    "    else:\n",
    "        reward = -1  # Step cost\n",
    "        done = False\n",
    "\n",
    "    return next_state, reward, done\n",
    "\n",
    "\n",
    "def train_agent(episodes=5, max_steps_per_episode=10, verbose=True):\n",
    "    \"\"\"Train agent with organized logging\"\"\"\n",
    "\n",
    "    # Initialize\n",
    "    agent = QLearningAgent(num_states=5, num_actions=2, lr=0.1, gamma=0.9, epsilon=0.3)\n",
    "    logger = TrainingLogger()\n",
    "\n",
    "    # Training loop\n",
    "    for episode in range(episodes):\n",
    "        logger.log_episode_start(episode, episodes)\n",
    "\n",
    "        state = 0  # Start state\n",
    "        episode_reward = 0\n",
    "        step_count = 0\n",
    "\n",
    "        for step in range(max_steps_per_episode):\n",
    "            step_count += 1\n",
    "\n",
    "            if verbose:\n",
    "                logger.log_step_start(step_count, state)\n",
    "\n",
    "            # Get action\n",
    "            action = agent.get_action(state, verbose=verbose)\n",
    "\n",
    "            # Environment step\n",
    "            next_state, reward, done = simple_environment_step(state, action)\n",
    "            episode_reward += reward\n",
    "\n",
    "            if verbose:\n",
    "                logger.log_environment_response(action, state, next_state, reward, done)\n",
    "\n",
    "            # Update agent\n",
    "            agent.update(state, action, reward, next_state, done, verbose=verbose)\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        # Episode summary\n",
    "        logger.log_episode_summary(episode, step_count, episode_reward, done)\n",
    "\n",
    "        # Show Q-table and policy after each episode\n",
    "        agent.print_q_table(f\"Q-TABLE AFTER EPISODE {episode + 1}\")\n",
    "        agent.print_policy()\n",
    "\n",
    "    # Final summary\n",
    "    logger.print_training_summary(agent)\n",
    "    agent.print_q_table(\"FINAL Q-TABLE\")\n",
    "    agent.print_policy()\n",
    "\n",
    "    return agent\n",
    "\n",
    "\n",
    "def test_trained_agent(agent, max_steps=10):\n",
    "    \"\"\"Test the trained agent\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"üß™ TESTING TRAINED AGENT (Pure Exploitation)\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    agent.epsilon = 0.0  # No exploration\n",
    "    state = 0\n",
    "    step = 0\n",
    "\n",
    "    print(f\"Starting at state {state}\\n\")\n",
    "\n",
    "    while step < max_steps:\n",
    "        step += 1\n",
    "        print(f\"Step {step}:\")\n",
    "\n",
    "        action = agent.get_action(state, verbose=False)\n",
    "        next_state, reward, done = simple_environment_step(state, action)\n",
    "\n",
    "        action_name = \"LEFT\" if action == 0 else \"RIGHT\"\n",
    "        print(f\"  Action: {action} ({action_name}) ‚Üí State {state} ‚Üí {next_state} (Reward: {reward:+.1f})\")\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            print(f\"‚úÖ SUCCESS! Goal reached in {step} steps!\")\n",
    "            break\n",
    "\n",
    "    if not done:\n",
    "        print(f\"‚ùå FAILED! Could not reach goal in {max_steps} steps\")\n",
    "\n",
    "    print(\"=\"*60)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üöÄ Q-LEARNING TRAINING DEMONSTRATION\")\n",
    "    print(\"Environment: 5 states (0,1,2,3,4), 2 actions (left/right), goal=state 4\")\n",
    "\n",
    "    # Train with organized output\n",
    "    trained_agent = train_agent(episodes=10, max_steps_per_episode=10, verbose=True)\n",
    "\n",
    "    # Test the trained agent\n",
    "    test_trained_agent(trained_agent, max_steps=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key Differences QN-DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T08:32:43.858652Z",
     "start_time": "2025-08-03T08:32:43.855588Z"
    }
   },
   "outputs": [],
   "source": [
    "# Q-Table: Simple numpy array\n",
    "#q_table = np.zeros((states, actions))  # Direct storage\n",
    "\n",
    "# DQN: Neural network parameters\n",
    "#network = DQN(state_size, action_size)  # Learned representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Q-Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T08:33:11.787057Z",
     "start_time": "2025-08-03T08:33:11.783949Z"
    }
   },
   "outputs": [],
   "source": [
    "# Q-Table: Direct lookup\n",
    "#q_values = q_table[state]  # O(1) operation\n",
    "\n",
    "# DQN: Forward pass computation  \n",
    "#q_values = network(torch.tensor(state))  # O(network_size) operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning/Updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T08:33:46.519497Z",
     "start_time": "2025-08-03T08:33:46.516690Z"
    }
   },
   "outputs": [],
   "source": [
    "# Q-Table: Direct value update\n",
    "#q_table[state, action] = new_value  # Simple assignment\n",
    "\n",
    "# DQN: Gradient descent\n",
    "#loss.backward()          # Compute gradients\n",
    "#optimizer.step()         # Update all network weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T08:34:13.361279Z",
     "start_time": "2025-08-03T08:34:13.357814Z"
    }
   },
   "outputs": [],
   "source": [
    "# Q-Table: Memory grows with state space\n",
    "# 1M states √ó 4 actions = 4M values stored\n",
    "\n",
    "# DQN: Memory grows with network size\n",
    "# Network with 1000 parameters regardless of state space size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T08:36:09.791811Z",
     "start_time": "2025-08-03T08:36:09.761909Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'episodes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Q-Table training\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m episode \u001b[38;5;129;01min\u001b[39;00m \u001b[43mepisodes\u001b[49m:\n\u001b[1;32m      3\u001b[0m     state \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'episodes' is not defined"
     ]
    }
   ],
   "source": [
    "# Q-Table training\n",
    "for episode in episodes:\n",
    "    state = env.reset()\n",
    "    while not done:\n",
    "        action = agent.get_action(state)\n",
    "        next_state, reward, done = env.step(action)\n",
    "        agent.update(state, action, reward, next_state, done)  # Immediate update\n",
    "        state = next_state\n",
    "\n",
    "# DQN training  \n",
    "for episode in episodes:\n",
    "    state = env.reset()\n",
    "    while not done:\n",
    "        action = agent.get_action(state)\n",
    "        next_state, reward, done = env.step(action)\n",
    "        agent.remember(state, action, reward, next_state, done)  # Store experience\n",
    "        agent.replay()  # Batch learning from memory\n",
    "        state = next_state\n",
    "    agent.update_target_network()  # Periodic target update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNxMGavJfLxkr/LXurjCzUi",
   "include_colab_link": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "notify_time": "0",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "335.998px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
