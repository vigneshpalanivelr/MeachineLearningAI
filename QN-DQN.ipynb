{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNxMGavJfLxkr/LXurjCzUi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vigneshpalanivelr/MeachineLearningAI/blob/master/QN-DQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q-Learning Variables"
      ],
      "metadata": {
        "id": "dvsu_QkOJ5Mw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Learning Rate (lr) - New information vs old knowledge?"
      ],
      "metadata": {
        "id": "YAiGYhcExLI2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   **What it controls:** How much we update our Q-values with each new experience.\n",
        "2.   **Think of it as:** How much do you trust new information vs. old knowledge?\n",
        "\n",
        "**Examples:**\n",
        "\n",
        "0.   **lr = 0.01:** \"I'll barely change my belief, I trust my old knowledge more\"\n",
        "1.   **lr = 0.1:** \"I'll update my belief by 10% based on this new experience\"\n",
        "2.   **lr = 1.0:** \"I'll completely replace my old belief with this new experience\""
      ],
      "metadata": {
        "id": "6C4QLMgfvWzL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Current Q-value for \"go right\" = 5.0\n",
        "# New experience suggests it should be 8.0\n",
        "# Target = 8.0, Current = 5.0, Difference = 3.0\n",
        "\n",
        "# With lr = 0.1:\n",
        "new_q_value = 5.0 + 0.1 * (8.0 - 5.0) = 5.0 + 0.3 = 5.3\n",
        "\n",
        "# With lr = 0.5:\n",
        "new_q_value = 5.0 + 0.5 * (8.0 - 5.0) = 5.0 + 1.5 = 6.5\n",
        "\n",
        "# With lr = 1.0:\n",
        "new_q_value = 5.0 + 1.0 * (8.0 - 5.0) = 5.0 + 3.0 = 8.0"
      ],
      "metadata": {
        "id": "XMxUEF8_vS_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Discount Factor (gamma) - long-term vs. short-term gains"
      ],
      "metadata": {
        "id": "qv_XiITZw8y3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1.   **What it controls:** How much we value future rewards compared to immediate rewards.\n",
        "2.   **Think of it as:** How much do you care about long-term vs. short-term gains?\n",
        "3.   **Real-world analogy:** Would you rather have \\$10 now or $100  next year? Gamma represents your **\"patience level.\"**\n",
        "\n",
        "**Examples:**\n",
        "1.   **gamma = 0.95:** \"Future rewards are worth 95% of immediate rewards\"\n",
        "2.   **gamma = 0.0:** \"I only care about immediate rewards\" (greedy)\n",
        "3.   **gamma = 1.0:** \"Future rewards are just as valuable as immediate rewards\"\n"
      ],
      "metadata": {
        "id": "MqsPy2ynxToc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Immediate reward = 10\n",
        "# Expected future reward = 100\n",
        "\n",
        "# With gamma = 0.0:\n",
        "target = 10 + 0.0 * 100 = 10  # Only immediate reward matters\n",
        "\n",
        "# With gamma = 0.5:\n",
        "target = 10 + 0.5 * 100 = 60  # Future reward discounted by 50%\n",
        "\n",
        "# With gamma = 0.95:\n",
        "target = 10 + 0.95 * 100 = 105  # Future reward almost fully valued"
      ],
      "metadata": {
        "id": "BZZ1mPV5zwe6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Epsilon (epsilon)**"
      ],
      "metadata": {
        "id": "J6aOzxzV2Hpi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   **What it controls:** The exploration vs. exploitation trade-off during action selection.\n",
        "2.   **Think of it as:** How often should I try something new vs. stick with what I know works?\n",
        "\n",
        "**Examples:**\n",
        "1.   **epsilon = 0.1:** \"90% of the time, choose the best action; 10% of the time, explore randomly\"\n",
        "2.   **epsilon = 0.5:** \"50% exploration, 50% exploitation\"\n",
        "3.   **epsilon = 0.0:** \"Always choose the best known action\" (pure exploitation)"
      ],
      "metadata": {
        "id": "uHSwxUCV1cid"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Current Q-values: [2.1, 8.5, 1.2, 0.9] for actions [Up, Right, Down, Left]\n",
        "# Best action is \"Right\" (index 1) with Q-value 8.5\n",
        "\n",
        "# With epsilon = 0.1:\n",
        "# 90% chance: Choose \"Right\"\n",
        "# 10% chance: Choose random action (Up, Right, Down, or Left)\n",
        "\n",
        "# With epsilon = 0.0:\n",
        "# 100% chance: Always choose \"Right\""
      ],
      "metadata": {
        "id": "53RE2noF24S9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_agent():\n",
        "    state = env.reset()\n",
        "\n",
        "    while not done:\n",
        "        # Epsilon controls exploration\n",
        "        if random.random() < epsilon:\n",
        "            action = random.choice(actions)  # Explore\n",
        "        else:\n",
        "            action = argmax(q_table[state])  # Exploit\n",
        "\n",
        "        next_state, reward, done = env.step(action)\n",
        "\n",
        "        # Gamma affects how we value future rewards\n",
        "        if done:\n",
        "            target = reward\n",
        "        else:\n",
        "            target = reward + gamma * max(q_table[next_state])\n",
        "\n",
        "        # Learning rate controls how much we update\n",
        "        q_table[state, action] += lr * (target - q_table[state, action])\n",
        "\n",
        "        state = next_state"
      ],
      "metadata": {
        "id": "gBCU3DSY3M_W"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## np.argmax()"
      ],
      "metadata": {
        "id": "LakIqoWdJn3w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def demonstrate_argmax():\n",
        "    \"\"\"Demonstrate how np.argmax works in different scenarios\"\"\"\n",
        "\n",
        "    print(\"=\"*60)\n",
        "    print(\"üîç UNDERSTANDING np.argmax()\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Example 1: Basic usage\n",
        "    print(\"\\nüìç Example 1: Basic Array\")\n",
        "    print(\"-\"*30)\n",
        "    values = [1.2, 3.8, 2.1, 0.5]\n",
        "    max_index = np.argmax(values)\n",
        "    max_value = values[max_index]\n",
        "\n",
        "    print(f\"Array:           {values}\")\n",
        "    print(f\"Indices:         [0, 1, 2, 3]\")\n",
        "    print(f\"np.argmax():     {max_index}\")\n",
        "    print(f\"Max value:       {max_value}\")\n",
        "    print(f\"Explanation:     Index {max_index} contains the largest value ({max_value})\")\n",
        "\n",
        "    # Example 2: Q-learning context\n",
        "    print(\"\\nüìç Example 2: Q-Learning Actions\")\n",
        "    print(\"-\"*30)\n",
        "    q_values = np.array([2.1, -1.0, 0.5, 3.5])\n",
        "    actions = [\"Up\", \"Down\", \"Left\", \"Right\"]\n",
        "    best_action_index = np.argmax(q_values)\n",
        "    best_action_name = actions[best_action_index]\n",
        "    best_q_value = q_values[best_action_index]\n",
        "\n",
        "    print(\"Q-values by action:\")\n",
        "    for i, (action, q_val) in enumerate(zip(actions, q_values)):\n",
        "        marker = \" üëà BEST!\" if i == best_action_index else \"\"\n",
        "        print(f\"  Index {i}: {action:>5} = {q_val:5.1f}{marker}\")\n",
        "\n",
        "    print(f\"\\nnp.argmax(q_values): {best_action_index}\")\n",
        "    print(f\"Best action:         {best_action_name}\")\n",
        "    print(f\"Best Q-value:        {best_q_value}\")\n",
        "\n",
        "    # Example 3: Edge cases\n",
        "    print(\"\\nüìç Example 3: Edge Cases\")\n",
        "    print(\"-\"*30)\n",
        "\n",
        "    # All same values\n",
        "    same_values = [2.0, 2.0, 2.0, 2.0]\n",
        "    print(f\"All same values: {same_values}\")\n",
        "    print(f\"np.argmax():     {np.argmax(same_values)} (returns first occurrence)\")\n",
        "\n",
        "    # Negative values\n",
        "    negative_values = [-5.0, -1.0, -3.0, -2.0]\n",
        "    print(f\"All negative:    {negative_values}\")\n",
        "    print(f\"np.argmax():     {np.argmax(negative_values)} (index of least negative)\")\n",
        "\n",
        "    # Single value\n",
        "    single_value = [42.0]\n",
        "    print(f\"Single value:    {single_value}\")\n",
        "    print(f\"np.argmax():     {np.argmax(single_value)} (only option)\")\n",
        "\n",
        "    # Example 4: Compare with related functions\n",
        "    print(\"\\nüìç Example 4: Related Functions Comparison\")\n",
        "    print(\"-\"*30)\n",
        "    test_array = [1.5, 4.2, 2.8, 0.9]\n",
        "\n",
        "    print(f\"Array:               {test_array}\")\n",
        "    print(f\"np.argmax():         {np.argmax(test_array)} (index of max)\")\n",
        "    print(f\"np.max():            {np.max(test_array)} (actual max value)\")\n",
        "    print(f\"np.argmin():         {np.argmin(test_array)} (index of min)\")\n",
        "    print(f\"np.min():            {np.min(test_array)} (actual min value)\")\n",
        "\n",
        "def q_learning_action_selection_demo():\n",
        "    \"\"\"Show how argmax is used in Q-learning action selection\"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"üéÆ Q-LEARNING ACTION SELECTION WITH ARGMAX\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Simulate different states with different Q-values\n",
        "    states_info = [\n",
        "        {\"state\": 0, \"q_values\": [0.1, 0.3, 0.2, 0.8], \"description\": \"Clear best choice\"},\n",
        "        {\"state\": 1, \"q_values\": [2.1, 2.1, 1.5, 2.1], \"description\": \"Tie between actions\"},\n",
        "        {\"state\": 2, \"q_values\": [-1.0, -0.5, -2.0, -0.3], \"description\": \"All negative Q-values\"},\n",
        "        {\"state\": 3, \"q_values\": [0.0, 0.0, 0.0, 0.0], \"description\": \"All zeros (untrained)\"}\n",
        "    ]\n",
        "\n",
        "    actions = [\"Up\", \"Down\", \"Left\", \"Right\"]\n",
        "\n",
        "    for state_info in states_info:\n",
        "        state = state_info[\"state\"]\n",
        "        q_values = np.array(state_info[\"q_values\"])\n",
        "        description = state_info[\"description\"]\n",
        "\n",
        "        print(f\"\\nüè† State {state}: {description}\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        # Show Q-values\n",
        "        print(\"Q-values:\")\n",
        "        for i, (action, q_val) in enumerate(zip(actions, q_values)):\n",
        "            print(f\"  Action {i} ({action:>5}): {q_val:5.1f}\")\n",
        "\n",
        "        # Apply argmax\n",
        "        best_action_idx = np.argmax(q_values)\n",
        "        best_action_name = actions[best_action_idx]\n",
        "        best_q_value = q_values[best_action_idx]\n",
        "\n",
        "        print(f\"\\nAction Selection:\")\n",
        "        print(f\"  np.argmax(q_values) = {best_action_idx}\")\n",
        "        print(f\"  ‚Üí Choose: Action {best_action_idx} ({best_action_name})\")\n",
        "        print(f\"  ‚Üí Q-value: {best_q_value}\")\n",
        "\n",
        "        # Show what happens with ties\n",
        "        if len(np.where(q_values == best_q_value)[0]) > 1:\n",
        "            tied_indices = np.where(q_values == best_q_value)[0]\n",
        "            print(f\"  ‚ö†Ô∏è  Note: Tied with actions {list(tied_indices)} - argmax picks first\")\n",
        "\n",
        "def manual_vs_argmax_comparison():\n",
        "    \"\"\"Compare manual max finding vs np.argmax\"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"üîß MANUAL vs np.argmax() COMPARISON\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    q_values = [1.2, 3.8, 2.1, 0.5]\n",
        "\n",
        "    print(f\"Q-values: {q_values}\")\n",
        "    print()\n",
        "\n",
        "    # Manual way (what argmax does internally)\n",
        "    print(\"üî® Manual Method:\")\n",
        "    max_value = q_values[0]\n",
        "    max_index = 0\n",
        "\n",
        "    for i in range(len(q_values)):\n",
        "        print(f\"  Step {i+1}: Check index {i}, value = {q_values[i]}\")\n",
        "        if q_values[i] > max_value:\n",
        "            max_value = q_values[i]\n",
        "            max_index = i\n",
        "            print(f\"           New maximum! Update max_index to {i}\")\n",
        "        else:\n",
        "            print(f\"           Not larger than current max ({max_value})\")\n",
        "\n",
        "    print(f\"  Final result: max_index = {max_index}, max_value = {max_value}\")\n",
        "\n",
        "    # Using argmax\n",
        "    print(f\"\\n‚ö° Using np.argmax():\")\n",
        "    argmax_result = np.argmax(q_values)\n",
        "    print(f\"  np.argmax(q_values) = {argmax_result}\")\n",
        "    print(f\"  Same result: {max_index == argmax_result} ‚úÖ\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demonstrate_argmax()\n",
        "    q_learning_action_selection_demo()\n",
        "    manual_vs_argmax_comparison()\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"üìù SUMMARY\")\n",
        "    print(\"=\"*60)\n",
        "    print(\"‚Ä¢ np.argmax(array) returns the INDEX of the largest value\")\n",
        "    print(\"‚Ä¢ In Q-learning: argmax selects the action with highest Q-value\")\n",
        "    print(\"‚Ä¢ If multiple values tie for max, argmax returns the first index\")\n",
        "    print(\"‚Ä¢ This is how agents choose the 'best' action (exploitation)\")\n",
        "    print(\"=\"*60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fzRKJBXw6VwC",
        "outputId": "c0744bda-d8c4-4eb4-88f9-802f330676c0"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "üîç UNDERSTANDING np.argmax()\n",
            "============================================================\n",
            "\n",
            "üìç Example 1: Basic Array\n",
            "------------------------------\n",
            "Array:           [1.2, 3.8, 2.1, 0.5]\n",
            "Indices:         [0, 1, 2, 3]\n",
            "np.argmax():     1\n",
            "Max value:       3.8\n",
            "Explanation:     Index 1 contains the largest value (3.8)\n",
            "\n",
            "üìç Example 2: Q-Learning Actions\n",
            "------------------------------\n",
            "Q-values by action:\n",
            "  Index 0:    Up =   2.1\n",
            "  Index 1:  Down =  -1.0\n",
            "  Index 2:  Left =   0.5\n",
            "  Index 3: Right =   3.5 üëà BEST!\n",
            "\n",
            "np.argmax(q_values): 3\n",
            "Best action:         Right\n",
            "Best Q-value:        3.5\n",
            "\n",
            "üìç Example 3: Edge Cases\n",
            "------------------------------\n",
            "All same values: [2.0, 2.0, 2.0, 2.0]\n",
            "np.argmax():     0 (returns first occurrence)\n",
            "All negative:    [-5.0, -1.0, -3.0, -2.0]\n",
            "np.argmax():     1 (index of least negative)\n",
            "Single value:    [42.0]\n",
            "np.argmax():     0 (only option)\n",
            "\n",
            "üìç Example 4: Related Functions Comparison\n",
            "------------------------------\n",
            "Array:               [1.5, 4.2, 2.8, 0.9]\n",
            "np.argmax():         1 (index of max)\n",
            "np.max():            4.2 (actual max value)\n",
            "np.argmin():         3 (index of min)\n",
            "np.min():            0.9 (actual min value)\n",
            "\n",
            "============================================================\n",
            "üéÆ Q-LEARNING ACTION SELECTION WITH ARGMAX\n",
            "============================================================\n",
            "\n",
            "üè† State 0: Clear best choice\n",
            "----------------------------------------\n",
            "Q-values:\n",
            "  Action 0 (   Up):   0.1\n",
            "  Action 1 ( Down):   0.3\n",
            "  Action 2 ( Left):   0.2\n",
            "  Action 3 (Right):   0.8\n",
            "\n",
            "Action Selection:\n",
            "  np.argmax(q_values) = 3\n",
            "  ‚Üí Choose: Action 3 (Right)\n",
            "  ‚Üí Q-value: 0.8\n",
            "\n",
            "üè† State 1: Tie between actions\n",
            "----------------------------------------\n",
            "Q-values:\n",
            "  Action 0 (   Up):   2.1\n",
            "  Action 1 ( Down):   2.1\n",
            "  Action 2 ( Left):   1.5\n",
            "  Action 3 (Right):   2.1\n",
            "\n",
            "Action Selection:\n",
            "  np.argmax(q_values) = 0\n",
            "  ‚Üí Choose: Action 0 (Up)\n",
            "  ‚Üí Q-value: 2.1\n",
            "  ‚ö†Ô∏è  Note: Tied with actions [np.int64(0), np.int64(1), np.int64(3)] - argmax picks first\n",
            "\n",
            "üè† State 2: All negative Q-values\n",
            "----------------------------------------\n",
            "Q-values:\n",
            "  Action 0 (   Up):  -1.0\n",
            "  Action 1 ( Down):  -0.5\n",
            "  Action 2 ( Left):  -2.0\n",
            "  Action 3 (Right):  -0.3\n",
            "\n",
            "Action Selection:\n",
            "  np.argmax(q_values) = 3\n",
            "  ‚Üí Choose: Action 3 (Right)\n",
            "  ‚Üí Q-value: -0.3\n",
            "\n",
            "üè† State 3: All zeros (untrained)\n",
            "----------------------------------------\n",
            "Q-values:\n",
            "  Action 0 (   Up):   0.0\n",
            "  Action 1 ( Down):   0.0\n",
            "  Action 2 ( Left):   0.0\n",
            "  Action 3 (Right):   0.0\n",
            "\n",
            "Action Selection:\n",
            "  np.argmax(q_values) = 0\n",
            "  ‚Üí Choose: Action 0 (Up)\n",
            "  ‚Üí Q-value: 0.0\n",
            "  ‚ö†Ô∏è  Note: Tied with actions [np.int64(0), np.int64(1), np.int64(2), np.int64(3)] - argmax picks first\n",
            "\n",
            "============================================================\n",
            "üîß MANUAL vs np.argmax() COMPARISON\n",
            "============================================================\n",
            "Q-values: [1.2, 3.8, 2.1, 0.5]\n",
            "\n",
            "üî® Manual Method:\n",
            "  Step 1: Check index 0, value = 1.2\n",
            "           Not larger than current max (1.2)\n",
            "  Step 2: Check index 1, value = 3.8\n",
            "           New maximum! Update max_index to 1\n",
            "  Step 3: Check index 2, value = 2.1\n",
            "           Not larger than current max (3.8)\n",
            "  Step 4: Check index 3, value = 0.5\n",
            "           Not larger than current max (3.8)\n",
            "  Final result: max_index = 1, max_value = 3.8\n",
            "\n",
            "‚ö° Using np.argmax():\n",
            "  np.argmax(q_values) = 1\n",
            "  Same result: True ‚úÖ\n",
            "\n",
            "============================================================\n",
            "üìù SUMMARY\n",
            "============================================================\n",
            "‚Ä¢ np.argmax(array) returns the INDEX of the largest value\n",
            "‚Ä¢ In Q-learning: argmax selects the action with highest Q-value\n",
            "‚Ä¢ If multiple values tie for max, argmax returns the first index\n",
            "‚Ä¢ This is how agents choose the 'best' action (exploitation)\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# QLearningAgent (Bare)"
      ],
      "metadata": {
        "id": "Hps9Vj4q7FqQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "7Fx5y7dWtqnL"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "class QLearningAgent:\n",
        "    def __init__(self, num_states, num_actions, lr=0.1, gamma=0.95, epsilon=0.1):\n",
        "        # Initialize Q-table with zeros\n",
        "        self.q_table = np.zeros((num_states, num_actions))\n",
        "        self.lr = lr\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def get_action(self, state):\n",
        "        # Epsilon-greedy action selection\n",
        "        if random.random() < self.epsilon:\n",
        "            return random.randint(0, len(self.q_table[state]) - 1)\n",
        "        else:\n",
        "            return np.argmax(self.q_table[state])  # Direct table lookup\n",
        "\n",
        "    def update(self, state, action, reward, next_state, done):\n",
        "        # Direct Q-table update using Bellman equation\n",
        "        if done:\n",
        "            target = reward\n",
        "        else:\n",
        "            target = reward + self.gamma * np.max(self.q_table[next_state])\n",
        "\n",
        "        # Update specific table entry\n",
        "        self.q_table[state, action] += self.lr * (target - self.q_table[state, action])\n",
        "\n",
        "# Usage\n",
        "agent = QLearningAgent(num_states=100, num_actions=4)\n",
        "state = 5\n",
        "action = agent.get_action(state)  # Just lookup q_table[5]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# QLearningAgent (Verbose)"
      ],
      "metadata": {
        "id": "hquyBYYn7iGd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "class QLearningAgent:\n",
        "    def __init__(self, num_states, num_actions, lr=0.1, gamma=0.95, epsilon=0.1):\n",
        "        # Initialize Q-table with zeros\n",
        "        self.q_table = np.zeros((num_states, num_actions))\n",
        "        self.lr = lr\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.num_states = num_states\n",
        "        self.num_actions = num_actions\n",
        "\n",
        "        # Logging counters\n",
        "        self.exploration_count = 0\n",
        "        self.exploitation_count = 0\n",
        "        self.update_count = 0\n",
        "\n",
        "        print(f\"ü§ñ Q-Learning Agent Initialized:\")\n",
        "        print(f\"   üìä Q-table shape   : {self.q_table.shape}\")\n",
        "        print(f\"   üìà Learning rate (lr)     : {self.lr}\")\n",
        "        print(f\"   üí∞ Discount fact (gamma)  : {self.gamma}\")\n",
        "        print(f\"   üéØ Explortn rate (epsilon): {self.epsilon}\")\n",
        "        print(f\"   üß† Initial Q-table (all zeros):\")\n",
        "        print(f\"{self.q_table}\\n\")\n",
        "\n",
        "    def get_action(self, state, verbose=True):\n",
        "        \"\"\"Get action using epsilon-greedy strategy with detailed logging\"\"\"\n",
        "\n",
        "        # Get current Q-values for this state\n",
        "        current_q_values = self.q_table[state]\n",
        "        best_action = np.argmax(current_q_values)\n",
        "        best_q_value = current_q_values[best_action]\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"üéÆ Action Selection for State {state}:\")\n",
        "            print(f\"   üìã Current Q-values: {current_q_values}\")\n",
        "            print(f\"   ‚≠ê Best action would be: {best_action} (Q-value: {best_q_value:.3f})\")\n",
        "\n",
        "        # Epsilon-greedy decision\n",
        "        random_prob = random.random()\n",
        "        if random_prob < self.epsilon:\n",
        "            # Exploration: choose random action\n",
        "            action = random.randint(0, self.num_actions - 1)\n",
        "            self.exploration_count += 1\n",
        "            if verbose:\n",
        "                print(f\"   üé≤ EXPLORING! Random prob {random_prob:.3f} < epsilon {self.epsilon}\")\n",
        "                print(f\"   ‚û°Ô∏è  Chose random action: {action}\")\n",
        "                print(f\"   üìä Exploration count: {self.exploration_count}\")\n",
        "        else:\n",
        "            # Exploitation: choose best known action\n",
        "            action = best_action\n",
        "            self.exploitation_count += 1\n",
        "            if verbose:\n",
        "                print(f\"   üéØ EXPLOITING! Random prob {random_prob:.3f} >= epsilon {self.epsilon}\")\n",
        "                print(f\"   ‚û°Ô∏è  Chose best action: {action}\")\n",
        "                print(f\"   üìä Exploitation count: {self.exploitation_count}\")\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"   üîÑ Explore/Exploit ratio: {self.exploration_count}/{self.exploitation_count}\\n\")\n",
        "\n",
        "        return action\n",
        "\n",
        "    def update(self, state, action, reward, next_state, done, verbose=True):\n",
        "        \"\"\"Update Q-table with detailed logging of the learning process\"\"\"\n",
        "\n",
        "        self.update_count += 1\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"üìö Q-Learning Update #{self.update_count}:\")\n",
        "            print(f\"   üèÅ State: {state} ‚Üí Action: {action} ‚Üí Reward: {reward} ‚Üí Next State: {next_state}\")\n",
        "            print(f\"   ‚ö° Episode done: {done}\")\n",
        "\n",
        "        # Store old Q-value for comparison\n",
        "        old_q_value = self.q_table[state, action]\n",
        "\n",
        "        # Calculate target Q-value\n",
        "        if done:\n",
        "            target = reward\n",
        "            if verbose:\n",
        "                print(f\"   üéØ Target calculation (episode ended):\")\n",
        "                print(f\"      Target = reward = {reward}\")\n",
        "        else:\n",
        "            next_q_values = self.q_table[next_state]\n",
        "            max_next_q = np.max(next_q_values)\n",
        "            best_next_action = np.argmax(next_q_values)\n",
        "            target = reward + self.gamma * max_next_q\n",
        "\n",
        "            if verbose:\n",
        "                print(f\"   üéØ Target calculation (episode continues):\")\n",
        "                print(f\"      Next state Q-values: {next_q_values}\")\n",
        "                print(f\"      Best next action: {best_next_action} (Q-value: {max_next_q:.3f})\")\n",
        "                print(f\"      Target = reward + gamma * max_next_Q\")\n",
        "                print(f\"      Target = {reward} + {self.gamma} * {max_next_q:.3f} = {target:.3f}\")\n",
        "\n",
        "        # Calculate temporal difference error\n",
        "        td_error = target - old_q_value\n",
        "\n",
        "        # Update Q-value using Q-learning formula\n",
        "        new_q_value = old_q_value + self.lr * td_error\n",
        "        self.q_table[state, action] = new_q_value\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"   üîÑ Q-value Update:\")\n",
        "            print(f\"      Old Q-value: {old_q_value:.3f}\")\n",
        "            print(f\"      TD Error: target - old = {target:.3f} - {old_q_value:.3f} = {td_error:.3f}\")\n",
        "            print(f\"      Learning step: lr * TD_error = {self.lr} * {td_error:.3f} = {self.lr * td_error:.3f}\")\n",
        "            print(f\"      New Q-value: old + learning_step = {old_q_value:.3f} + {self.lr * td_error:.3f} = {new_q_value:.3f}\")\n",
        "            print(f\"      üìà Change: {new_q_value - old_q_value:+.3f}\")\n",
        "\n",
        "        return td_error\n",
        "\n",
        "    def print_q_table(self, title=\"Current Q-Table\"):\n",
        "        \"\"\"Print the current Q-table in a readable format\"\"\"\n",
        "        print(f\"\\nüìä {title}:\")\n",
        "        print(\"State\\\\Action\", end=\"\")\n",
        "        for a in range(self.num_actions):\n",
        "            print(f\"     Action{a}\", end=\"\")\n",
        "        print()\n",
        "        print(\"-\" * (12 + 12 * self.num_actions))\n",
        "\n",
        "        for s in range(self.num_states):\n",
        "            print(f\"State {s:2d}   \", end=\"\")\n",
        "            for a in range(self.num_actions):\n",
        "                print(f\"{self.q_table[s, a]:8.3f}    \", end=\"\")\n",
        "            print()\n",
        "        print()\n",
        "\n",
        "    def print_policy(self):\n",
        "        \"\"\"Print the current policy (best action for each state)\"\"\"\n",
        "        print(\"üéØ Current Policy (Best Action per State):\")\n",
        "        for s in range(self.num_states):\n",
        "            best_action = np.argmax(self.q_table[s])\n",
        "            best_q_value = self.q_table[s, best_action]\n",
        "            print(f\"   State {s}: Action {best_action} (Q-value: {best_q_value:.3f})\")\n",
        "        print()\n",
        "\n",
        "\n",
        "def simple_environment_step(state, action, num_states=5):\n",
        "    \"\"\"\n",
        "    Simple environment for demonstration:\n",
        "    - States: 0, 1, 2, 3, 4\n",
        "    - Actions: 0 (left), 1 (right)\n",
        "    - Goal: Reach state 4 (rightmost)\n",
        "    - Reward: +10 for reaching goal, -1 for each step, -5 for going out of bounds\n",
        "    \"\"\"\n",
        "\n",
        "    if action == 0:  # Move left\n",
        "        next_state = max(0, state - 1)\n",
        "    else:  # Move right (action == 1)\n",
        "        next_state = min(num_states - 1, state + 1)\n",
        "\n",
        "    # Calculate reward\n",
        "    if next_state == num_states - 1:  # Reached goal\n",
        "        reward = 10\n",
        "        done = True\n",
        "    elif (action == 0 and state == 0) or (action == 1 and state == num_states - 1):\n",
        "        # Tried to go out of bounds\n",
        "        reward = -5\n",
        "        done = False\n",
        "    else:\n",
        "        reward = -1  # Normal step cost\n",
        "        done = False\n",
        "\n",
        "    return next_state, reward, done\n",
        "\n",
        "\n",
        "def train_agent(episodes=5, max_steps_per_episode=10, verbose=True):\n",
        "    \"\"\"Train the Q-learning agent with detailed logging\"\"\"\n",
        "\n",
        "    # Initialize agent\n",
        "    agent = QLearningAgent(num_states=5, num_actions=2, lr=0.1, gamma=0.9, epsilon=0.3)\n",
        "\n",
        "    print(\"üöÄ Starting Training!\\n\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    total_rewards = []\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        print(f\"\\nüé¨ EPISODE {episode + 1}/{episodes}\")\n",
        "        print(\"=\" * 40)\n",
        "\n",
        "        # Reset environment\n",
        "        state = 0  # Always start at leftmost state\n",
        "        episode_reward = 0\n",
        "        step_count = 0\n",
        "\n",
        "        print(f\"üèÅ Starting at state {state}\")\n",
        "\n",
        "        for step in range(max_steps_per_episode):\n",
        "            step_count += 1\n",
        "            print(f\"\\nüìç Step {step_count}:\")\n",
        "            print(\"-\" * 20)\n",
        "\n",
        "            # Get action\n",
        "            action = agent.get_action(state, verbose=verbose)\n",
        "\n",
        "            # Take action in environment\n",
        "            next_state, reward, done = simple_environment_step(state, action)\n",
        "            episode_reward += reward\n",
        "\n",
        "            if verbose:\n",
        "                print(f\"üåç Environment Response:\")\n",
        "                print(f\"   üé¨ Action taken: {action} ({'left' if action == 0 else 'right'})\")\n",
        "                print(f\"   üìç State transition: {state} ‚Üí {next_state}\")\n",
        "                print(f\"   üí∞ Reward received: {reward}\")\n",
        "                print(f\"   üìä Episode reward so far: {episode_reward}\")\n",
        "\n",
        "            # Update Q-table\n",
        "            td_error = agent.update(state, action, reward, next_state, done, verbose=verbose)\n",
        "\n",
        "            # Move to next state\n",
        "            state = next_state\n",
        "\n",
        "            if done:\n",
        "                if verbose:\n",
        "                    print(f\"‚úÖ Episode completed! Goal reached in {step_count} steps!\")\n",
        "                break\n",
        "\n",
        "            if step_count >= max_steps_per_episode:\n",
        "                if verbose:\n",
        "                    print(f\"‚è∞ Episode ended: Maximum steps ({max_steps_per_episode}) reached\")\n",
        "                break\n",
        "\n",
        "        total_rewards.append(episode_reward)\n",
        "\n",
        "        print(f\"\\nüìà Episode {episode + 1} Summary:\")\n",
        "        print(f\"   Total Reward: {episode_reward}\")\n",
        "        print(f\"   Steps Taken: {step_count}\")\n",
        "        print(f\"   Goal Reached: {'Yes' if done else 'No'}\")\n",
        "\n",
        "        # Print Q-table after each episode\n",
        "        agent.print_q_table(f\"Q-Table after Episode {episode + 1}\")\n",
        "        agent.print_policy()\n",
        "\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "    print(f\"\\nüéâ Training Complete!\")\n",
        "    print(f\"üìä Total Episodes: {episodes}\")\n",
        "    print(f\"üìà Rewards per Episode: {total_rewards}\")\n",
        "    print(f\"üéØ Average Reward: {np.mean(total_rewards):.2f}\")\n",
        "    print(f\"üîç Exploration vs Exploitation: {agent.exploration_count} vs {agent.exploitation_count}\")\n",
        "\n",
        "    agent.print_q_table(\"Final Q-Table\")\n",
        "    agent.print_policy()\n",
        "\n",
        "    return agent\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Train with detailed logging\n",
        "    trained_agent = train_agent(episodes=3, max_steps_per_episode=8, verbose=True)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"üß™ Testing the trained agent (no more learning, just exploitation):\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Test the trained agent\n",
        "    trained_agent.epsilon = 0.0  # No more exploration, pure exploitation\n",
        "    state = 0\n",
        "    step = 0\n",
        "\n",
        "    print(f\"üèÅ Starting test at state {state}\")\n",
        "\n",
        "    while step < 10:\n",
        "        step += 1\n",
        "        action = trained_agent.get_action(state, verbose=True)\n",
        "        next_state, reward, done = simple_environment_step(state, action)\n",
        "        print(f\"   üé¨ Took action {action} ‚Üí moved to state {next_state}, got reward {reward}\")\n",
        "        state = next_state\n",
        "\n",
        "        if done:\n",
        "            print(f\"‚úÖ Goal reached in {step} steps!\")\n",
        "            break\n",
        "\n",
        "    if not done:\n",
        "        print(f\"‚ùå Failed to reach goal in {step} steps\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I1x29bq63ZVM",
        "outputId": "1eb0612c-bc78-4f7c-b210-05249ed43f73"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ü§ñ Q-Learning Agent Initialized:\n",
            "   üìä Q-table shape: (5, 2)\n",
            "   üìà Learning rate (lr): 0.1\n",
            "   üí∞ Discount factor (gamma): 0.9\n",
            "   üéØ Exploration rate (epsilon): 0.3\n",
            "   üß† Initial Q-table (all zeros):\n",
            "      [[0. 0.]\n",
            " [0. 0.]\n",
            " [0. 0.]\n",
            " [0. 0.]\n",
            " [0. 0.]]\n",
            "\n",
            "üöÄ Starting Training!\n",
            "\n",
            "================================================================================\n",
            "\n",
            "üé¨ EPISODE 1/3\n",
            "========================================\n",
            "üèÅ Starting at state 0\n",
            "\n",
            "üìç Step 1:\n",
            "--------------------\n",
            "üéÆ Action Selection for State 0:\n",
            "   üìã Current Q-values: [0. 0.]\n",
            "   ‚≠ê Best action would be: 0 (Q-value: 0.000)\n",
            "   üéØ EXPLOITING! Random prob 0.311 >= epsilon 0.3\n",
            "   ‚û°Ô∏è  Chose best action: 0\n",
            "   üìä Exploitation count: 1\n",
            "   üîÑ Explore/Exploit ratio: 0/1\n",
            "\n",
            "üåç Environment Response:\n",
            "   üé¨ Action taken: 0 (left)\n",
            "   üìç State transition: 0 ‚Üí 0\n",
            "   üí∞ Reward received: -5\n",
            "   üìä Episode reward so far: -5\n",
            "üìö Q-Learning Update #1:\n",
            "   üèÅ State: 0 ‚Üí Action: 0 ‚Üí Reward: -5 ‚Üí Next State: 0\n",
            "   ‚ö° Episode done: False\n",
            "   üéØ Target calculation (episode continues):\n",
            "      Next state Q-values: [0. 0.]\n",
            "      Best next action: 0 (Q-value: 0.000)\n",
            "      Target = reward + gamma * max_next_Q\n",
            "      Target = -5 + 0.9 * 0.000 = -5.000\n",
            "   üîÑ Q-value Update:\n",
            "      Old Q-value: 0.000\n",
            "      TD Error: target - old = -5.000 - 0.000 = -5.000\n",
            "      Learning step: lr * TD_error = 0.1 * -5.000 = -0.500\n",
            "      New Q-value: old + learning_step = 0.000 + -0.500 = -0.500\n",
            "      üìà Change: -0.500\n",
            "\n",
            "üìç Step 2:\n",
            "--------------------\n",
            "üéÆ Action Selection for State 0:\n",
            "   üìã Current Q-values: [-0.5  0. ]\n",
            "   ‚≠ê Best action would be: 1 (Q-value: 0.000)\n",
            "   üé≤ EXPLORING! Random prob 0.046 < epsilon 0.3\n",
            "   ‚û°Ô∏è  Chose random action: 1\n",
            "   üìä Exploration count: 1\n",
            "   üîÑ Explore/Exploit ratio: 1/1\n",
            "\n",
            "üåç Environment Response:\n",
            "   üé¨ Action taken: 1 (right)\n",
            "   üìç State transition: 0 ‚Üí 1\n",
            "   üí∞ Reward received: -1\n",
            "   üìä Episode reward so far: -6\n",
            "üìö Q-Learning Update #2:\n",
            "   üèÅ State: 0 ‚Üí Action: 1 ‚Üí Reward: -1 ‚Üí Next State: 1\n",
            "   ‚ö° Episode done: False\n",
            "   üéØ Target calculation (episode continues):\n",
            "      Next state Q-values: [0. 0.]\n",
            "      Best next action: 0 (Q-value: 0.000)\n",
            "      Target = reward + gamma * max_next_Q\n",
            "      Target = -1 + 0.9 * 0.000 = -1.000\n",
            "   üîÑ Q-value Update:\n",
            "      Old Q-value: 0.000\n",
            "      TD Error: target - old = -1.000 - 0.000 = -1.000\n",
            "      Learning step: lr * TD_error = 0.1 * -1.000 = -0.100\n",
            "      New Q-value: old + learning_step = 0.000 + -0.100 = -0.100\n",
            "      üìà Change: -0.100\n",
            "\n",
            "üìç Step 3:\n",
            "--------------------\n",
            "üéÆ Action Selection for State 1:\n",
            "   üìã Current Q-values: [0. 0.]\n",
            "   ‚≠ê Best action would be: 0 (Q-value: 0.000)\n",
            "   üé≤ EXPLORING! Random prob 0.099 < epsilon 0.3\n",
            "   ‚û°Ô∏è  Chose random action: 0\n",
            "   üìä Exploration count: 2\n",
            "   üîÑ Explore/Exploit ratio: 2/1\n",
            "\n",
            "üåç Environment Response:\n",
            "   üé¨ Action taken: 0 (left)\n",
            "   üìç State transition: 1 ‚Üí 0\n",
            "   üí∞ Reward received: -1\n",
            "   üìä Episode reward so far: -7\n",
            "üìö Q-Learning Update #3:\n",
            "   üèÅ State: 1 ‚Üí Action: 0 ‚Üí Reward: -1 ‚Üí Next State: 0\n",
            "   ‚ö° Episode done: False\n",
            "   üéØ Target calculation (episode continues):\n",
            "      Next state Q-values: [-0.5 -0.1]\n",
            "      Best next action: 1 (Q-value: -0.100)\n",
            "      Target = reward + gamma * max_next_Q\n",
            "      Target = -1 + 0.9 * -0.100 = -1.090\n",
            "   üîÑ Q-value Update:\n",
            "      Old Q-value: 0.000\n",
            "      TD Error: target - old = -1.090 - 0.000 = -1.090\n",
            "      Learning step: lr * TD_error = 0.1 * -1.090 = -0.109\n",
            "      New Q-value: old + learning_step = 0.000 + -0.109 = -0.109\n",
            "      üìà Change: -0.109\n",
            "\n",
            "üìç Step 4:\n",
            "--------------------\n",
            "üéÆ Action Selection for State 0:\n",
            "   üìã Current Q-values: [-0.5 -0.1]\n",
            "   ‚≠ê Best action would be: 1 (Q-value: -0.100)\n",
            "   üéØ EXPLOITING! Random prob 0.969 >= epsilon 0.3\n",
            "   ‚û°Ô∏è  Chose best action: 1\n",
            "   üìä Exploitation count: 2\n",
            "   üîÑ Explore/Exploit ratio: 2/2\n",
            "\n",
            "üåç Environment Response:\n",
            "   üé¨ Action taken: 1 (right)\n",
            "   üìç State transition: 0 ‚Üí 1\n",
            "   üí∞ Reward received: -1\n",
            "   üìä Episode reward so far: -8\n",
            "üìö Q-Learning Update #4:\n",
            "   üèÅ State: 0 ‚Üí Action: 1 ‚Üí Reward: -1 ‚Üí Next State: 1\n",
            "   ‚ö° Episode done: False\n",
            "   üéØ Target calculation (episode continues):\n",
            "      Next state Q-values: [-0.109  0.   ]\n",
            "      Best next action: 1 (Q-value: 0.000)\n",
            "      Target = reward + gamma * max_next_Q\n",
            "      Target = -1 + 0.9 * 0.000 = -1.000\n",
            "   üîÑ Q-value Update:\n",
            "      Old Q-value: -0.100\n",
            "      TD Error: target - old = -1.000 - -0.100 = -0.900\n",
            "      Learning step: lr * TD_error = 0.1 * -0.900 = -0.090\n",
            "      New Q-value: old + learning_step = -0.100 + -0.090 = -0.190\n",
            "      üìà Change: -0.090\n",
            "\n",
            "üìç Step 5:\n",
            "--------------------\n",
            "üéÆ Action Selection for State 1:\n",
            "   üìã Current Q-values: [-0.109  0.   ]\n",
            "   ‚≠ê Best action would be: 1 (Q-value: 0.000)\n",
            "   üéØ EXPLOITING! Random prob 0.401 >= epsilon 0.3\n",
            "   ‚û°Ô∏è  Chose best action: 1\n",
            "   üìä Exploitation count: 3\n",
            "   üîÑ Explore/Exploit ratio: 2/3\n",
            "\n",
            "üåç Environment Response:\n",
            "   üé¨ Action taken: 1 (right)\n",
            "   üìç State transition: 1 ‚Üí 2\n",
            "   üí∞ Reward received: -1\n",
            "   üìä Episode reward so far: -9\n",
            "üìö Q-Learning Update #5:\n",
            "   üèÅ State: 1 ‚Üí Action: 1 ‚Üí Reward: -1 ‚Üí Next State: 2\n",
            "   ‚ö° Episode done: False\n",
            "   üéØ Target calculation (episode continues):\n",
            "      Next state Q-values: [0. 0.]\n",
            "      Best next action: 0 (Q-value: 0.000)\n",
            "      Target = reward + gamma * max_next_Q\n",
            "      Target = -1 + 0.9 * 0.000 = -1.000\n",
            "   üîÑ Q-value Update:\n",
            "      Old Q-value: 0.000\n",
            "      TD Error: target - old = -1.000 - 0.000 = -1.000\n",
            "      Learning step: lr * TD_error = 0.1 * -1.000 = -0.100\n",
            "      New Q-value: old + learning_step = 0.000 + -0.100 = -0.100\n",
            "      üìà Change: -0.100\n",
            "\n",
            "üìç Step 6:\n",
            "--------------------\n",
            "üéÆ Action Selection for State 2:\n",
            "   üìã Current Q-values: [0. 0.]\n",
            "   ‚≠ê Best action would be: 0 (Q-value: 0.000)\n",
            "   üé≤ EXPLORING! Random prob 0.147 < epsilon 0.3\n",
            "   ‚û°Ô∏è  Chose random action: 1\n",
            "   üìä Exploration count: 3\n",
            "   üîÑ Explore/Exploit ratio: 3/3\n",
            "\n",
            "üåç Environment Response:\n",
            "   üé¨ Action taken: 1 (right)\n",
            "   üìç State transition: 2 ‚Üí 3\n",
            "   üí∞ Reward received: -1\n",
            "   üìä Episode reward so far: -10\n",
            "üìö Q-Learning Update #6:\n",
            "   üèÅ State: 2 ‚Üí Action: 1 ‚Üí Reward: -1 ‚Üí Next State: 3\n",
            "   ‚ö° Episode done: False\n",
            "   üéØ Target calculation (episode continues):\n",
            "      Next state Q-values: [0. 0.]\n",
            "      Best next action: 0 (Q-value: 0.000)\n",
            "      Target = reward + gamma * max_next_Q\n",
            "      Target = -1 + 0.9 * 0.000 = -1.000\n",
            "   üîÑ Q-value Update:\n",
            "      Old Q-value: 0.000\n",
            "      TD Error: target - old = -1.000 - 0.000 = -1.000\n",
            "      Learning step: lr * TD_error = 0.1 * -1.000 = -0.100\n",
            "      New Q-value: old + learning_step = 0.000 + -0.100 = -0.100\n",
            "      üìà Change: -0.100\n",
            "\n",
            "üìç Step 7:\n",
            "--------------------\n",
            "üéÆ Action Selection for State 3:\n",
            "   üìã Current Q-values: [0. 0.]\n",
            "   ‚≠ê Best action would be: 0 (Q-value: 0.000)\n",
            "   üé≤ EXPLORING! Random prob 0.136 < epsilon 0.3\n",
            "   ‚û°Ô∏è  Chose random action: 1\n",
            "   üìä Exploration count: 4\n",
            "   üîÑ Explore/Exploit ratio: 4/3\n",
            "\n",
            "üåç Environment Response:\n",
            "   üé¨ Action taken: 1 (right)\n",
            "   üìç State transition: 3 ‚Üí 4\n",
            "   üí∞ Reward received: 10\n",
            "   üìä Episode reward so far: 0\n",
            "üìö Q-Learning Update #7:\n",
            "   üèÅ State: 3 ‚Üí Action: 1 ‚Üí Reward: 10 ‚Üí Next State: 4\n",
            "   ‚ö° Episode done: True\n",
            "   üéØ Target calculation (episode ended):\n",
            "      Target = reward = 10\n",
            "   üîÑ Q-value Update:\n",
            "      Old Q-value: 0.000\n",
            "      TD Error: target - old = 10.000 - 0.000 = 10.000\n",
            "      Learning step: lr * TD_error = 0.1 * 10.000 = 1.000\n",
            "      New Q-value: old + learning_step = 0.000 + 1.000 = 1.000\n",
            "      üìà Change: +1.000\n",
            "‚úÖ Episode completed! Goal reached in 7 steps!\n",
            "\n",
            "üìà Episode 1 Summary:\n",
            "   Total Reward: 0\n",
            "   Steps Taken: 7\n",
            "   Goal Reached: Yes\n",
            "\n",
            "üìä Q-Table after Episode 1:\n",
            "State\\Action     Action0     Action1\n",
            "------------------------------------\n",
            "State  0     -0.500      -0.190    \n",
            "State  1     -0.109      -0.100    \n",
            "State  2      0.000      -0.100    \n",
            "State  3      0.000       1.000    \n",
            "State  4      0.000       0.000    \n",
            "\n",
            "üéØ Current Policy (Best Action per State):\n",
            "   State 0: Action 1 (Q-value: -0.190)\n",
            "   State 1: Action 1 (Q-value: -0.100)\n",
            "   State 2: Action 0 (Q-value: 0.000)\n",
            "   State 3: Action 1 (Q-value: 1.000)\n",
            "   State 4: Action 0 (Q-value: 0.000)\n",
            "\n",
            "================================================================================\n",
            "\n",
            "üé¨ EPISODE 2/3\n",
            "========================================\n",
            "üèÅ Starting at state 0\n",
            "\n",
            "üìç Step 1:\n",
            "--------------------\n",
            "üéÆ Action Selection for State 0:\n",
            "   üìã Current Q-values: [-0.5  -0.19]\n",
            "   ‚≠ê Best action would be: 1 (Q-value: -0.190)\n",
            "   üé≤ EXPLORING! Random prob 0.056 < epsilon 0.3\n",
            "   ‚û°Ô∏è  Chose random action: 0\n",
            "   üìä Exploration count: 5\n",
            "   üîÑ Explore/Exploit ratio: 5/3\n",
            "\n",
            "üåç Environment Response:\n",
            "   üé¨ Action taken: 0 (left)\n",
            "   üìç State transition: 0 ‚Üí 0\n",
            "   üí∞ Reward received: -5\n",
            "   üìä Episode reward so far: -5\n",
            "üìö Q-Learning Update #8:\n",
            "   üèÅ State: 0 ‚Üí Action: 0 ‚Üí Reward: -5 ‚Üí Next State: 0\n",
            "   ‚ö° Episode done: False\n",
            "   üéØ Target calculation (episode continues):\n",
            "      Next state Q-values: [-0.5  -0.19]\n",
            "      Best next action: 1 (Q-value: -0.190)\n",
            "      Target = reward + gamma * max_next_Q\n",
            "      Target = -5 + 0.9 * -0.190 = -5.171\n",
            "   üîÑ Q-value Update:\n",
            "      Old Q-value: -0.500\n",
            "      TD Error: target - old = -5.171 - -0.500 = -4.671\n",
            "      Learning step: lr * TD_error = 0.1 * -4.671 = -0.467\n",
            "      New Q-value: old + learning_step = -0.500 + -0.467 = -0.967\n",
            "      üìà Change: -0.467\n",
            "\n",
            "üìç Step 2:\n",
            "--------------------\n",
            "üéÆ Action Selection for State 0:\n",
            "   üìã Current Q-values: [-0.9671 -0.19  ]\n",
            "   ‚≠ê Best action would be: 1 (Q-value: -0.190)\n",
            "   üéØ EXPLOITING! Random prob 0.466 >= epsilon 0.3\n",
            "   ‚û°Ô∏è  Chose best action: 1\n",
            "   üìä Exploitation count: 4\n",
            "   üîÑ Explore/Exploit ratio: 5/4\n",
            "\n",
            "üåç Environment Response:\n",
            "   üé¨ Action taken: 1 (right)\n",
            "   üìç State transition: 0 ‚Üí 1\n",
            "   üí∞ Reward received: -1\n",
            "   üìä Episode reward so far: -6\n",
            "üìö Q-Learning Update #9:\n",
            "   üèÅ State: 0 ‚Üí Action: 1 ‚Üí Reward: -1 ‚Üí Next State: 1\n",
            "   ‚ö° Episode done: False\n",
            "   üéØ Target calculation (episode continues):\n",
            "      Next state Q-values: [-0.109 -0.1  ]\n",
            "      Best next action: 1 (Q-value: -0.100)\n",
            "      Target = reward + gamma * max_next_Q\n",
            "      Target = -1 + 0.9 * -0.100 = -1.090\n",
            "   üîÑ Q-value Update:\n",
            "      Old Q-value: -0.190\n",
            "      TD Error: target - old = -1.090 - -0.190 = -0.900\n",
            "      Learning step: lr * TD_error = 0.1 * -0.900 = -0.090\n",
            "      New Q-value: old + learning_step = -0.190 + -0.090 = -0.280\n",
            "      üìà Change: -0.090\n",
            "\n",
            "üìç Step 3:\n",
            "--------------------\n",
            "üéÆ Action Selection for State 1:\n",
            "   üìã Current Q-values: [-0.109 -0.1  ]\n",
            "   ‚≠ê Best action would be: 1 (Q-value: -0.100)\n",
            "   üé≤ EXPLORING! Random prob 0.184 < epsilon 0.3\n",
            "   ‚û°Ô∏è  Chose random action: 1\n",
            "   üìä Exploration count: 6\n",
            "   üîÑ Explore/Exploit ratio: 6/4\n",
            "\n",
            "üåç Environment Response:\n",
            "   üé¨ Action taken: 1 (right)\n",
            "   üìç State transition: 1 ‚Üí 2\n",
            "   üí∞ Reward received: -1\n",
            "   üìä Episode reward so far: -7\n",
            "üìö Q-Learning Update #10:\n",
            "   üèÅ State: 1 ‚Üí Action: 1 ‚Üí Reward: -1 ‚Üí Next State: 2\n",
            "   ‚ö° Episode done: False\n",
            "   üéØ Target calculation (episode continues):\n",
            "      Next state Q-values: [ 0.  -0.1]\n",
            "      Best next action: 0 (Q-value: 0.000)\n",
            "      Target = reward + gamma * max_next_Q\n",
            "      Target = -1 + 0.9 * 0.000 = -1.000\n",
            "   üîÑ Q-value Update:\n",
            "      Old Q-value: -0.100\n",
            "      TD Error: target - old = -1.000 - -0.100 = -0.900\n",
            "      Learning step: lr * TD_error = 0.1 * -0.900 = -0.090\n",
            "      New Q-value: old + learning_step = -0.100 + -0.090 = -0.190\n",
            "      üìà Change: -0.090\n",
            "\n",
            "üìç Step 4:\n",
            "--------------------\n",
            "üéÆ Action Selection for State 2:\n",
            "   üìã Current Q-values: [ 0.  -0.1]\n",
            "   ‚≠ê Best action would be: 0 (Q-value: 0.000)\n",
            "   üéØ EXPLOITING! Random prob 0.537 >= epsilon 0.3\n",
            "   ‚û°Ô∏è  Chose best action: 0\n",
            "   üìä Exploitation count: 5\n",
            "   üîÑ Explore/Exploit ratio: 6/5\n",
            "\n",
            "üåç Environment Response:\n",
            "   üé¨ Action taken: 0 (left)\n",
            "   üìç State transition: 2 ‚Üí 1\n",
            "   üí∞ Reward received: -1\n",
            "   üìä Episode reward so far: -8\n",
            "üìö Q-Learning Update #11:\n",
            "   üèÅ State: 2 ‚Üí Action: 0 ‚Üí Reward: -1 ‚Üí Next State: 1\n",
            "   ‚ö° Episode done: False\n",
            "   üéØ Target calculation (episode continues):\n",
            "      Next state Q-values: [-0.109 -0.19 ]\n",
            "      Best next action: 0 (Q-value: -0.109)\n",
            "      Target = reward + gamma * max_next_Q\n",
            "      Target = -1 + 0.9 * -0.109 = -1.098\n",
            "   üîÑ Q-value Update:\n",
            "      Old Q-value: 0.000\n",
            "      TD Error: target - old = -1.098 - 0.000 = -1.098\n",
            "      Learning step: lr * TD_error = 0.1 * -1.098 = -0.110\n",
            "      New Q-value: old + learning_step = 0.000 + -0.110 = -0.110\n",
            "      üìà Change: -0.110\n",
            "\n",
            "üìç Step 5:\n",
            "--------------------\n",
            "üéÆ Action Selection for State 1:\n",
            "   üìã Current Q-values: [-0.109 -0.19 ]\n",
            "   ‚≠ê Best action would be: 0 (Q-value: -0.109)\n",
            "   üéØ EXPLOITING! Random prob 0.559 >= epsilon 0.3\n",
            "   ‚û°Ô∏è  Chose best action: 0\n",
            "   üìä Exploitation count: 6\n",
            "   üîÑ Explore/Exploit ratio: 6/6\n",
            "\n",
            "üåç Environment Response:\n",
            "   üé¨ Action taken: 0 (left)\n",
            "   üìç State transition: 1 ‚Üí 0\n",
            "   üí∞ Reward received: -1\n",
            "   üìä Episode reward so far: -9\n",
            "üìö Q-Learning Update #12:\n",
            "   üèÅ State: 1 ‚Üí Action: 0 ‚Üí Reward: -1 ‚Üí Next State: 0\n",
            "   ‚ö° Episode done: False\n",
            "   üéØ Target calculation (episode continues):\n",
            "      Next state Q-values: [-0.9671 -0.28  ]\n",
            "      Best next action: 1 (Q-value: -0.280)\n",
            "      Target = reward + gamma * max_next_Q\n",
            "      Target = -1 + 0.9 * -0.280 = -1.252\n",
            "   üîÑ Q-value Update:\n",
            "      Old Q-value: -0.109\n",
            "      TD Error: target - old = -1.252 - -0.109 = -1.143\n",
            "      Learning step: lr * TD_error = 0.1 * -1.143 = -0.114\n",
            "      New Q-value: old + learning_step = -0.109 + -0.114 = -0.223\n",
            "      üìà Change: -0.114\n",
            "\n",
            "üìç Step 6:\n",
            "--------------------\n",
            "üéÆ Action Selection for State 0:\n",
            "   üìã Current Q-values: [-0.9671 -0.28  ]\n",
            "   ‚≠ê Best action would be: 1 (Q-value: -0.280)\n",
            "   üéØ EXPLOITING! Random prob 0.579 >= epsilon 0.3\n",
            "   ‚û°Ô∏è  Chose best action: 1\n",
            "   üìä Exploitation count: 7\n",
            "   üîÑ Explore/Exploit ratio: 6/7\n",
            "\n",
            "üåç Environment Response:\n",
            "   üé¨ Action taken: 1 (right)\n",
            "   üìç State transition: 0 ‚Üí 1\n",
            "   üí∞ Reward received: -1\n",
            "   üìä Episode reward so far: -10\n",
            "üìö Q-Learning Update #13:\n",
            "   üèÅ State: 0 ‚Üí Action: 1 ‚Üí Reward: -1 ‚Üí Next State: 1\n",
            "   ‚ö° Episode done: False\n",
            "   üéØ Target calculation (episode continues):\n",
            "      Next state Q-values: [-0.2233 -0.19  ]\n",
            "      Best next action: 1 (Q-value: -0.190)\n",
            "      Target = reward + gamma * max_next_Q\n",
            "      Target = -1 + 0.9 * -0.190 = -1.171\n",
            "   üîÑ Q-value Update:\n",
            "      Old Q-value: -0.280\n",
            "      TD Error: target - old = -1.171 - -0.280 = -0.891\n",
            "      Learning step: lr * TD_error = 0.1 * -0.891 = -0.089\n",
            "      New Q-value: old + learning_step = -0.280 + -0.089 = -0.369\n",
            "      üìà Change: -0.089\n",
            "\n",
            "üìç Step 7:\n",
            "--------------------\n",
            "üéÆ Action Selection for State 1:\n",
            "   üìã Current Q-values: [-0.2233 -0.19  ]\n",
            "   ‚≠ê Best action would be: 1 (Q-value: -0.190)\n",
            "   üéØ EXPLOITING! Random prob 0.511 >= epsilon 0.3\n",
            "   ‚û°Ô∏è  Chose best action: 1\n",
            "   üìä Exploitation count: 8\n",
            "   üîÑ Explore/Exploit ratio: 6/8\n",
            "\n",
            "üåç Environment Response:\n",
            "   üé¨ Action taken: 1 (right)\n",
            "   üìç State transition: 1 ‚Üí 2\n",
            "   üí∞ Reward received: -1\n",
            "   üìä Episode reward so far: -11\n",
            "üìö Q-Learning Update #14:\n",
            "   üèÅ State: 1 ‚Üí Action: 1 ‚Üí Reward: -1 ‚Üí Next State: 2\n",
            "   ‚ö° Episode done: False\n",
            "   üéØ Target calculation (episode continues):\n",
            "      Next state Q-values: [-0.10981 -0.1    ]\n",
            "      Best next action: 1 (Q-value: -0.100)\n",
            "      Target = reward + gamma * max_next_Q\n",
            "      Target = -1 + 0.9 * -0.100 = -1.090\n",
            "   üîÑ Q-value Update:\n",
            "      Old Q-value: -0.190\n",
            "      TD Error: target - old = -1.090 - -0.190 = -0.900\n",
            "      Learning step: lr * TD_error = 0.1 * -0.900 = -0.090\n",
            "      New Q-value: old + learning_step = -0.190 + -0.090 = -0.280\n",
            "      üìà Change: -0.090\n",
            "\n",
            "üìç Step 8:\n",
            "--------------------\n",
            "üéÆ Action Selection for State 2:\n",
            "   üìã Current Q-values: [-0.10981 -0.1    ]\n",
            "   ‚≠ê Best action would be: 1 (Q-value: -0.100)\n",
            "   üé≤ EXPLORING! Random prob 0.032 < epsilon 0.3\n",
            "   ‚û°Ô∏è  Chose random action: 1\n",
            "   üìä Exploration count: 7\n",
            "   üîÑ Explore/Exploit ratio: 7/8\n",
            "\n",
            "üåç Environment Response:\n",
            "   üé¨ Action taken: 1 (right)\n",
            "   üìç State transition: 2 ‚Üí 3\n",
            "   üí∞ Reward received: -1\n",
            "   üìä Episode reward so far: -12\n",
            "üìö Q-Learning Update #15:\n",
            "   üèÅ State: 2 ‚Üí Action: 1 ‚Üí Reward: -1 ‚Üí Next State: 3\n",
            "   ‚ö° Episode done: False\n",
            "   üéØ Target calculation (episode continues):\n",
            "      Next state Q-values: [0. 1.]\n",
            "      Best next action: 1 (Q-value: 1.000)\n",
            "      Target = reward + gamma * max_next_Q\n",
            "      Target = -1 + 0.9 * 1.000 = -0.100\n",
            "   üîÑ Q-value Update:\n",
            "      Old Q-value: -0.100\n",
            "      TD Error: target - old = -0.100 - -0.100 = 0.000\n",
            "      Learning step: lr * TD_error = 0.1 * 0.000 = 0.000\n",
            "      New Q-value: old + learning_step = -0.100 + 0.000 = -0.100\n",
            "      üìà Change: +0.000\n",
            "‚è∞ Episode ended: Maximum steps (8) reached\n",
            "\n",
            "üìà Episode 2 Summary:\n",
            "   Total Reward: -12\n",
            "   Steps Taken: 8\n",
            "   Goal Reached: No\n",
            "\n",
            "üìä Q-Table after Episode 2:\n",
            "State\\Action     Action0     Action1\n",
            "------------------------------------\n",
            "State  0     -0.967      -0.369    \n",
            "State  1     -0.223      -0.280    \n",
            "State  2     -0.110      -0.100    \n",
            "State  3      0.000       1.000    \n",
            "State  4      0.000       0.000    \n",
            "\n",
            "üéØ Current Policy (Best Action per State):\n",
            "   State 0: Action 1 (Q-value: -0.369)\n",
            "   State 1: Action 0 (Q-value: -0.223)\n",
            "   State 2: Action 1 (Q-value: -0.100)\n",
            "   State 3: Action 1 (Q-value: 1.000)\n",
            "   State 4: Action 0 (Q-value: 0.000)\n",
            "\n",
            "================================================================================\n",
            "\n",
            "üé¨ EPISODE 3/3\n",
            "========================================\n",
            "üèÅ Starting at state 0\n",
            "\n",
            "üìç Step 1:\n",
            "--------------------\n",
            "üéÆ Action Selection for State 0:\n",
            "   üìã Current Q-values: [-0.9671 -0.3691]\n",
            "   ‚≠ê Best action would be: 1 (Q-value: -0.369)\n",
            "   üéØ EXPLOITING! Random prob 0.946 >= epsilon 0.3\n",
            "   ‚û°Ô∏è  Chose best action: 1\n",
            "   üìä Exploitation count: 9\n",
            "   üîÑ Explore/Exploit ratio: 7/9\n",
            "\n",
            "üåç Environment Response:\n",
            "   üé¨ Action taken: 1 (right)\n",
            "   üìç State transition: 0 ‚Üí 1\n",
            "   üí∞ Reward received: -1\n",
            "   üìä Episode reward so far: -1\n",
            "üìö Q-Learning Update #16:\n",
            "   üèÅ State: 0 ‚Üí Action: 1 ‚Üí Reward: -1 ‚Üí Next State: 1\n",
            "   ‚ö° Episode done: False\n",
            "   üéØ Target calculation (episode continues):\n",
            "      Next state Q-values: [-0.2233 -0.28  ]\n",
            "      Best next action: 0 (Q-value: -0.223)\n",
            "      Target = reward + gamma * max_next_Q\n",
            "      Target = -1 + 0.9 * -0.223 = -1.201\n",
            "   üîÑ Q-value Update:\n",
            "      Old Q-value: -0.369\n",
            "      TD Error: target - old = -1.201 - -0.369 = -0.832\n",
            "      Learning step: lr * TD_error = 0.1 * -0.832 = -0.083\n",
            "      New Q-value: old + learning_step = -0.369 + -0.083 = -0.452\n",
            "      üìà Change: -0.083\n",
            "\n",
            "üìç Step 2:\n",
            "--------------------\n",
            "üéÆ Action Selection for State 1:\n",
            "   üìã Current Q-values: [-0.2233 -0.28  ]\n",
            "   ‚≠ê Best action would be: 0 (Q-value: -0.223)\n",
            "   üé≤ EXPLORING! Random prob 0.257 < epsilon 0.3\n",
            "   ‚û°Ô∏è  Chose random action: 0\n",
            "   üìä Exploration count: 8\n",
            "   üîÑ Explore/Exploit ratio: 8/9\n",
            "\n",
            "üåç Environment Response:\n",
            "   üé¨ Action taken: 0 (left)\n",
            "   üìç State transition: 1 ‚Üí 0\n",
            "   üí∞ Reward received: -1\n",
            "   üìä Episode reward so far: -2\n",
            "üìö Q-Learning Update #17:\n",
            "   üèÅ State: 1 ‚Üí Action: 0 ‚Üí Reward: -1 ‚Üí Next State: 0\n",
            "   ‚ö° Episode done: False\n",
            "   üéØ Target calculation (episode continues):\n",
            "      Next state Q-values: [-0.9671   -0.452287]\n",
            "      Best next action: 1 (Q-value: -0.452)\n",
            "      Target = reward + gamma * max_next_Q\n",
            "      Target = -1 + 0.9 * -0.452 = -1.407\n",
            "   üîÑ Q-value Update:\n",
            "      Old Q-value: -0.223\n",
            "      TD Error: target - old = -1.407 - -0.223 = -1.184\n",
            "      Learning step: lr * TD_error = 0.1 * -1.184 = -0.118\n",
            "      New Q-value: old + learning_step = -0.223 + -0.118 = -0.342\n",
            "      üìà Change: -0.118\n",
            "\n",
            "üìç Step 3:\n",
            "--------------------\n",
            "üéÆ Action Selection for State 0:\n",
            "   üìã Current Q-values: [-0.9671   -0.452287]\n",
            "   ‚≠ê Best action would be: 1 (Q-value: -0.452)\n",
            "   üéØ EXPLOITING! Random prob 0.572 >= epsilon 0.3\n",
            "   ‚û°Ô∏è  Chose best action: 1\n",
            "   üìä Exploitation count: 10\n",
            "   üîÑ Explore/Exploit ratio: 8/10\n",
            "\n",
            "üåç Environment Response:\n",
            "   üé¨ Action taken: 1 (right)\n",
            "   üìç State transition: 0 ‚Üí 1\n",
            "   üí∞ Reward received: -1\n",
            "   üìä Episode reward so far: -3\n",
            "üìö Q-Learning Update #18:\n",
            "   üèÅ State: 0 ‚Üí Action: 1 ‚Üí Reward: -1 ‚Üí Next State: 1\n",
            "   ‚ö° Episode done: False\n",
            "   üéØ Target calculation (episode continues):\n",
            "      Next state Q-values: [-0.34167583 -0.28      ]\n",
            "      Best next action: 1 (Q-value: -0.280)\n",
            "      Target = reward + gamma * max_next_Q\n",
            "      Target = -1 + 0.9 * -0.280 = -1.252\n",
            "   üîÑ Q-value Update:\n",
            "      Old Q-value: -0.452\n",
            "      TD Error: target - old = -1.252 - -0.452 = -0.800\n",
            "      Learning step: lr * TD_error = 0.1 * -0.800 = -0.080\n",
            "      New Q-value: old + learning_step = -0.452 + -0.080 = -0.532\n",
            "      üìà Change: -0.080\n",
            "\n",
            "üìç Step 4:\n",
            "--------------------\n",
            "üéÆ Action Selection for State 1:\n",
            "   üìã Current Q-values: [-0.34167583 -0.28      ]\n",
            "   ‚≠ê Best action would be: 1 (Q-value: -0.280)\n",
            "   üéØ EXPLOITING! Random prob 0.971 >= epsilon 0.3\n",
            "   ‚û°Ô∏è  Chose best action: 1\n",
            "   üìä Exploitation count: 11\n",
            "   üîÑ Explore/Exploit ratio: 8/11\n",
            "\n",
            "üåç Environment Response:\n",
            "   üé¨ Action taken: 1 (right)\n",
            "   üìç State transition: 1 ‚Üí 2\n",
            "   üí∞ Reward received: -1\n",
            "   üìä Episode reward so far: -4\n",
            "üìö Q-Learning Update #19:\n",
            "   üèÅ State: 1 ‚Üí Action: 1 ‚Üí Reward: -1 ‚Üí Next State: 2\n",
            "   ‚ö° Episode done: False\n",
            "   üéØ Target calculation (episode continues):\n",
            "      Next state Q-values: [-0.10981 -0.1    ]\n",
            "      Best next action: 1 (Q-value: -0.100)\n",
            "      Target = reward + gamma * max_next_Q\n",
            "      Target = -1 + 0.9 * -0.100 = -1.090\n",
            "   üîÑ Q-value Update:\n",
            "      Old Q-value: -0.280\n",
            "      TD Error: target - old = -1.090 - -0.280 = -0.810\n",
            "      Learning step: lr * TD_error = 0.1 * -0.810 = -0.081\n",
            "      New Q-value: old + learning_step = -0.280 + -0.081 = -0.361\n",
            "      üìà Change: -0.081\n",
            "\n",
            "üìç Step 5:\n",
            "--------------------\n",
            "üéÆ Action Selection for State 2:\n",
            "   üìã Current Q-values: [-0.10981 -0.1    ]\n",
            "   ‚≠ê Best action would be: 1 (Q-value: -0.100)\n",
            "   üé≤ EXPLORING! Random prob 0.172 < epsilon 0.3\n",
            "   ‚û°Ô∏è  Chose random action: 1\n",
            "   üìä Exploration count: 9\n",
            "   üîÑ Explore/Exploit ratio: 9/11\n",
            "\n",
            "üåç Environment Response:\n",
            "   üé¨ Action taken: 1 (right)\n",
            "   üìç State transition: 2 ‚Üí 3\n",
            "   üí∞ Reward received: -1\n",
            "   üìä Episode reward so far: -5\n",
            "üìö Q-Learning Update #20:\n",
            "   üèÅ State: 2 ‚Üí Action: 1 ‚Üí Reward: -1 ‚Üí Next State: 3\n",
            "   ‚ö° Episode done: False\n",
            "   üéØ Target calculation (episode continues):\n",
            "      Next state Q-values: [0. 1.]\n",
            "      Best next action: 1 (Q-value: 1.000)\n",
            "      Target = reward + gamma * max_next_Q\n",
            "      Target = -1 + 0.9 * 1.000 = -0.100\n",
            "   üîÑ Q-value Update:\n",
            "      Old Q-value: -0.100\n",
            "      TD Error: target - old = -0.100 - -0.100 = 0.000\n",
            "      Learning step: lr * TD_error = 0.1 * 0.000 = 0.000\n",
            "      New Q-value: old + learning_step = -0.100 + 0.000 = -0.100\n",
            "      üìà Change: +0.000\n",
            "\n",
            "üìç Step 6:\n",
            "--------------------\n",
            "üéÆ Action Selection for State 3:\n",
            "   üìã Current Q-values: [0. 1.]\n",
            "   ‚≠ê Best action would be: 1 (Q-value: 1.000)\n",
            "   üé≤ EXPLORING! Random prob 0.033 < epsilon 0.3\n",
            "   ‚û°Ô∏è  Chose random action: 1\n",
            "   üìä Exploration count: 10\n",
            "   üîÑ Explore/Exploit ratio: 10/11\n",
            "\n",
            "üåç Environment Response:\n",
            "   üé¨ Action taken: 1 (right)\n",
            "   üìç State transition: 3 ‚Üí 4\n",
            "   üí∞ Reward received: 10\n",
            "   üìä Episode reward so far: 5\n",
            "üìö Q-Learning Update #21:\n",
            "   üèÅ State: 3 ‚Üí Action: 1 ‚Üí Reward: 10 ‚Üí Next State: 4\n",
            "   ‚ö° Episode done: True\n",
            "   üéØ Target calculation (episode ended):\n",
            "      Target = reward = 10\n",
            "   üîÑ Q-value Update:\n",
            "      Old Q-value: 1.000\n",
            "      TD Error: target - old = 10.000 - 1.000 = 9.000\n",
            "      Learning step: lr * TD_error = 0.1 * 9.000 = 0.900\n",
            "      New Q-value: old + learning_step = 1.000 + 0.900 = 1.900\n",
            "      üìà Change: +0.900\n",
            "‚úÖ Episode completed! Goal reached in 6 steps!\n",
            "\n",
            "üìà Episode 3 Summary:\n",
            "   Total Reward: 5\n",
            "   Steps Taken: 6\n",
            "   Goal Reached: Yes\n",
            "\n",
            "üìä Q-Table after Episode 3:\n",
            "State\\Action     Action0     Action1\n",
            "------------------------------------\n",
            "State  0     -0.967      -0.532    \n",
            "State  1     -0.342      -0.361    \n",
            "State  2     -0.110      -0.100    \n",
            "State  3      0.000       1.900    \n",
            "State  4      0.000       0.000    \n",
            "\n",
            "üéØ Current Policy (Best Action per State):\n",
            "   State 0: Action 1 (Q-value: -0.532)\n",
            "   State 1: Action 0 (Q-value: -0.342)\n",
            "   State 2: Action 1 (Q-value: -0.100)\n",
            "   State 3: Action 1 (Q-value: 1.900)\n",
            "   State 4: Action 0 (Q-value: 0.000)\n",
            "\n",
            "================================================================================\n",
            "\n",
            "üéâ Training Complete!\n",
            "üìä Total Episodes: 3\n",
            "üìà Rewards per Episode: [0, -12, 5]\n",
            "üéØ Average Reward: -2.33\n",
            "üîç Exploration vs Exploitation: 10 vs 11\n",
            "\n",
            "üìä Final Q-Table:\n",
            "State\\Action     Action0     Action1\n",
            "------------------------------------\n",
            "State  0     -0.967      -0.532    \n",
            "State  1     -0.342      -0.361    \n",
            "State  2     -0.110      -0.100    \n",
            "State  3      0.000       1.900    \n",
            "State  4      0.000       0.000    \n",
            "\n",
            "üéØ Current Policy (Best Action per State):\n",
            "   State 0: Action 1 (Q-value: -0.532)\n",
            "   State 1: Action 0 (Q-value: -0.342)\n",
            "   State 2: Action 1 (Q-value: -0.100)\n",
            "   State 3: Action 1 (Q-value: 1.900)\n",
            "   State 4: Action 0 (Q-value: 0.000)\n",
            "\n",
            "\n",
            "================================================================================\n",
            "üß™ Testing the trained agent (no more learning, just exploitation):\n",
            "================================================================================\n",
            "üèÅ Starting test at state 0\n",
            "üéÆ Action Selection for State 0:\n",
            "   üìã Current Q-values: [-0.9671    -0.5322583]\n",
            "   ‚≠ê Best action would be: 1 (Q-value: -0.532)\n",
            "   üéØ EXPLOITING! Random prob 0.967 >= epsilon 0.0\n",
            "   ‚û°Ô∏è  Chose best action: 1\n",
            "   üìä Exploitation count: 12\n",
            "   üîÑ Explore/Exploit ratio: 10/12\n",
            "\n",
            "   üé¨ Took action 1 ‚Üí moved to state 1, got reward -1\n",
            "üéÆ Action Selection for State 1:\n",
            "   üìã Current Q-values: [-0.34167583 -0.361     ]\n",
            "   ‚≠ê Best action would be: 0 (Q-value: -0.342)\n",
            "   üéØ EXPLOITING! Random prob 0.591 >= epsilon 0.0\n",
            "   ‚û°Ô∏è  Chose best action: 0\n",
            "   üìä Exploitation count: 13\n",
            "   üîÑ Explore/Exploit ratio: 10/13\n",
            "\n",
            "   üé¨ Took action 0 ‚Üí moved to state 0, got reward -1\n",
            "üéÆ Action Selection for State 0:\n",
            "   üìã Current Q-values: [-0.9671    -0.5322583]\n",
            "   ‚≠ê Best action would be: 1 (Q-value: -0.532)\n",
            "   üéØ EXPLOITING! Random prob 0.870 >= epsilon 0.0\n",
            "   ‚û°Ô∏è  Chose best action: 1\n",
            "   üìä Exploitation count: 14\n",
            "   üîÑ Explore/Exploit ratio: 10/14\n",
            "\n",
            "   üé¨ Took action 1 ‚Üí moved to state 1, got reward -1\n",
            "üéÆ Action Selection for State 1:\n",
            "   üìã Current Q-values: [-0.34167583 -0.361     ]\n",
            "   ‚≠ê Best action would be: 0 (Q-value: -0.342)\n",
            "   üéØ EXPLOITING! Random prob 0.223 >= epsilon 0.0\n",
            "   ‚û°Ô∏è  Chose best action: 0\n",
            "   üìä Exploitation count: 15\n",
            "   üîÑ Explore/Exploit ratio: 10/15\n",
            "\n",
            "   üé¨ Took action 0 ‚Üí moved to state 0, got reward -1\n",
            "üéÆ Action Selection for State 0:\n",
            "   üìã Current Q-values: [-0.9671    -0.5322583]\n",
            "   ‚≠ê Best action would be: 1 (Q-value: -0.532)\n",
            "   üéØ EXPLOITING! Random prob 0.040 >= epsilon 0.0\n",
            "   ‚û°Ô∏è  Chose best action: 1\n",
            "   üìä Exploitation count: 16\n",
            "   üîÑ Explore/Exploit ratio: 10/16\n",
            "\n",
            "   üé¨ Took action 1 ‚Üí moved to state 1, got reward -1\n",
            "üéÆ Action Selection for State 1:\n",
            "   üìã Current Q-values: [-0.34167583 -0.361     ]\n",
            "   ‚≠ê Best action would be: 0 (Q-value: -0.342)\n",
            "   üéØ EXPLOITING! Random prob 0.263 >= epsilon 0.0\n",
            "   ‚û°Ô∏è  Chose best action: 0\n",
            "   üìä Exploitation count: 17\n",
            "   üîÑ Explore/Exploit ratio: 10/17\n",
            "\n",
            "   üé¨ Took action 0 ‚Üí moved to state 0, got reward -1\n",
            "üéÆ Action Selection for State 0:\n",
            "   üìã Current Q-values: [-0.9671    -0.5322583]\n",
            "   ‚≠ê Best action would be: 1 (Q-value: -0.532)\n",
            "   üéØ EXPLOITING! Random prob 0.267 >= epsilon 0.0\n",
            "   ‚û°Ô∏è  Chose best action: 1\n",
            "   üìä Exploitation count: 18\n",
            "   üîÑ Explore/Exploit ratio: 10/18\n",
            "\n",
            "   üé¨ Took action 1 ‚Üí moved to state 1, got reward -1\n",
            "üéÆ Action Selection for State 1:\n",
            "   üìã Current Q-values: [-0.34167583 -0.361     ]\n",
            "   ‚≠ê Best action would be: 0 (Q-value: -0.342)\n",
            "   üéØ EXPLOITING! Random prob 0.326 >= epsilon 0.0\n",
            "   ‚û°Ô∏è  Chose best action: 0\n",
            "   üìä Exploitation count: 19\n",
            "   üîÑ Explore/Exploit ratio: 10/19\n",
            "\n",
            "   üé¨ Took action 0 ‚Üí moved to state 0, got reward -1\n",
            "üéÆ Action Selection for State 0:\n",
            "   üìã Current Q-values: [-0.9671    -0.5322583]\n",
            "   ‚≠ê Best action would be: 1 (Q-value: -0.532)\n",
            "   üéØ EXPLOITING! Random prob 0.854 >= epsilon 0.0\n",
            "   ‚û°Ô∏è  Chose best action: 1\n",
            "   üìä Exploitation count: 20\n",
            "   üîÑ Explore/Exploit ratio: 10/20\n",
            "\n",
            "   üé¨ Took action 1 ‚Üí moved to state 1, got reward -1\n",
            "üéÆ Action Selection for State 1:\n",
            "   üìã Current Q-values: [-0.34167583 -0.361     ]\n",
            "   ‚≠ê Best action would be: 0 (Q-value: -0.342)\n",
            "   üéØ EXPLOITING! Random prob 0.884 >= epsilon 0.0\n",
            "   ‚û°Ô∏è  Chose best action: 0\n",
            "   üìä Exploitation count: 21\n",
            "   üîÑ Explore/Exploit ratio: 10/21\n",
            "\n",
            "   üé¨ Took action 0 ‚Üí moved to state 0, got reward -1\n",
            "‚ùå Failed to reach goal in 10 steps\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# QLearningAgent(Table Verbose)"
      ],
      "metadata": {
        "id": "h1jCGyUE74pW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from datetime import datetime\n",
        "\n",
        "class QLearningAgent:\n",
        "    def __init__(self, num_states, num_actions, lr=0.1, gamma=0.95, epsilon=0.1):\n",
        "        # Initialize Q-table with zeros\n",
        "        self.q_table = np.zeros((num_states, num_actions))\n",
        "        self.lr = lr\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.num_states = num_states\n",
        "        self.num_actions = num_actions\n",
        "\n",
        "        # Logging counters\n",
        "        self.exploration_count = 0\n",
        "        self.exploitation_count = 0\n",
        "        self.update_count = 0\n",
        "\n",
        "        self._print_initialization()\n",
        "\n",
        "    def _print_initialization(self):\n",
        "        \"\"\"Print agent initialization in organized format\"\"\"\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"ü§ñ Q-LEARNING AGENT INITIALIZATION\")\n",
        "        print(\"=\"*60)\n",
        "        print(f\"‚îÇ Q-table Shape       ‚îÇ {self.q_table.shape}\")\n",
        "        print(f\"‚îÇ Learning Rate (Œ±)   ‚îÇ {self.lr}\")\n",
        "        print(f\"‚îÇ Discount Factor (Œ≥) ‚îÇ {self.gamma}\")\n",
        "        print(f\"‚îÇ Exploration Rate (Œµ)‚îÇ {self.epsilon}\")\n",
        "        print(\"‚îÄ\"*60)\n",
        "        print(\"Initial Q-Table (all zeros):\")\n",
        "        self._print_q_table_compact()\n",
        "        print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "    def get_action(self, state, verbose=True):\n",
        "        \"\"\"Get action using epsilon-greedy strategy\"\"\"\n",
        "        current_q_values = self.q_table[state]\n",
        "        best_action = np.argmax(current_q_values)\n",
        "        best_q_value = current_q_values[best_action]\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"‚îå‚îÄ ACTION SELECTION (State {state}) \" + \"‚îÄ\"*25)\n",
        "            print(f\"‚îÇ Q-values: {self._format_array(current_q_values)}\")\n",
        "            print(f\"‚îÇ Best:     Action {best_action} (Q={best_q_value:.3f})\")\n",
        "\n",
        "        # Epsilon-greedy decision\n",
        "        random_prob = random.random()\n",
        "        if random_prob < self.epsilon:\n",
        "            action = random.randint(0, self.num_actions - 1)\n",
        "            self.exploration_count += 1\n",
        "            decision_type = \"EXPLORE üé≤\"\n",
        "            if verbose:\n",
        "                print(f\"‚îÇ Decision: {decision_type} ({random_prob:.3f} < {self.epsilon})\")\n",
        "                print(f\"‚îÇ Chosen:   Action {action} (random)\")\n",
        "        # Exploit decision\n",
        "        else:\n",
        "            action = best_action\n",
        "            self.exploitation_count += 1\n",
        "            decision_type = \"EXPLOIT üéØ\"\n",
        "            if verbose:\n",
        "                print(f\"‚îÇ Decision: {decision_type} ({random_prob:.3f} ‚â• {self.epsilon})\")\n",
        "                print(f\"‚îÇ Chosen:   Action {action} (greedy)\")\n",
        "\n",
        "        if verbose:\n",
        "            total_decisions = self.exploration_count + self.exploitation_count\n",
        "            explore_pct = (self.exploration_count / total_decisions * 100) if total_decisions > 0 else 0\n",
        "            print(f\"‚îÇ Stats:    Explore {self.exploration_count}/{total_decisions} ({explore_pct:.1f}%)\")\n",
        "            print(\"‚îî\" + \"‚îÄ\"*50)\n",
        "\n",
        "        return action\n",
        "\n",
        "    def update(self, state, action, reward, next_state, done, verbose=True):\n",
        "        \"\"\"Update Q-table with organized logging\"\"\"\n",
        "        self.update_count += 1\n",
        "        old_q_value = self.q_table[state, action]\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"‚îå‚îÄ Q-UPDATE #{self.update_count} \" + \"‚îÄ\"*35)\n",
        "            print(f\"‚îÇ Transition: S{state} --A{action}--> S{next_state} (R={reward:+.1f})\")\n",
        "            print(f\"‚îÇ Done:       {done}\")\n",
        "\n",
        "        # Calculate target\n",
        "        if done:\n",
        "            target = reward\n",
        "            if verbose:\n",
        "                print(f\"‚îÇ Target:     {target:.3f} (episode ended, no future)\")\n",
        "        else:\n",
        "            next_q_values = self.q_table[next_state]\n",
        "            max_next_q = np.max(next_q_values)\n",
        "            target = reward + self.gamma * max_next_q\n",
        "            if verbose:\n",
        "                print(f\"‚îÇ Next Q's:   {self._format_array(next_q_values)}\")\n",
        "                print(f\"‚îÇ Target:     {reward} + {self.gamma}√ó{max_next_q:.3f} = {target:.3f}\")\n",
        "\n",
        "        # Update Q-value\n",
        "        td_error = target - old_q_value\n",
        "        new_q_value = old_q_value + self.lr * td_error\n",
        "        self.q_table[state, action] = new_q_value\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"‚îÇ Q-Update:   {old_q_value:.3f} + {self.lr}√ó{td_error:.3f} = {new_q_value:.3f}\")\n",
        "            print(f\"‚îÇ Change:     {new_q_value - old_q_value:+.3f}\")\n",
        "            print(\"‚îî\" + \"‚îÄ\"*50)\n",
        "\n",
        "        return td_error\n",
        "\n",
        "    def _format_array(self, arr, decimals=2):\n",
        "        \"\"\"Format numpy array for clean display\"\"\"\n",
        "        return \"[\" + \", \".join([f\"{x:.{decimals}f}\" for x in arr]) + \"]\"\n",
        "\n",
        "    def _print_q_table_compact(self):\n",
        "        \"\"\"Print Q-table in compact format\"\"\"\n",
        "        print(\"     \", end=\"\")\n",
        "        for a in range(self.num_actions):\n",
        "            print(f\"   A{a}  \", end=\"\")\n",
        "        print()\n",
        "\n",
        "        for s in range(self.num_states):\n",
        "            print(f\"S{s} ‚îÇ \", end=\"\")\n",
        "            for a in range(self.num_actions):\n",
        "                print(f\"{self.q_table[s, a]:5.2f}\", end=\" \")\n",
        "            print()\n",
        "\n",
        "    def print_q_table(self, title=\"Q-TABLE\"):\n",
        "        \"\"\"Print Q-table with header\"\"\"\n",
        "        print(f\"\\n‚îå‚îÄ {title} \" + \"‚îÄ\"*(50-len(title)))\n",
        "        self._print_q_table_compact()\n",
        "        print(\"‚îî\" + \"‚îÄ\"*50)\n",
        "\n",
        "    def print_policy(self):\n",
        "        \"\"\"Print current policy\"\"\"\n",
        "        print(\"\\n‚îå‚îÄ CURRENT POLICY \" + \"‚îÄ\"*33)\n",
        "        for s in range(self.num_states):\n",
        "            best_action = np.argmax(self.q_table[s])\n",
        "            best_q_value = self.q_table[s, best_action]\n",
        "            print(f\"‚îÇ State {s}: Action {best_action} (Q={best_q_value:.3f})\")\n",
        "        print(\"‚îî\" + \"‚îÄ\"*50)\n",
        "\n",
        "\n",
        "class TrainingLogger:\n",
        "    \"\"\"Separate class to handle training-level logging\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.episode_data = []\n",
        "        self.start_time = datetime.now()\n",
        "\n",
        "    def log_episode_start(self, episode, total_episodes):\n",
        "        \"\"\"Log episode start\"\"\"\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"üé¨ EPISODE {episode + 1:2d}/{total_episodes} - {datetime.now().strftime('%H:%M:%S')}\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "    def log_step_start(self, step, state):\n",
        "        \"\"\"Log step start\"\"\"\n",
        "        print(f\"\\nüìç STEP {step:2d} - Current State: {state}\")\n",
        "        print(\"‚îÄ\"*30)\n",
        "\n",
        "    def log_environment_response(self, action, state, next_state, reward, done):\n",
        "        \"\"\"Log environment response\"\"\"\n",
        "        action_name = \"LEFT\" if action == 0 else \"RIGHT\"\n",
        "        print(f\"‚îå‚îÄ ENVIRONMENT RESPONSE \" + \"‚îÄ\"*27)\n",
        "        print(f\"‚îÇ Action:     {action} ({action_name})\")\n",
        "        print(f\"‚îÇ Transition: {state} ‚Üí {next_state}\")\n",
        "        print(f\"‚îÇ Reward:     {reward:+.1f}\")\n",
        "        print(f\"‚îÇ Done:       {done}\")\n",
        "        print(\"‚îî\" + \"‚îÄ\"*50)\n",
        "\n",
        "    def log_episode_summary(self, episode, steps, reward, done, goal_state=4):\n",
        "        \"\"\"Log episode summary\"\"\"\n",
        "        status = \"SUCCESS ‚úÖ\" if done and steps > 0 else \"TIMEOUT ‚è∞\"\n",
        "        self.episode_data.append({\n",
        "            'episode': episode + 1,\n",
        "            'steps': steps,\n",
        "            'reward': reward,\n",
        "            'success': done\n",
        "        })\n",
        "\n",
        "        print(f\"\\n‚îå‚îÄ EPISODE {episode + 1} SUMMARY \" + \"‚îÄ\"*28)\n",
        "        print(f\"‚îÇ Status:     {status}\")\n",
        "        print(f\"‚îÇ Steps:      {steps}\")\n",
        "        print(f\"‚îÇ Reward:     {reward:+.1f}\")\n",
        "        print(f\"‚îÇ Goal:       {'Reached' if done else 'Not reached'}\")\n",
        "        print(\"‚îî\" + \"‚îÄ\"*50)\n",
        "\n",
        "    def print_training_summary(self, agent):\n",
        "        \"\"\"Print final training summary\"\"\"\n",
        "        total_episodes = len(self.episode_data)\n",
        "        successful_episodes = sum(1 for ep in self.episode_data if ep['success'])\n",
        "        avg_reward = np.mean([ep['reward'] for ep in self.episode_data])\n",
        "        avg_steps = np.mean([ep['steps'] for ep in self.episode_data])\n",
        "\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(\"üéâ TRAINING COMPLETE\")\n",
        "        print(f\"{'='*60}\")\n",
        "        print(f\"‚îÇ Episodes:      {total_episodes}\")\n",
        "        print(f\"‚îÇ Success Rate:  {successful_episodes}/{total_episodes} ({successful_episodes/total_episodes*100:.1f}%)\")\n",
        "        print(f\"‚îÇ Avg Reward:    {avg_reward:+.2f}\")\n",
        "        print(f\"‚îÇ Avg Steps:     {avg_steps:.1f}\")\n",
        "        print(f\"‚îÇ Exploration:   {agent.exploration_count}/{agent.exploration_count + agent.exploitation_count} ({agent.exploration_count/(agent.exploration_count + agent.exploitation_count)*100:.1f}%)\")\n",
        "        print(f\"‚îÇ Duration:      {(datetime.now() - self.start_time).total_seconds():.1f}s\")\n",
        "        print(\"‚îÄ\"*60)\n",
        "\n",
        "        # Episode-by-episode breakdown\n",
        "        print(\"EPISODE BREAKDOWN:\")\n",
        "        print(\"Ep# ‚îÇ Steps ‚îÇ Reward ‚îÇ Status\")\n",
        "        print(\"‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\")\n",
        "        for ep in self.episode_data:\n",
        "            status = \"‚úÖ\" if ep['success'] else \"‚è∞\"\n",
        "            print(f\"{ep['episode']:2d}  ‚îÇ  {ep['steps']:2d}   ‚îÇ {ep['reward']:+6.1f} ‚îÇ   {status}\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "\n",
        "def simple_environment_step(state, action, num_states=5):\n",
        "    \"\"\"Simple environment: move left/right, goal is rightmost state\"\"\"\n",
        "    if action == 0:  # Move left\n",
        "        next_state = max(0, state - 1)\n",
        "    else:  # Move right\n",
        "        next_state = min(num_states - 1, state + 1)\n",
        "\n",
        "    # Rewards\n",
        "    if next_state == num_states - 1:  # Goal reached\n",
        "        reward = 10\n",
        "        done = True\n",
        "    elif (action == 0 and state == 0) or (action == 1 and state == num_states - 1):\n",
        "        reward = -5  # Hit boundary\n",
        "        done = False\n",
        "    else:\n",
        "        reward = -1  # Step cost\n",
        "        done = False\n",
        "\n",
        "    return next_state, reward, done\n",
        "\n",
        "\n",
        "def train_agent(episodes=5, max_steps_per_episode=10, verbose=True):\n",
        "    \"\"\"Train agent with organized logging\"\"\"\n",
        "\n",
        "    # Initialize\n",
        "    agent = QLearningAgent(num_states=5, num_actions=2, lr=0.1, gamma=0.9, epsilon=0.3)\n",
        "    logger = TrainingLogger()\n",
        "\n",
        "    # Training loop\n",
        "    for episode in range(episodes):\n",
        "        logger.log_episode_start(episode, episodes)\n",
        "\n",
        "        state = 0  # Start state\n",
        "        episode_reward = 0\n",
        "        step_count = 0\n",
        "\n",
        "        for step in range(max_steps_per_episode):\n",
        "            step_count += 1\n",
        "\n",
        "            if verbose:\n",
        "                logger.log_step_start(step_count, state)\n",
        "\n",
        "            # Get action\n",
        "            action = agent.get_action(state, verbose=verbose)\n",
        "\n",
        "            # Environment step\n",
        "            next_state, reward, done = simple_environment_step(state, action)\n",
        "            episode_reward += reward\n",
        "\n",
        "            if verbose:\n",
        "                logger.log_environment_response(action, state, next_state, reward, done)\n",
        "\n",
        "            # Update agent\n",
        "            agent.update(state, action, reward, next_state, done, verbose=verbose)\n",
        "\n",
        "            state = next_state\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        # Episode summary\n",
        "        logger.log_episode_summary(episode, step_count, episode_reward, done)\n",
        "\n",
        "        # Show Q-table and policy after each episode\n",
        "        agent.print_q_table(f\"Q-TABLE AFTER EPISODE {episode + 1}\")\n",
        "        agent.print_policy()\n",
        "\n",
        "    # Final summary\n",
        "    logger.print_training_summary(agent)\n",
        "    agent.print_q_table(\"FINAL Q-TABLE\")\n",
        "    agent.print_policy()\n",
        "\n",
        "    return agent\n",
        "\n",
        "\n",
        "def test_trained_agent(agent, max_steps=10):\n",
        "    \"\"\"Test the trained agent\"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"üß™ TESTING TRAINED AGENT (Pure Exploitation)\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    agent.epsilon = 0.0  # No exploration\n",
        "    state = 0\n",
        "    step = 0\n",
        "\n",
        "    print(f\"Starting at state {state}\\n\")\n",
        "\n",
        "    while step < max_steps:\n",
        "        step += 1\n",
        "        print(f\"Step {step}:\")\n",
        "\n",
        "        action = agent.get_action(state, verbose=False)\n",
        "        next_state, reward, done = simple_environment_step(state, action)\n",
        "\n",
        "        action_name = \"LEFT\" if action == 0 else \"RIGHT\"\n",
        "        print(f\"  Action: {action} ({action_name}) ‚Üí State {state} ‚Üí {next_state} (Reward: {reward:+.1f})\")\n",
        "\n",
        "        state = next_state\n",
        "\n",
        "        if done:\n",
        "            print(f\"‚úÖ SUCCESS! Goal reached in {step} steps!\")\n",
        "            break\n",
        "\n",
        "    if not done:\n",
        "        print(f\"‚ùå FAILED! Could not reach goal in {max_steps} steps\")\n",
        "\n",
        "    print(\"=\"*60)\n",
        "\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"üöÄ Q-LEARNING TRAINING DEMONSTRATION\")\n",
        "    print(\"Environment: 5 states (0,1,2,3,4), 2 actions (left/right), goal=state 4\")\n",
        "\n",
        "    # Train with organized output\n",
        "    trained_agent = train_agent(episodes=4, max_steps_per_episode=8, verbose=True)\n",
        "\n",
        "    # Test the trained agent\n",
        "    test_trained_agent(trained_agent, max_steps=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cw_cnRQ54ZDx",
        "outputId": "46d0f9e2-e4ba-4989-a62b-ec560c040b10"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Q-LEARNING TRAINING DEMONSTRATION\n",
            "Environment: 5 states (0,1,2,3,4), 2 actions (left/right), goal=state 4\n",
            "\n",
            "============================================================\n",
            "ü§ñ Q-LEARNING AGENT INITIALIZATION\n",
            "============================================================\n",
            "‚îÇ Q-table Shape       ‚îÇ (5, 2)\n",
            "‚îÇ Learning Rate (Œ±)   ‚îÇ 0.1\n",
            "‚îÇ Discount Factor (Œ≥) ‚îÇ 0.9\n",
            "‚îÇ Exploration Rate (Œµ)‚îÇ 0.3\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "Initial Q-Table (all zeros):\n",
            "        A0     A1  \n",
            "S0 ‚îÇ  0.00  0.00 \n",
            "S1 ‚îÇ  0.00  0.00 \n",
            "S2 ‚îÇ  0.00  0.00 \n",
            "S3 ‚îÇ  0.00  0.00 \n",
            "S4 ‚îÇ  0.00  0.00 \n",
            "============================================================\n",
            "\n",
            "\n",
            "============================================================\n",
            "üé¨ EPISODE  1/4 - 03:11:44\n",
            "============================================================\n",
            "\n",
            "üìç STEP  1 - Current State: 0\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îå‚îÄ ACTION SELECTION (State 0) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îÇ Q-values: [0.00, 0.00]\n",
            "‚îÇ Best:     Action 0 (Q=0.000)\n",
            "‚îÇ Decision: EXPLOIT üéØ (0.480 ‚â• 0.3)\n",
            "‚îÇ Chosen:   Action 0 (greedy)\n",
            "‚îÇ Stats:    Explore 0/1 (0.0%)\n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îå‚îÄ ENVIRONMENT RESPONSE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îÇ Action:     0 (LEFT)\n",
            "‚îÇ Transition: 0 ‚Üí 0\n",
            "‚îÇ Reward:     -5.0\n",
            "‚îÇ Done:       False\n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îå‚îÄ Q-UPDATE #1 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îÇ Transition: S0 --A0--> S0 (R=-5.0)\n",
            "‚îÇ Done:       False\n",
            "‚îÇ Next Q's:   [0.00, 0.00]\n",
            "‚îÇ Target:     -5 + 0.9√ó0.000 = -5.000\n",
            "‚îÇ Q-Update:   0.000 + 0.1√ó-5.000 = -0.500\n",
            "‚îÇ Change:     -0.500\n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "\n",
            "üìç STEP  2 - Current State: 0\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îå‚îÄ ACTION SELECTION (State 0) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îÇ Q-values: [-0.50, 0.00]\n",
            "‚îÇ Best:     Action 1 (Q=0.000)\n",
            "‚îÇ Decision: EXPLOIT üéØ (0.533 ‚â• 0.3)\n",
            "‚îÇ Chosen:   Action 1 (greedy)\n",
            "‚îÇ Stats:    Explore 0/2 (0.0%)\n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îå‚îÄ ENVIRONMENT RESPONSE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îÇ Action:     1 (RIGHT)\n",
            "‚îÇ Transition: 0 ‚Üí 1\n",
            "‚îÇ Reward:     -1.0\n",
            "‚îÇ Done:       False\n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îå‚îÄ Q-UPDATE #2 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îÇ Transition: S0 --A1--> S1 (R=-1.0)\n",
            "‚îÇ Done:       False\n",
            "‚îÇ Next Q's:   [0.00, 0.00]\n",
            "‚îÇ Target:     -1 + 0.9√ó0.000 = -1.000\n",
            "‚îÇ Q-Update:   0.000 + 0.1√ó-1.000 = -0.100\n",
            "‚îÇ Change:     -0.100\n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "\n",
            "üìç STEP  3 - Current State: 1\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îå‚îÄ ACTION SELECTION (State 1) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îÇ Q-values: [0.00, 0.00]\n",
            "‚îÇ Best:     Action 0 (Q=0.000)\n",
            "‚îÇ Decision: EXPLOIT üéØ (0.420 ‚â• 0.3)\n",
            "‚îÇ Chosen:   Action 0 (greedy)\n",
            "‚îÇ Stats:    Explore 0/3 (0.0%)\n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îå‚îÄ ENVIRONMENT RESPONSE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îÇ Action:     0 (LEFT)\n",
            "‚îÇ Transition: 1 ‚Üí 0\n",
            "‚îÇ Reward:     -1.0\n",
            "‚îÇ Done:       False\n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îå‚îÄ Q-UPDATE #3 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îÇ Transition: S1 --A0--> S0 (R=-1.0)\n",
            "‚îÇ Done:       False\n",
            "‚îÇ Next Q's:   [-0.50, -0.10]\n",
            "‚îÇ Target:     -1 + 0.9√ó-0.100 = -1.090\n",
            "‚îÇ Q-Update:   0.000 + 0.1√ó-1.090 = -0.109\n",
            "‚îÇ Change:     -0.109\n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "\n",
            "üìç STEP  4 - Current State: 0\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îå‚îÄ ACTION SELECTION (State 0) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îÇ Q-values: [-0.50, -0.10]\n",
            "‚îÇ Best:     Action 1 (Q=-0.100)\n",
            "‚îÇ Decision: EXPLOIT üéØ (0.876 ‚â• 0.3)\n",
            "‚îÇ Chosen:   Action 1 (greedy)\n",
            "‚îÇ Stats:    Explore 0/4 (0.0%)\n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îå‚îÄ ENVIRONMENT RESPONSE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îÇ Action:     1 (RIGHT)\n",
            "‚îÇ Transition: 0 ‚Üí 1\n",
            "‚îÇ Reward:     -1.0\n",
            "‚îÇ Done:       False\n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îå‚îÄ Q-UPDATE #4 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îÇ Transition: S0 --A1--> S1 (R=-1.0)\n",
            "‚îÇ Done:       False\n",
            "‚îÇ Next Q's:   [-0.11, 0.00]\n",
            "‚îÇ Target:     -1 + 0.9√ó0.000 = -1.000\n",
            "‚îÇ Q-Update:   -0.100 + 0.1√ó-0.900 = -0.190\n",
            "‚îÇ Change:     -0.090\n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "\n",
            "üìç STEP  5 - Current State: 1\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îå‚îÄ ACTION SELECTION (State 1) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îÇ Q-values: [-0.11, 0.00]\n",
            "‚îÇ Best:     Action 1 (Q=0.000)\n",
            "‚îÇ Decision: EXPLOIT üéØ (0.981 ‚â• 0.3)\n",
            "‚îÇ Chosen:   Action 1 (greedy)\n",
            "‚îÇ Stats:    Explore 0/5 (0.0%)\n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îå‚îÄ ENVIRONMENT RESPONSE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îÇ Action:     1 (RIGHT)\n",
            "‚îÇ Transition: 1 ‚Üí 2\n",
            "‚îÇ Reward:     -1.0\n",
            "‚îÇ Done:       False\n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îå‚îÄ Q-UPDATE #5 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îÇ Transition: S1 --A1--> S2 (R=-1.0)\n",
            "‚îÇ Done:       False\n",
            "‚îÇ Next Q's:   [0.00, 0.00]\n",
            "‚îÇ Target:     -1 + 0.9√ó0.000 = -1.000\n",
            "‚îÇ Q-Update:   0.000 + 0.1√ó-1.000 = -0.100\n",
            "‚îÇ Change:     -0.100\n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "\n",
            "üìç STEP  6 - Current State: 2\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îå‚îÄ ACTION SELECTION (State 2) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îÇ Q-values: [0.00, 0.00]\n",
            "‚îÇ Best:     Action 0 (Q=0.000)\n",
            "‚îÇ Decision: EXPLOIT üéØ (0.680 ‚â• 0.3)\n",
            "‚îÇ Chosen:   Action 0 (greedy)\n",
            "‚îÇ Stats:    Explore 0/6 (0.0%)\n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îå‚îÄ ENVIRONMENT RESPONSE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îÇ Action:     0 (LEFT)\n",
            "‚îÇ Transition: 2 ‚Üí 1\n",
            "‚îÇ Reward:     -1.0\n",
            "‚îÇ Done:       False\n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îå‚îÄ Q-UPDATE #6 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îÇ Transition: S2 --A0--> S1 (R=-1.0)\n",
            "‚îÇ Done:       False\n",
            "‚îÇ Next Q's:   [-0.11, -0.10]\n",
            "‚îÇ Target:     -1 + 0.9√ó-0.100 = -1.090\n",
            "‚îÇ Q-Update:   0.000 + 0.1√ó-1.090 = -0.109\n",
            "‚îÇ Change:     -0.109\n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "\n",
            "üìç STEP  7 - Current State: 1\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îå‚îÄ ACTION SELECTION (State 1) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îÇ Q-values: [-0.11, -0.10]\n",
            "‚îÇ Best:     Action 1 (Q=-0.100)\n",
            "‚îÇ Decision: EXPLOIT üéØ (0.674 ‚â• 0.3)\n",
            "‚îÇ Chosen:   Action 1 (greedy)\n",
            "‚îÇ Stats:    Explore 0/7 (0.0%)\n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îå‚îÄ ENVIRONMENT RESPONSE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îÇ Action:     1 (RIGHT)\n",
            "‚îÇ Transition: 1 ‚Üí 2\n",
            "‚îÇ Reward:     -1.0\n",
            "‚îÇ Done:       False\n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îå‚îÄ Q-UPDATE #7 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îÇ Transition: S1 --A1--> S2 (R=-1.0)\n",
            "‚îÇ Done:       False\n",
            "‚îÇ Next Q's:   [-0.11, 0.00]\n",
            "‚îÇ Target:     -1 + 0.9√ó0.000 = -1.000\n",
            "‚îÇ Q-Update:   -0.100 + 0.1√ó-0.900 = -0.190\n",
            "‚îÇ Change:     -0.090\n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "\n",
            "üìç STEP  8 - Current State: 2\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îå‚îÄ ACTION SELECTION (State 2) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îÇ Q-values: [-0.11, 0.00]\n",
            "‚îÇ Best:     Action 1 (Q=0.000)\n",
            "‚îÇ Decision: EXPLORE üé≤ (0.070 < 0.3)\n",
            "‚îÇ Chosen:   Action 0 (random)\n",
            "‚îÇ Stats:    Explore 1/8 (12.5%)\n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îå‚îÄ ENVIRONMENT RESPONSE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îÇ Action:     0 (LEFT)\n",
            "‚îÇ Transition: 2 ‚Üí 1\n",
            "‚îÇ Reward:     -1.0\n",
            "‚îÇ Done:       False\n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îå‚îÄ Q-UPDATE #8 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îÇ Transition: S2 --A0--> S1 (R=-1.0)\n",
            "‚îÇ Done:       False\n",
            "‚îÇ Next Q's:   [-0.11, -0.19]\n",
            "‚îÇ Target:     -1 + 0.9√ó-0.109 = -1.098\n",
            "‚îÇ Q-Update:   -0.109 + 0.1√ó-0.989 = -0.208\n",
            "‚îÇ Change:     -0.099\n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "\n",
            "‚îå‚îÄ EPISODE 1 SUMMARY ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îÇ Status:     TIMEOUT ‚è∞\n",
            "‚îÇ Steps:      8\n",
            "‚îÇ Reward:     -12.0\n",
            "‚îÇ Goal:       Not reached\n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "\n",
            "‚îå‚îÄ Q-TABLE AFTER EPISODE 1 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "        A0     A1  \n",
            "S0 ‚îÇ -0.50 -0.19 \n",
            "S1 ‚îÇ -0.11 -0.19 \n",
            "S2 ‚îÇ -0.21  0.00 \n",
            "S3 ‚îÇ  0.00  0.00 \n",
            "S4 ‚îÇ  0.00  0.00 \n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "\n",
            "‚îå‚îÄ CURRENT POLICY ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îÇ State 0: Action 1 (Q=-0.190)\n",
            "‚îÇ State 1: Action 0 (Q=-0.109)\n",
            "‚îÇ State 2: Action 1 (Q=0.000)\n",
            "‚îÇ State 3: Action 0 (Q=0.000)\n",
            "‚îÇ State 4: Action 0 (Q=0.000)\n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "\n",
            "============================================================\n",
            "üé¨ EPISODE  2/4 - 03:11:44\n",
            "============================================================\n",
            "\n",
            "üìç STEP  1 - Current State: 0\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îå‚îÄ ACTION SELECTION (State 0) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îÇ Q-values: [-0.50, -0.19]\n",
            "‚îÇ Best:     Action 1 (Q=-0.190)\n",
            "‚îÇ Decision: EXPLORE üé≤ (0.081 < 0.3)\n",
            "‚îÇ Chosen:   Action 0 (random)\n",
            "‚îÇ Stats:    Explore 2/9 (22.2%)\n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îå‚îÄ ENVIRONMENT RESPONSE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îÇ Action:     0 (LEFT)\n",
            "‚îÇ Transition: 0 ‚Üí 0\n",
            "‚îÇ Reward:     -5.0\n",
            "‚îÇ Done:       False\n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îå‚îÄ Q-UPDATE #9 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îÇ Transition: S0 --A0--> S0 (R=-5.0)\n",
            "‚îÇ Done:       False\n",
            "‚îÇ Next Q's:   [-0.50, -0.19]\n",
            "‚îÇ Target:     -5 + 0.9√ó-0.190 = -5.171\n",
            "‚îÇ Q-Update:   -0.500 + 0.1√ó-4.671 = -0.967\n",
            "‚îÇ Change:     -0.467\n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "\n",
            "üìç STEP  2 - Current State: 0\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îå‚îÄ ACTION SELECTION (State 0) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îÇ Q-values: [-0.97, -0.19]\n",
            "‚îÇ Best:     Action 1 (Q=-0.190)\n",
            "‚îÇ Decision: EXPLORE üé≤ (0.232 < 0.3)\n",
            "‚îÇ Chosen:   Action 0 (random)\n",
            "‚îÇ Stats:    Explore 3/10 (30.0%)\n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îå‚îÄ ENVIRONMENT RESPONSE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îÇ Action:     0 (LEFT)\n",
            "‚îÇ Transition: 0 ‚Üí 0\n",
            "‚îÇ Reward:     -5.0\n",
            "‚îÇ Done:       False\n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îå‚îÄ Q-UPDATE #10 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îÇ Transition: S0 --A0--> S0 (R=-5.0)\n",
            "‚îÇ Done:       False\n",
            "‚îÇ Next Q's:   [-0.97, -0.19]\n",
            "‚îÇ Target:     -5 + 0.9√ó-0.190 = -5.171\n",
            "‚îÇ Q-Update:   -0.967 + 0.1√ó-4.204 = -1.387\n",
            "‚îÇ Change:     -0.420\n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "\n",
            "üìç STEP  3 - Current State: 0\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îå‚îÄ ACTION SELECTION (State 0) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îÇ Q-values: [-1.39, -0.19]\n",
            "‚îÇ Best:     Action 1 (Q=-0.190)\n",
            "‚îÇ Decision: EXPLOIT üéØ (0.658 ‚â• 0.3)\n",
            "‚îÇ Chosen:   Action 1 (greedy)\n",
            "‚îÇ Stats:    Explore 3/11 (27.3%)\n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îå‚îÄ ENVIRONMENT RESPONSE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îÇ Action:     1 (RIGHT)\n",
            "‚îÇ Transition: 0 ‚Üí 1\n",
            "‚îÇ Reward:     -1.0\n",
            "‚îÇ Done:       False\n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îå‚îÄ Q-UPDATE #11 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îÇ Transition: S0 --A1--> S1 (R=-1.0)\n",
            "‚îÇ Done:       False\n",
            "‚îÇ Next Q's:   [-0.11, -0.19]\n",
            "‚îÇ Target:     -1 + 0.9√ó-0.109 = -1.098\n",
            "‚îÇ Q-Update:   -0.190 + 0.1√ó-0.908 = -0.281\n",
            "‚îÇ Change:     -0.091\n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "\n",
            "üìç STEP  4 - Current State: 1\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îå‚îÄ ACTION SELECTION (State 1) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îÇ Q-values: [-0.11, -0.19]\n",
            "‚îÇ Best:     Action 0 (Q=-0.109)\n",
            "‚îÇ Decision: EXPLORE üé≤ (0.082 < 0.3)\n",
            "‚îÇ Chosen:   Action 1 (random)\n",
            "‚îÇ Stats:    Explore 4/12 (33.3%)\n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îå‚îÄ ENVIRONMENT RESPONSE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îÇ Action:     1 (RIGHT)\n",
            "‚îÇ Transition: 1 ‚Üí 2\n",
            "‚îÇ Reward:     -1.0\n",
            "‚îÇ Done:       False\n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îå‚îÄ Q-UPDATE #12 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îÇ Transition: S1 --A1--> S2 (R=-1.0)\n",
            "‚îÇ Done:       False\n",
            "‚îÇ Next Q's:   [-0.21, 0.00]\n",
            "‚îÇ Target:     -1 + 0.9√ó0.000 = -1.000\n",
            "‚îÇ Q-Update:   -0.190 + 0.1√ó-0.810 = -0.271\n",
            "‚îÇ Change:     -0.081\n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "\n",
            "üìç STEP  5 - Current State: 2\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îå‚îÄ ACTION SELECTION (State 2) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îÇ Q-values: [-0.21, 0.00]\n",
            "‚îÇ Best:     Action 1 (Q=0.000)\n",
            "‚îÇ Decision: EXPLOIT üéØ (0.796 ‚â• 0.3)\n",
            "‚îÇ Chosen:   Action 1 (greedy)\n",
            "‚îÇ Stats:    Explore 4/13 (30.8%)\n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îå‚îÄ ENVIRONMENT RESPONSE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îÇ Action:     1 (RIGHT)\n",
            "‚îÇ Transition: 2 ‚Üí 3\n",
            "‚îÇ Reward:     -1.0\n",
            "‚îÇ Done:       False\n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îå‚îÄ Q-UPDATE #13 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îÇ Transition: S2 --A1--> S3 (R=-1.0)\n",
            "‚îÇ Done:       False\n",
            "‚îÇ Next Q's:   [0.00, 0.00]\n",
            "‚îÇ Target:     -1 + 0.9√ó0.000 = -1.000\n",
            "‚îÇ Q-Update:   0.000 + 0.1√ó-1.000 = -0.100\n",
            "‚îÇ Change:     -0.100\n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "\n",
            "üìç STEP  6 - Current State: 3\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îå‚îÄ ACTION SELECTION (State 3) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îÇ Q-values: [0.00, 0.00]\n",
            "‚îÇ Best:     Action 0 (Q=0.000)\n",
            "‚îÇ Decision: EXPLOIT üéØ (0.864 ‚â• 0.3)\n",
            "‚îÇ Chosen:   Action 0 (greedy)\n",
            "‚îÇ Stats:    Explore 4/14 (28.6%)\n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îå‚îÄ ENVIRONMENT RESPONSE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îÇ Action:     0 (LEFT)\n",
            "‚îÇ Transition: 3 ‚Üí 2\n",
            "‚îÇ Reward:     -1.0\n",
            "‚îÇ Done:       False\n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îå‚îÄ Q-UPDATE #14 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îÇ Transition: S3 --A0--> S2 (R=-1.0)\n",
            "‚îÇ Done:       False\n",
            "‚îÇ Next Q's:   [-0.21, -0.10]\n",
            "‚îÇ Target:     -1 + 0.9√ó-0.100 = -1.090\n",
            "‚îÇ Q-Update:   0.000 + 0.1√ó-1.090 = -0.109\n",
            "‚îÇ Change:     -0.109\n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "\n",
            "üìç STEP  7 - Current State: 2\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îå‚îÄ ACTION SELECTION (State 2) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îÇ Q-values: [-0.21, -0.10]\n",
            "‚îÇ Best:     Action 1 (Q=-0.100)\n",
            "‚îÇ Decision: EXPLOIT üéØ (0.421 ‚â• 0.3)\n",
            "‚îÇ Chosen:   Action 1 (greedy)\n",
            "‚îÇ Stats:    Explore 4/15 (26.7%)\n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îå‚îÄ ENVIRONMENT RESPONSE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îÇ Action:     1 (RIGHT)\n",
            "‚îÇ Transition: 2 ‚Üí 3\n",
            "‚îÇ Reward:     -1.0\n",
            "‚îÇ Done:       False\n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îå‚îÄ Q-UPDATE #15 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îÇ Transition: S2 --A1--> S3 (R=-1.0)\n",
            "‚îÇ Done:       False\n",
            "‚îÇ Next Q's:   [-0.11, 0.00]\n",
            "‚îÇ Target:     -1 + 0.9√ó0.000 = -1.000\n",
            "‚îÇ Q-Update:   -0.100 + 0.1√ó-0.900 = -0.190\n",
            "‚îÇ Change:     -0.090\n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "\n",
            "üìç STEP  8 - Current State: 3\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îå‚îÄ ACTION SELECTION (State 3) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îÇ Q-values: [-0.11, 0.00]\n",
            "‚îÇ Best:     Action 1 (Q=0.000)\n",
            "‚îÇ Decision: EXPLORE üé≤ (0.215 < 0.3)\n",
            "‚îÇ Chosen:   Action 0 (random)\n",
            "‚îÇ Stats:    Explore 5/16 (31.2%)\n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îå‚îÄ ENVIRONMENT RESPONSE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îÇ Action:     0 (LEFT)\n",
            "‚îÇ Transition: 3 ‚Üí 2\n",
            "‚îÇ Reward:     -1.0\n",
            "‚îÇ Done:       False\n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îå‚îÄ Q-UPDATE #16 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îÇ Transition: S3 --A0--> S2 (R=-1.0)\n",
            "‚îÇ Done:       False\n",
            "‚îÇ Next Q's:   [-0.21, -0.19]\n",
            "‚îÇ Target:     -1 + 0.9√ó-0.190 = -1.171\n",
            "‚îÇ Q-Update:   -0.109 + 0.1√ó-1.062 = -0.215\n",
            "‚îÇ Change:     -0.106\n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "\n",
            "‚îå‚îÄ EPISODE 2 SUMMARY ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îÇ Status:     TIMEOUT ‚è∞\n",
            "‚îÇ Steps:      8\n",
            "‚îÇ Reward:     -16.0\n",
            "‚îÇ Goal:       Not reached\n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "\n",
            "‚îå‚îÄ Q-TABLE AFTER EPISODE 2 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "        A0     A1  \n",
            "S0 ‚îÇ -1.39 -0.28 \n",
            "S1 ‚îÇ -0.11 -0.27 \n",
            "S2 ‚îÇ -0.21 -0.19 \n",
            "S3 ‚îÇ -0.22  0.00 \n",
            "S4 ‚îÇ  0.00  0.00 \n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "\n",
            "‚îå‚îÄ CURRENT POLICY ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îÇ State 0: Action 1 (Q=-0.281)\n",
            "‚îÇ State 1: Action 0 (Q=-0.109)\n",
            "‚îÇ State 2: Action 1 (Q=-0.190)\n",
            "‚îÇ State 3: Action 1 (Q=0.000)\n",
            "‚îÇ State 4: Action 0 (Q=0.000)\n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "\n",
            "============================================================\n",
            "üé¨ EPISODE  3/4 - 03:11:44\n",
            "============================================================\n",
            "\n",
            "üìç STEP  1 - Current State: 0\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îå‚îÄ ACTION SELECTION (State 0) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îÇ Q-values: [-1.39, -0.28]\n",
            "‚îÇ Best:     Action 1 (Q=-0.281)\n",
            "‚îÇ Decision: EXPLORE üé≤ (0.269 < 0.3)\n",
            "‚îÇ Chosen:   Action 0 (random)\n",
            "‚îÇ Stats:    Explore 6/17 (35.3%)\n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îå‚îÄ ENVIRONMENT RESPONSE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îÇ Action:     0 (LEFT)\n",
            "‚îÇ Transition: 0 ‚Üí 0\n",
            "‚îÇ Reward:     -5.0\n",
            "‚îÇ Done:       False\n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îå‚îÄ Q-UPDATE #17 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îÇ Transition: S0 --A0--> S0 (R=-5.0)\n",
            "‚îÇ Done:       False\n",
            "‚îÇ Next Q's:   [-1.39, -0.28]\n",
            "‚îÇ Target:     -5 + 0.9√ó-0.281 = -5.253\n",
            "‚îÇ Q-Update:   -1.387 + 0.1√ó-3.865 = -1.774\n",
            "‚îÇ Change:     -0.387\n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "\n",
            "üìç STEP  2 - Current State: 0\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îå‚îÄ ACTION SELECTION (State 0) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îÇ Q-values: [-1.77, -0.28]\n",
            "‚îÇ Best:     Action 1 (Q=-0.281)\n",
            "‚îÇ Decision: EXPLOIT üéØ (0.375 ‚â• 0.3)\n",
            "‚îÇ Chosen:   Action 1 (greedy)\n",
            "‚îÇ Stats:    Explore 6/18 (33.3%)\n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îå‚îÄ ENVIRONMENT RESPONSE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îÇ Action:     1 (RIGHT)\n",
            "‚îÇ Transition: 0 ‚Üí 1\n",
            "‚îÇ Reward:     -1.0\n",
            "‚îÇ Done:       False\n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îå‚îÄ Q-UPDATE #18 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îÇ Transition: S0 --A1--> S1 (R=-1.0)\n",
            "‚îÇ Done:       False\n",
            "‚îÇ Next Q's:   [-0.11, -0.27]\n",
            "‚îÇ Target:     -1 + 0.9√ó-0.109 = -1.098\n",
            "‚îÇ Q-Update:   -0.281 + 0.1√ó-0.817 = -0.363\n",
            "‚îÇ Change:     -0.082\n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "\n",
            "üìç STEP  3 - Current State: 1\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îå‚îÄ ACTION SELECTION (State 1) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îÇ Q-values: [-0.11, -0.27]\n",
            "‚îÇ Best:     Action 0 (Q=-0.109)\n",
            "‚îÇ Decision: EXPLOIT üéØ (0.378 ‚â• 0.3)\n",
            "‚îÇ Chosen:   Action 0 (greedy)\n",
            "‚îÇ Stats:    Explore 6/19 (31.6%)\n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îå‚îÄ ENVIRONMENT RESPONSE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îÇ Action:     0 (LEFT)\n",
            "‚îÇ Transition: 1 ‚Üí 0\n",
            "‚îÇ Reward:     -1.0\n",
            "‚îÇ Done:       False\n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îå‚îÄ Q-UPDATE #19 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îÇ Transition: S1 --A0--> S0 (R=-1.0)\n",
            "‚îÇ Done:       False\n",
            "‚îÇ Next Q's:   [-1.77, -0.36]\n",
            "‚îÇ Target:     -1 + 0.9√ó-0.363 = -1.326\n",
            "‚îÇ Q-Update:   -0.109 + 0.1√ó-1.217 = -0.231\n",
            "‚îÇ Change:     -0.122\n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "\n",
            "üìç STEP  4 - Current State: 0\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îå‚îÄ ACTION SELECTION (State 0) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îÇ Q-values: [-1.77, -0.36]\n",
            "‚îÇ Best:     Action 1 (Q=-0.363)\n",
            "‚îÇ Decision: EXPLOIT üéØ (0.414 ‚â• 0.3)\n",
            "‚îÇ Chosen:   Action 1 (greedy)\n",
            "‚îÇ Stats:    Explore 6/20 (30.0%)\n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îå‚îÄ ENVIRONMENT RESPONSE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îÇ Action:     1 (RIGHT)\n",
            "‚îÇ Transition: 0 ‚Üí 1\n",
            "‚îÇ Reward:     -1.0\n",
            "‚îÇ Done:       False\n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îå‚îÄ Q-UPDATE #20 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îÇ Transition: S0 --A1--> S1 (R=-1.0)\n",
            "‚îÇ Done:       False\n",
            "‚îÇ Next Q's:   [-0.23, -0.27]\n",
            "‚îÇ Target:     -1 + 0.9√ó-0.231 = -1.208\n",
            "‚îÇ Q-Update:   -0.363 + 0.1√ó-0.845 = -0.447\n",
            "‚îÇ Change:     -0.085\n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "\n",
            "üìç STEP  5 - Current State: 1\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îå‚îÄ ACTION SELECTION (State 1) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îÇ Q-values: [-0.23, -0.27]\n",
            "‚îÇ Best:     Action 0 (Q=-0.231)\n",
            "‚îÇ Decision: EXPLOIT üéØ (0.703 ‚â• 0.3)\n",
            "‚îÇ Chosen:   Action 0 (greedy)\n",
            "‚îÇ Stats:    Explore 6/21 (28.6%)\n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îå‚îÄ ENVIRONMENT RESPONSE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îÇ Action:     0 (LEFT)\n",
            "‚îÇ Transition: 1 ‚Üí 0\n",
            "‚îÇ Reward:     -1.0\n",
            "‚îÇ Done:       False\n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îå‚îÄ Q-UPDATE #21 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îÇ Transition: S1 --A0--> S0 (R=-1.0)\n",
            "‚îÇ Done:       False\n",
            "‚îÇ Next Q's:   [-1.77, -0.45]\n",
            "‚îÇ Target:     -1 + 0.9√ó-0.447 = -1.402\n",
            "‚îÇ Q-Update:   -0.231 + 0.1√ó-1.172 = -0.348\n",
            "‚îÇ Change:     -0.117\n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "\n",
            "üìç STEP  6 - Current State: 0\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îå‚îÄ ACTION SELECTION (State 0) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îÇ Q-values: [-1.77, -0.45]\n",
            "‚îÇ Best:     Action 1 (Q=-0.447)\n",
            "‚îÇ Decision: EXPLOIT üéØ (0.417 ‚â• 0.3)\n",
            "‚îÇ Chosen:   Action 1 (greedy)\n",
            "‚îÇ Stats:    Explore 6/22 (27.3%)\n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îå‚îÄ ENVIRONMENT RESPONSE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îÇ Action:     1 (RIGHT)\n",
            "‚îÇ Transition: 0 ‚Üí 1\n",
            "‚îÇ Reward:     -1.0\n",
            "‚îÇ Done:       False\n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îå‚îÄ Q-UPDATE #22 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îÇ Transition: S0 --A1--> S1 (R=-1.0)\n",
            "‚îÇ Done:       False\n",
            "‚îÇ Next Q's:   [-0.35, -0.27]\n",
            "‚îÇ Target:     -1 + 0.9√ó-0.271 = -1.244\n",
            "‚îÇ Q-Update:   -0.447 + 0.1√ó-0.797 = -0.527\n",
            "‚îÇ Change:     -0.080\n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "\n",
            "üìç STEP  7 - Current State: 1\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îå‚îÄ ACTION SELECTION (State 1) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îÇ Q-values: [-0.35, -0.27]\n",
            "‚îÇ Best:     Action 1 (Q=-0.271)\n",
            "‚îÇ Decision: EXPLORE üé≤ (0.052 < 0.3)\n",
            "‚îÇ Chosen:   Action 1 (random)\n",
            "‚îÇ Stats:    Explore 7/23 (30.4%)\n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îå‚îÄ ENVIRONMENT RESPONSE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îÇ Action:     1 (RIGHT)\n",
            "‚îÇ Transition: 1 ‚Üí 2\n",
            "‚îÇ Reward:     -1.0\n",
            "‚îÇ Done:       False\n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îå‚îÄ Q-UPDATE #23 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îÇ Transition: S1 --A1--> S2 (R=-1.0)\n",
            "‚îÇ Done:       False\n",
            "‚îÇ Next Q's:   [-0.21, -0.19]\n",
            "‚îÇ Target:     -1 + 0.9√ó-0.190 = -1.171\n",
            "‚îÇ Q-Update:   -0.271 + 0.1√ó-0.900 = -0.361\n",
            "‚îÇ Change:     -0.090\n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "\n",
            "üìç STEP  8 - Current State: 2\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îå‚îÄ ACTION SELECTION (State 2) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îÇ Q-values: [-0.21, -0.19]\n",
            "‚îÇ Best:     Action 1 (Q=-0.190)\n",
            "‚îÇ Decision: EXPLOIT üéØ (0.613 ‚â• 0.3)\n",
            "‚îÇ Chosen:   Action 1 (greedy)\n",
            "‚îÇ Stats:    Explore 7/24 (29.2%)\n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îå‚îÄ ENVIRONMENT RESPONSE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îÇ Action:     1 (RIGHT)\n",
            "‚îÇ Transition: 2 ‚Üí 3\n",
            "‚îÇ Reward:     -1.0\n",
            "‚îÇ Done:       False\n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îå‚îÄ Q-UPDATE #24 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îÇ Transition: S2 --A1--> S3 (R=-1.0)\n",
            "‚îÇ Done:       False\n",
            "‚îÇ Next Q's:   [-0.22, 0.00]\n",
            "‚îÇ Target:     -1 + 0.9√ó0.000 = -1.000\n",
            "‚îÇ Q-Update:   -0.190 + 0.1√ó-0.810 = -0.271\n",
            "‚îÇ Change:     -0.081\n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "\n",
            "‚îå‚îÄ EPISODE 3 SUMMARY ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îÇ Status:     TIMEOUT ‚è∞\n",
            "‚îÇ Steps:      8\n",
            "‚îÇ Reward:     -12.0\n",
            "‚îÇ Goal:       Not reached\n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "\n",
            "‚îå‚îÄ Q-TABLE AFTER EPISODE 3 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "        A0     A1  \n",
            "S0 ‚îÇ -1.77 -0.53 \n",
            "S1 ‚îÇ -0.35 -0.36 \n",
            "S2 ‚îÇ -0.21 -0.27 \n",
            "S3 ‚îÇ -0.22  0.00 \n",
            "S4 ‚îÇ  0.00  0.00 \n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "\n",
            "‚îå‚îÄ CURRENT POLICY ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îÇ State 0: Action 1 (Q=-0.527)\n",
            "‚îÇ State 1: Action 0 (Q=-0.348)\n",
            "‚îÇ State 2: Action 0 (Q=-0.208)\n",
            "‚îÇ State 3: Action 1 (Q=0.000)\n",
            "‚îÇ State 4: Action 0 (Q=0.000)\n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "\n",
            "============================================================\n",
            "üé¨ EPISODE  4/4 - 03:11:44\n",
            "============================================================\n",
            "\n",
            "üìç STEP  1 - Current State: 0\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îå‚îÄ ACTION SELECTION (State 0) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îÇ Q-values: [-1.77, -0.53]\n",
            "‚îÇ Best:     Action 1 (Q=-0.527)\n",
            "‚îÇ Decision: EXPLOIT üéØ (0.666 ‚â• 0.3)\n",
            "‚îÇ Chosen:   Action 1 (greedy)\n",
            "‚îÇ Stats:    Explore 7/25 (28.0%)\n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îå‚îÄ ENVIRONMENT RESPONSE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îÇ Action:     1 (RIGHT)\n",
            "‚îÇ Transition: 0 ‚Üí 1\n",
            "‚îÇ Reward:     -1.0\n",
            "‚îÇ Done:       False\n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îå‚îÄ Q-UPDATE #25 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îÇ Transition: S0 --A1--> S1 (R=-1.0)\n",
            "‚îÇ Done:       False\n",
            "‚îÇ Next Q's:   [-0.35, -0.36]\n",
            "‚îÇ Target:     -1 + 0.9√ó-0.348 = -1.313\n",
            "‚îÇ Q-Update:   -0.527 + 0.1√ó-0.786 = -0.605\n",
            "‚îÇ Change:     -0.079\n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "\n",
            "üìç STEP  2 - Current State: 1\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îå‚îÄ ACTION SELECTION (State 1) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îÇ Q-values: [-0.35, -0.36]\n",
            "‚îÇ Best:     Action 0 (Q=-0.348)\n",
            "‚îÇ Decision: EXPLOIT üéØ (0.378 ‚â• 0.3)\n",
            "‚îÇ Chosen:   Action 0 (greedy)\n",
            "‚îÇ Stats:    Explore 7/26 (26.9%)\n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îå‚îÄ ENVIRONMENT RESPONSE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îÇ Action:     0 (LEFT)\n",
            "‚îÇ Transition: 1 ‚Üí 0\n",
            "‚îÇ Reward:     -1.0\n",
            "‚îÇ Done:       False\n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îå‚îÄ Q-UPDATE #26 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îÇ Transition: S1 --A0--> S0 (R=-1.0)\n",
            "‚îÇ Done:       False\n",
            "‚îÇ Next Q's:   [-1.77, -0.61]\n",
            "‚îÇ Target:     -1 + 0.9√ó-0.605 = -1.545\n",
            "‚îÇ Q-Update:   -0.348 + 0.1√ó-1.197 = -0.468\n",
            "‚îÇ Change:     -0.120\n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "\n",
            "üìç STEP  3 - Current State: 0\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îå‚îÄ ACTION SELECTION (State 0) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îÇ Q-values: [-1.77, -0.61]\n",
            "‚îÇ Best:     Action 1 (Q=-0.605)\n",
            "‚îÇ Decision: EXPLORE üé≤ (0.233 < 0.3)\n",
            "‚îÇ Chosen:   Action 0 (random)\n",
            "‚îÇ Stats:    Explore 8/27 (29.6%)\n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îå‚îÄ ENVIRONMENT RESPONSE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îÇ Action:     0 (LEFT)\n",
            "‚îÇ Transition: 0 ‚Üí 0\n",
            "‚îÇ Reward:     -5.0\n",
            "‚îÇ Done:       False\n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îå‚îÄ Q-UPDATE #27 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îÇ Transition: S0 --A0--> S0 (R=-5.0)\n",
            "‚îÇ Done:       False\n",
            "‚îÇ Next Q's:   [-1.77, -0.61]\n",
            "‚îÇ Target:     -5 + 0.9√ó-0.605 = -5.545\n",
            "‚îÇ Q-Update:   -1.774 + 0.1√ó-3.771 = -2.151\n",
            "‚îÇ Change:     -0.377\n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "\n",
            "üìç STEP  4 - Current State: 0\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îå‚îÄ ACTION SELECTION (State 0) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îÇ Q-values: [-2.15, -0.61]\n",
            "‚îÇ Best:     Action 1 (Q=-0.605)\n",
            "‚îÇ Decision: EXPLOIT üéØ (0.516 ‚â• 0.3)\n",
            "‚îÇ Chosen:   Action 1 (greedy)\n",
            "‚îÇ Stats:    Explore 8/28 (28.6%)\n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îå‚îÄ ENVIRONMENT RESPONSE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îÇ Action:     1 (RIGHT)\n",
            "‚îÇ Transition: 0 ‚Üí 1\n",
            "‚îÇ Reward:     -1.0\n",
            "‚îÇ Done:       False\n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îå‚îÄ Q-UPDATE #28 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îÇ Transition: S0 --A1--> S1 (R=-1.0)\n",
            "‚îÇ Done:       False\n",
            "‚îÇ Next Q's:   [-0.47, -0.36]\n",
            "‚îÇ Target:     -1 + 0.9√ó-0.361 = -1.325\n",
            "‚îÇ Q-Update:   -0.605 + 0.1√ó-0.720 = -0.677\n",
            "‚îÇ Change:     -0.072\n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "\n",
            "üìç STEP  5 - Current State: 1\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îå‚îÄ ACTION SELECTION (State 1) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îÇ Q-values: [-0.47, -0.36]\n",
            "‚îÇ Best:     Action 1 (Q=-0.361)\n",
            "‚îÇ Decision: EXPLOIT üéØ (0.927 ‚â• 0.3)\n",
            "‚îÇ Chosen:   Action 1 (greedy)\n",
            "‚îÇ Stats:    Explore 8/29 (27.6%)\n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îå‚îÄ ENVIRONMENT RESPONSE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îÇ Action:     1 (RIGHT)\n",
            "‚îÇ Transition: 1 ‚Üí 2\n",
            "‚îÇ Reward:     -1.0\n",
            "‚îÇ Done:       False\n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îå‚îÄ Q-UPDATE #29 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îÇ Transition: S1 --A1--> S2 (R=-1.0)\n",
            "‚îÇ Done:       False\n",
            "‚îÇ Next Q's:   [-0.21, -0.27]\n",
            "‚îÇ Target:     -1 + 0.9√ó-0.208 = -1.187\n",
            "‚îÇ Q-Update:   -0.361 + 0.1√ó-0.826 = -0.444\n",
            "‚îÇ Change:     -0.083\n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "\n",
            "üìç STEP  6 - Current State: 2\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îå‚îÄ ACTION SELECTION (State 2) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îÇ Q-values: [-0.21, -0.27]\n",
            "‚îÇ Best:     Action 0 (Q=-0.208)\n",
            "‚îÇ Decision: EXPLORE üé≤ (0.217 < 0.3)\n",
            "‚îÇ Chosen:   Action 0 (random)\n",
            "‚îÇ Stats:    Explore 9/30 (30.0%)\n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îå‚îÄ ENVIRONMENT RESPONSE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îÇ Action:     0 (LEFT)\n",
            "‚îÇ Transition: 2 ‚Üí 1\n",
            "‚îÇ Reward:     -1.0\n",
            "‚îÇ Done:       False\n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îå‚îÄ Q-UPDATE #30 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îÇ Transition: S2 --A0--> S1 (R=-1.0)\n",
            "‚îÇ Done:       False\n",
            "‚îÇ Next Q's:   [-0.47, -0.44]\n",
            "‚îÇ Target:     -1 + 0.9√ó-0.444 = -1.399\n",
            "‚îÇ Q-Update:   -0.208 + 0.1√ó-1.191 = -0.327\n",
            "‚îÇ Change:     -0.119\n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "\n",
            "üìç STEP  7 - Current State: 1\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îå‚îÄ ACTION SELECTION (State 1) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îÇ Q-values: [-0.47, -0.44]\n",
            "‚îÇ Best:     Action 1 (Q=-0.444)\n",
            "‚îÇ Decision: EXPLORE üé≤ (0.159 < 0.3)\n",
            "‚îÇ Chosen:   Action 1 (random)\n",
            "‚îÇ Stats:    Explore 10/31 (32.3%)\n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îå‚îÄ ENVIRONMENT RESPONSE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îÇ Action:     1 (RIGHT)\n",
            "‚îÇ Transition: 1 ‚Üí 2\n",
            "‚îÇ Reward:     -1.0\n",
            "‚îÇ Done:       False\n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îå‚îÄ Q-UPDATE #31 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îÇ Transition: S1 --A1--> S2 (R=-1.0)\n",
            "‚îÇ Done:       False\n",
            "‚îÇ Next Q's:   [-0.33, -0.27]\n",
            "‚îÇ Target:     -1 + 0.9√ó-0.271 = -1.244\n",
            "‚îÇ Q-Update:   -0.444 + 0.1√ó-0.800 = -0.524\n",
            "‚îÇ Change:     -0.080\n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "\n",
            "üìç STEP  8 - Current State: 2\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îå‚îÄ ACTION SELECTION (State 2) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îÇ Q-values: [-0.33, -0.27]\n",
            "‚îÇ Best:     Action 1 (Q=-0.271)\n",
            "‚îÇ Decision: EXPLOIT üéØ (0.809 ‚â• 0.3)\n",
            "‚îÇ Chosen:   Action 1 (greedy)\n",
            "‚îÇ Stats:    Explore 10/32 (31.2%)\n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îå‚îÄ ENVIRONMENT RESPONSE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îÇ Action:     1 (RIGHT)\n",
            "‚îÇ Transition: 2 ‚Üí 3\n",
            "‚îÇ Reward:     -1.0\n",
            "‚îÇ Done:       False\n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îå‚îÄ Q-UPDATE #32 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îÇ Transition: S2 --A1--> S3 (R=-1.0)\n",
            "‚îÇ Done:       False\n",
            "‚îÇ Next Q's:   [-0.22, 0.00]\n",
            "‚îÇ Target:     -1 + 0.9√ó0.000 = -1.000\n",
            "‚îÇ Q-Update:   -0.271 + 0.1√ó-0.729 = -0.344\n",
            "‚îÇ Change:     -0.073\n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "\n",
            "‚îå‚îÄ EPISODE 4 SUMMARY ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îÇ Status:     TIMEOUT ‚è∞\n",
            "‚îÇ Steps:      8\n",
            "‚îÇ Reward:     -12.0\n",
            "‚îÇ Goal:       Not reached\n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "\n",
            "‚îå‚îÄ Q-TABLE AFTER EPISODE 4 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "        A0     A1  \n",
            "S0 ‚îÇ -2.15 -0.68 \n",
            "S1 ‚îÇ -0.47 -0.52 \n",
            "S2 ‚îÇ -0.33 -0.34 \n",
            "S3 ‚îÇ -0.22  0.00 \n",
            "S4 ‚îÇ  0.00  0.00 \n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "\n",
            "‚îå‚îÄ CURRENT POLICY ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îÇ State 0: Action 1 (Q=-0.677)\n",
            "‚îÇ State 1: Action 0 (Q=-0.468)\n",
            "‚îÇ State 2: Action 0 (Q=-0.327)\n",
            "‚îÇ State 3: Action 1 (Q=0.000)\n",
            "‚îÇ State 4: Action 0 (Q=0.000)\n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "\n",
            "============================================================\n",
            "üéâ TRAINING COMPLETE\n",
            "============================================================\n",
            "‚îÇ Episodes:      4\n",
            "‚îÇ Success Rate:  0/4 (0.0%)\n",
            "‚îÇ Avg Reward:    -13.00\n",
            "‚îÇ Avg Steps:     8.0\n",
            "‚îÇ Exploration:   10/32 (31.2%)\n",
            "‚îÇ Duration:      0.0s\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "EPISODE BREAKDOWN:\n",
            "Ep# ‚îÇ Steps ‚îÇ Reward ‚îÇ Status\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            " 1  ‚îÇ   8   ‚îÇ  -12.0 ‚îÇ   ‚è∞\n",
            " 2  ‚îÇ   8   ‚îÇ  -16.0 ‚îÇ   ‚è∞\n",
            " 3  ‚îÇ   8   ‚îÇ  -12.0 ‚îÇ   ‚è∞\n",
            " 4  ‚îÇ   8   ‚îÇ  -12.0 ‚îÇ   ‚è∞\n",
            "============================================================\n",
            "\n",
            "‚îå‚îÄ FINAL Q-TABLE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "        A0     A1  \n",
            "S0 ‚îÇ -2.15 -0.68 \n",
            "S1 ‚îÇ -0.47 -0.52 \n",
            "S2 ‚îÇ -0.33 -0.34 \n",
            "S3 ‚îÇ -0.22  0.00 \n",
            "S4 ‚îÇ  0.00  0.00 \n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "\n",
            "‚îå‚îÄ CURRENT POLICY ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚îÇ State 0: Action 1 (Q=-0.677)\n",
            "‚îÇ State 1: Action 0 (Q=-0.468)\n",
            "‚îÇ State 2: Action 0 (Q=-0.327)\n",
            "‚îÇ State 3: Action 1 (Q=0.000)\n",
            "‚îÇ State 4: Action 0 (Q=0.000)\n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "\n",
            "============================================================\n",
            "üß™ TESTING TRAINED AGENT (Pure Exploitation)\n",
            "============================================================\n",
            "Starting at state 0\n",
            "\n",
            "Step 1:\n",
            "  Action: 1 (RIGHT) ‚Üí State 0 ‚Üí 1 (Reward: -1.0)\n",
            "Step 2:\n",
            "  Action: 0 (LEFT) ‚Üí State 1 ‚Üí 0 (Reward: -1.0)\n",
            "Step 3:\n",
            "  Action: 1 (RIGHT) ‚Üí State 0 ‚Üí 1 (Reward: -1.0)\n",
            "Step 4:\n",
            "  Action: 0 (LEFT) ‚Üí State 1 ‚Üí 0 (Reward: -1.0)\n",
            "Step 5:\n",
            "  Action: 1 (RIGHT) ‚Üí State 0 ‚Üí 1 (Reward: -1.0)\n",
            "Step 6:\n",
            "  Action: 0 (LEFT) ‚Üí State 1 ‚Üí 0 (Reward: -1.0)\n",
            "Step 7:\n",
            "  Action: 1 (RIGHT) ‚Üí State 0 ‚Üí 1 (Reward: -1.0)\n",
            "Step 8:\n",
            "  Action: 0 (LEFT) ‚Üí State 1 ‚Üí 0 (Reward: -1.0)\n",
            "Step 9:\n",
            "  Action: 1 (RIGHT) ‚Üí State 0 ‚Üí 1 (Reward: -1.0)\n",
            "Step 10:\n",
            "  Action: 0 (LEFT) ‚Üí State 1 ‚Üí 0 (Reward: -1.0)\n",
            "‚ùå FAILED! Could not reach goal in 10 steps\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nITnL6VEIfuQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}