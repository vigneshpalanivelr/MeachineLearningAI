{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9SxI3HkmMuY"
      },
      "source": [
        "# Step 1: Importing Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "31WU3rO0l-W5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print (device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uxxbw7jpmPQx"
      },
      "source": [
        "Step 2: Defining Image Transformations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NS7RrrcemO0j"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OA0cTyHwmXc9"
      },
      "source": [
        "Step 3: Loading the CIFAR-10 Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dB8sAneNmZq_",
        "outputId": "fd71d155-c0af-4dd6-838b-9259e8248da3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 170M/170M [00:02<00:00, 64.1MB/s]\n"
          ]
        }
      ],
      "source": [
        "train_dataset = datasets.CIFAR10(root='./data',\\\n",
        "              train=True, download=True, transform=transform)\n",
        "dataloader = torch.utils.data.DataLoader(train_dataset, \\\n",
        "                                batch_size=32, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_mp9681md1G"
      },
      "source": [
        "Step 4: Defining GAN Hyperparameters Set important training parameters:\n",
        "\n",
        "latent_dim: Dimensionality of the noise vector.\n",
        "lr: Learning rate of the optimizer.\n",
        "beta1, beta2: Beta parameters for Adam optimizer (e.g 0.5, 0.999)\n",
        "num_epochs: Number of times the entire dataset will be processed (e.g 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e0CnJ9Qxmf4M"
      },
      "outputs": [],
      "source": [
        "latent_dim = 100\n",
        "lr = 0.0002\n",
        "beta1 = 0.5\n",
        "beta2 = 0.999\n",
        "num_epochs = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIT_iNW4mnYr"
      },
      "source": [
        "Step 5: Building the Generator\n",
        "Create a neural network that converts random noise into images. Use transpose convolutional layers, batch normalization and ReLU activations. The final layer uses Tanh activation to scale outputs to the range [-1, 1].\n",
        "\n",
        "nn.Linear(latent_dim, 128 * 8 * 8): Defines a fully connected layer that projects the noise vector into a higher dimensional feature space.\n",
        "nn.Upsample(scale_factor=2): Doubles the spatial resolution of the feature maps by upsampling.\n",
        "nn.Conv2d(128, 128, kernel_size=3, padding=1): Applies a convolutional layer keeping the number of channels the same to refine features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jGepz7skmp7Z"
      },
      "outputs": [],
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, latent_dim):\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(latent_dim, 128 * 8 * 8),\n",
        "            nn.ReLU(),\n",
        "            nn.Unflatten(1, (128, 8, 8)),\n",
        "            nn.Upsample(scale_factor=2),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128, momentum=0.78),\n",
        "            nn.ReLU(),\n",
        "            nn.Upsample(scale_factor=2),\n",
        "            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64, momentum=0.78),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 3, kernel_size=3, padding=1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        img = self.model(z)\n",
        "        return img"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "weVrqozpmtWy"
      },
      "source": [
        "Step 6: Building the Discriminator\n",
        "Create a binary classifier network that distinguishes real from fake images. Use convolutional layers, batch normalization, dropout, LeakyReLU activation and a Sigmoid output layer to give a probability between 0 and 1.\n",
        "\n",
        "nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1): Second convolutional layer increasing channels to 64, downsampling further.\n",
        "nn.BatchNorm2d(256, momentum=0.8): Batch normalization for 256 feature maps with momentum 0.8."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rQnGw0lnmmc_"
      },
      "outputs": [],
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "        nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1),\n",
        "        nn.LeakyReLU(0.2),\n",
        "        nn.Dropout(0.25),\n",
        "        nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n",
        "        nn.ZeroPad2d((0, 1, 0, 1)),\n",
        "        nn.BatchNorm2d(64, momentum=0.82),\n",
        "        nn.LeakyReLU(0.25),\n",
        "        nn.Dropout(0.25),\n",
        "        nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
        "        nn.BatchNorm2d(128, momentum=0.82),\n",
        "        nn.LeakyReLU(0.2),\n",
        "        nn.Dropout(0.25),\n",
        "        nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
        "        nn.BatchNorm2d(256, momentum=0.8),\n",
        "        nn.LeakyReLU(0.25),\n",
        "        nn.Dropout(0.25),\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(256 * 5 * 5, 1),\n",
        "        nn.Sigmoid()\n",
        "    )\n",
        "\n",
        "    def forward(self, img):\n",
        "        validity = self.model(img)\n",
        "        return validity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZlN0efqmx7-"
      },
      "source": [
        "Step 7: Initializing GAN Components\n",
        "Generator and Discriminator are initialized on the available device (GPU or CPU).\n",
        "Binary Cross-Entropy (BCE) Loss is chosen as the loss function.\n",
        "Adam optimizers are defined separately for the generator and discriminator with specified learning rates and betas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "twhcNqFrm0Ou"
      },
      "outputs": [],
      "source": [
        "generator = Generator(latent_dim).to(device)\n",
        "discriminator = Discriminator().to(device)\n",
        "\n",
        "adversarial_loss = nn.BCELoss()\n",
        "\n",
        "optimizer_G = optim.Adam(generator.parameters()\\\n",
        "                         , lr=lr, betas=(beta1, beta2))\n",
        "optimizer_D = optim.Adam(discriminator.parameters()\\\n",
        "                         , lr=lr, betas=(beta1, beta2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMO9Zf14m2GV"
      },
      "source": [
        "Step 8: Training the GAN\n",
        "Train for the set number of epochs:\n",
        "\n",
        "1. For each batch train the discriminator on real images and fake images generated by the generator.\n",
        "\n",
        "2. Then train the generator to fool the discriminator.\n",
        "\n",
        "3. Calculate and backpropagate the respective losses.\n",
        "\n",
        "4. Print loss values every 200 batches for progress tracking.\n",
        "\n",
        "5. After each epoch generate and display sample images created by the generator for visual inspection.\n",
        "\n",
        "valid = torch.ones(real_images.size(0), 1, device=device): Create a tensor of ones representing real labels for the discriminator.\n",
        "fake = torch.zeros(real_images.size(0), 1, device=device): Create a tensor of zeros representing fake labels for the discriminator.\n",
        "z = torch.randn(real_images.size(0), latent_dim, device=device): Generate random noise vectors as input for the generator.\n",
        "g_loss = adversarial_loss(discriminator(gen_images), valid): Calculate generator loss based on the discriminator classifying fake images as real.\n",
        "grid = torchvision.utils.make_grid(generated, nrow=4, normalize=True): Arrange generated images into a grid for display, normalizing pixel values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_4aYB3sm4zk",
        "outputId": "cd93f22c-699d-4503-dd95-40968c9a43bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10]                        Batch 100/1563 Discriminator Loss: 0.5099 Generator Loss: 1.3447\n",
            "Epoch [1/10]                        Batch 200/1563 Discriminator Loss: 0.6883 Generator Loss: 0.8880\n",
            "Epoch [1/10]                        Batch 300/1563 Discriminator Loss: 0.5767 Generator Loss: 1.1418\n",
            "Epoch [1/10]                        Batch 400/1563 Discriminator Loss: 0.5058 Generator Loss: 1.4714\n",
            "Epoch [1/10]                        Batch 500/1563 Discriminator Loss: 0.6169 Generator Loss: 1.4345\n",
            "Epoch [1/10]                        Batch 600/1563 Discriminator Loss: 0.6264 Generator Loss: 0.9983\n",
            "Epoch [1/10]                        Batch 700/1563 Discriminator Loss: 0.7263 Generator Loss: 0.7577\n",
            "Epoch [1/10]                        Batch 800/1563 Discriminator Loss: 0.6127 Generator Loss: 0.9320\n",
            "Epoch [1/10]                        Batch 900/1563 Discriminator Loss: 0.6342 Generator Loss: 1.1159\n",
            "Epoch [1/10]                        Batch 1000/1563 Discriminator Loss: 0.6255 Generator Loss: 0.9755\n",
            "Epoch [1/10]                        Batch 1100/1563 Discriminator Loss: 0.7078 Generator Loss: 1.1682\n",
            "Epoch [1/10]                        Batch 1200/1563 Discriminator Loss: 0.4722 Generator Loss: 1.2424\n",
            "Epoch [1/10]                        Batch 1300/1563 Discriminator Loss: 0.5455 Generator Loss: 1.1531\n",
            "Epoch [1/10]                        Batch 1400/1563 Discriminator Loss: 0.7525 Generator Loss: 0.9403\n",
            "Epoch [1/10]                        Batch 1500/1563 Discriminator Loss: 0.6693 Generator Loss: 0.9170\n",
            "Epoch [2/10]                        Batch 100/1563 Discriminator Loss: 0.3703 Generator Loss: 1.4275\n",
            "Epoch [2/10]                        Batch 200/1563 Discriminator Loss: 0.7367 Generator Loss: 1.2531\n",
            "Epoch [2/10]                        Batch 300/1563 Discriminator Loss: 0.5440 Generator Loss: 1.2002\n",
            "Epoch [2/10]                        Batch 400/1563 Discriminator Loss: 0.5055 Generator Loss: 1.7280\n",
            "Epoch [2/10]                        Batch 500/1563 Discriminator Loss: 0.7269 Generator Loss: 1.2677\n",
            "Epoch [2/10]                        Batch 600/1563 Discriminator Loss: 0.6506 Generator Loss: 1.0988\n",
            "Epoch [2/10]                        Batch 700/1563 Discriminator Loss: 0.8242 Generator Loss: 0.8408\n",
            "Epoch [2/10]                        Batch 800/1563 Discriminator Loss: 0.6630 Generator Loss: 0.9520\n",
            "Epoch [2/10]                        Batch 900/1563 Discriminator Loss: 0.6258 Generator Loss: 0.9742\n",
            "Epoch [2/10]                        Batch 1000/1563 Discriminator Loss: 0.8170 Generator Loss: 0.7315\n",
            "Epoch [2/10]                        Batch 1100/1563 Discriminator Loss: 0.5103 Generator Loss: 1.1287\n",
            "Epoch [2/10]                        Batch 1200/1563 Discriminator Loss: 0.6433 Generator Loss: 2.3793\n",
            "Epoch [2/10]                        Batch 1300/1563 Discriminator Loss: 0.4610 Generator Loss: 1.1946\n",
            "Epoch [2/10]                        Batch 1400/1563 Discriminator Loss: 0.4634 Generator Loss: 1.8162\n",
            "Epoch [2/10]                        Batch 1500/1563 Discriminator Loss: 0.6100 Generator Loss: 1.0169\n",
            "Epoch [3/10]                        Batch 100/1563 Discriminator Loss: 0.6568 Generator Loss: 0.9715\n",
            "Epoch [3/10]                        Batch 200/1563 Discriminator Loss: 0.5669 Generator Loss: 1.0223\n",
            "Epoch [3/10]                        Batch 300/1563 Discriminator Loss: 0.5906 Generator Loss: 1.3232\n",
            "Epoch [3/10]                        Batch 400/1563 Discriminator Loss: 0.5317 Generator Loss: 0.9602\n",
            "Epoch [3/10]                        Batch 500/1563 Discriminator Loss: 0.4822 Generator Loss: 1.4356\n",
            "Epoch [3/10]                        Batch 600/1563 Discriminator Loss: 0.5372 Generator Loss: 1.1684\n",
            "Epoch [3/10]                        Batch 700/1563 Discriminator Loss: 0.7232 Generator Loss: 0.7354\n",
            "Epoch [3/10]                        Batch 800/1563 Discriminator Loss: 0.6293 Generator Loss: 0.9483\n",
            "Epoch [3/10]                        Batch 900/1563 Discriminator Loss: 0.5745 Generator Loss: 1.1862\n",
            "Epoch [3/10]                        Batch 1000/1563 Discriminator Loss: 0.9529 Generator Loss: 1.1527\n",
            "Epoch [3/10]                        Batch 1100/1563 Discriminator Loss: 0.5207 Generator Loss: 0.8988\n",
            "Epoch [3/10]                        Batch 1200/1563 Discriminator Loss: 0.6006 Generator Loss: 1.0024\n",
            "Epoch [3/10]                        Batch 1300/1563 Discriminator Loss: 0.7668 Generator Loss: 1.0299\n",
            "Epoch [3/10]                        Batch 1400/1563 Discriminator Loss: 0.7782 Generator Loss: 1.0176\n",
            "Epoch [3/10]                        Batch 1500/1563 Discriminator Loss: 0.6429 Generator Loss: 0.7541\n",
            "Epoch [4/10]                        Batch 100/1563 Discriminator Loss: 0.5419 Generator Loss: 1.3991\n",
            "Epoch [4/10]                        Batch 200/1563 Discriminator Loss: 0.3886 Generator Loss: 1.1109\n",
            "Epoch [4/10]                        Batch 300/1563 Discriminator Loss: 0.4898 Generator Loss: 1.8057\n",
            "Epoch [4/10]                        Batch 400/1563 Discriminator Loss: 0.4157 Generator Loss: 1.2345\n",
            "Epoch [4/10]                        Batch 500/1563 Discriminator Loss: 0.4589 Generator Loss: 1.7557\n",
            "Epoch [4/10]                        Batch 600/1563 Discriminator Loss: 0.6221 Generator Loss: 0.7671\n",
            "Epoch [4/10]                        Batch 700/1563 Discriminator Loss: 0.3609 Generator Loss: 2.4731\n",
            "Epoch [4/10]                        Batch 800/1563 Discriminator Loss: 0.3272 Generator Loss: 1.5375\n",
            "Epoch [4/10]                        Batch 900/1563 Discriminator Loss: 0.5328 Generator Loss: 1.1550\n",
            "Epoch [4/10]                        Batch 1000/1563 Discriminator Loss: 0.5123 Generator Loss: 1.6200\n",
            "Epoch [4/10]                        Batch 1100/1563 Discriminator Loss: 0.5334 Generator Loss: 1.2535\n",
            "Epoch [4/10]                        Batch 1200/1563 Discriminator Loss: 0.3947 Generator Loss: 1.8410\n",
            "Epoch [4/10]                        Batch 1300/1563 Discriminator Loss: 0.3851 Generator Loss: 2.2081\n",
            "Epoch [4/10]                        Batch 1400/1563 Discriminator Loss: 0.7132 Generator Loss: 0.8948\n",
            "Epoch [4/10]                        Batch 1500/1563 Discriminator Loss: 0.6574 Generator Loss: 1.3177\n",
            "Epoch [5/10]                        Batch 100/1563 Discriminator Loss: 0.7177 Generator Loss: 0.9012\n",
            "Epoch [5/10]                        Batch 200/1563 Discriminator Loss: 0.4560 Generator Loss: 1.6022\n",
            "Epoch [5/10]                        Batch 300/1563 Discriminator Loss: 0.5334 Generator Loss: 2.1428\n",
            "Epoch [5/10]                        Batch 400/1563 Discriminator Loss: 0.7045 Generator Loss: 0.8546\n",
            "Epoch [5/10]                        Batch 500/1563 Discriminator Loss: 0.3976 Generator Loss: 1.7819\n",
            "Epoch [5/10]                        Batch 600/1563 Discriminator Loss: 0.5956 Generator Loss: 1.5913\n",
            "Epoch [5/10]                        Batch 700/1563 Discriminator Loss: 0.5924 Generator Loss: 0.6480\n",
            "Epoch [5/10]                        Batch 800/1563 Discriminator Loss: 0.4804 Generator Loss: 1.2555\n",
            "Epoch [5/10]                        Batch 900/1563 Discriminator Loss: 0.4613 Generator Loss: 1.4848\n",
            "Epoch [5/10]                        Batch 1000/1563 Discriminator Loss: 0.9289 Generator Loss: 0.9551\n",
            "Epoch [5/10]                        Batch 1100/1563 Discriminator Loss: 0.4868 Generator Loss: 1.6316\n",
            "Epoch [5/10]                        Batch 1200/1563 Discriminator Loss: 0.5724 Generator Loss: 1.3343\n",
            "Epoch [5/10]                        Batch 1300/1563 Discriminator Loss: 0.6287 Generator Loss: 1.3298\n",
            "Epoch [5/10]                        Batch 1400/1563 Discriminator Loss: 0.2784 Generator Loss: 2.2062\n",
            "Epoch [5/10]                        Batch 1500/1563 Discriminator Loss: 0.6748 Generator Loss: 1.6123\n",
            "Epoch [6/10]                        Batch 100/1563 Discriminator Loss: 0.4815 Generator Loss: 2.2981\n",
            "Epoch [6/10]                        Batch 200/1563 Discriminator Loss: 0.6127 Generator Loss: 1.8425\n",
            "Epoch [6/10]                        Batch 300/1563 Discriminator Loss: 0.3456 Generator Loss: 1.1737\n",
            "Epoch [6/10]                        Batch 400/1563 Discriminator Loss: 0.5691 Generator Loss: 1.3121\n",
            "Epoch [6/10]                        Batch 500/1563 Discriminator Loss: 0.4614 Generator Loss: 0.9457\n",
            "Epoch [6/10]                        Batch 600/1563 Discriminator Loss: 0.6433 Generator Loss: 1.5425\n",
            "Epoch [6/10]                        Batch 700/1563 Discriminator Loss: 0.4696 Generator Loss: 1.7202\n",
            "Epoch [6/10]                        Batch 800/1563 Discriminator Loss: 0.5427 Generator Loss: 0.6914\n",
            "Epoch [6/10]                        Batch 900/1563 Discriminator Loss: 0.4732 Generator Loss: 1.5239\n",
            "Epoch [6/10]                        Batch 1000/1563 Discriminator Loss: 0.3363 Generator Loss: 2.1422\n",
            "Epoch [6/10]                        Batch 1100/1563 Discriminator Loss: 0.5734 Generator Loss: 1.3921\n",
            "Epoch [6/10]                        Batch 1200/1563 Discriminator Loss: 0.5359 Generator Loss: 1.5590\n",
            "Epoch [6/10]                        Batch 1300/1563 Discriminator Loss: 0.5762 Generator Loss: 1.0131\n",
            "Epoch [6/10]                        Batch 1400/1563 Discriminator Loss: 0.4308 Generator Loss: 1.7521\n",
            "Epoch [6/10]                        Batch 1500/1563 Discriminator Loss: 0.4480 Generator Loss: 1.2779\n",
            "Epoch [7/10]                        Batch 100/1563 Discriminator Loss: 0.3488 Generator Loss: 2.4049\n",
            "Epoch [7/10]                        Batch 200/1563 Discriminator Loss: 0.5819 Generator Loss: 1.4238\n",
            "Epoch [7/10]                        Batch 300/1563 Discriminator Loss: 0.5884 Generator Loss: 1.0778\n",
            "Epoch [7/10]                        Batch 400/1563 Discriminator Loss: 0.5578 Generator Loss: 0.9341\n",
            "Epoch [7/10]                        Batch 500/1563 Discriminator Loss: 0.8251 Generator Loss: 0.9163\n",
            "Epoch [7/10]                        Batch 600/1563 Discriminator Loss: 0.4898 Generator Loss: 1.6427\n",
            "Epoch [7/10]                        Batch 700/1563 Discriminator Loss: 0.8668 Generator Loss: 0.4694\n",
            "Epoch [7/10]                        Batch 800/1563 Discriminator Loss: 0.4886 Generator Loss: 0.8516\n",
            "Epoch [7/10]                        Batch 900/1563 Discriminator Loss: 0.4109 Generator Loss: 1.5245\n",
            "Epoch [7/10]                        Batch 1000/1563 Discriminator Loss: 0.7447 Generator Loss: 1.7898\n",
            "Epoch [7/10]                        Batch 1100/1563 Discriminator Loss: 0.5576 Generator Loss: 0.9587\n",
            "Epoch [7/10]                        Batch 1200/1563 Discriminator Loss: 0.4558 Generator Loss: 1.5274\n",
            "Epoch [7/10]                        Batch 1300/1563 Discriminator Loss: 0.4641 Generator Loss: 1.3341\n",
            "Epoch [7/10]                        Batch 1400/1563 Discriminator Loss: 0.3655 Generator Loss: 1.3309\n",
            "Epoch [7/10]                        Batch 1500/1563 Discriminator Loss: 0.3464 Generator Loss: 1.6389\n",
            "Epoch [8/10]                        Batch 100/1563 Discriminator Loss: 0.7069 Generator Loss: 1.5996\n",
            "Epoch [8/10]                        Batch 200/1563 Discriminator Loss: 0.3768 Generator Loss: 0.7890\n",
            "Epoch [8/10]                        Batch 300/1563 Discriminator Loss: 0.7215 Generator Loss: 1.7108\n",
            "Epoch [8/10]                        Batch 400/1563 Discriminator Loss: 0.8358 Generator Loss: 2.4079\n",
            "Epoch [8/10]                        Batch 500/1563 Discriminator Loss: 0.5723 Generator Loss: 0.9160\n",
            "Epoch [8/10]                        Batch 600/1563 Discriminator Loss: 0.7278 Generator Loss: 0.9069\n",
            "Epoch [8/10]                        Batch 700/1563 Discriminator Loss: 0.7518 Generator Loss: 0.9124\n",
            "Epoch [8/10]                        Batch 800/1563 Discriminator Loss: 0.6111 Generator Loss: 1.1205\n",
            "Epoch [8/10]                        Batch 900/1563 Discriminator Loss: 0.5925 Generator Loss: 0.6266\n",
            "Epoch [8/10]                        Batch 1000/1563 Discriminator Loss: 0.3879 Generator Loss: 0.9094\n",
            "Epoch [8/10]                        Batch 1100/1563 Discriminator Loss: 0.6370 Generator Loss: 0.8161\n",
            "Epoch [8/10]                        Batch 1200/1563 Discriminator Loss: 0.6711 Generator Loss: 1.9683\n",
            "Epoch [8/10]                        Batch 1300/1563 Discriminator Loss: 0.4622 Generator Loss: 1.3105\n",
            "Epoch [8/10]                        Batch 1400/1563 Discriminator Loss: 0.4755 Generator Loss: 1.1697\n",
            "Epoch [8/10]                        Batch 1500/1563 Discriminator Loss: 0.5344 Generator Loss: 1.1898\n",
            "Epoch [9/10]                        Batch 100/1563 Discriminator Loss: 0.7921 Generator Loss: 1.1774\n",
            "Epoch [9/10]                        Batch 200/1563 Discriminator Loss: 0.5428 Generator Loss: 2.0153\n",
            "Epoch [9/10]                        Batch 300/1563 Discriminator Loss: 0.5213 Generator Loss: 1.2329\n",
            "Epoch [9/10]                        Batch 400/1563 Discriminator Loss: 0.5980 Generator Loss: 1.4235\n",
            "Epoch [9/10]                        Batch 500/1563 Discriminator Loss: 0.7465 Generator Loss: 1.0859\n",
            "Epoch [9/10]                        Batch 600/1563 Discriminator Loss: 0.4867 Generator Loss: 1.0119\n",
            "Epoch [9/10]                        Batch 700/1563 Discriminator Loss: 0.3887 Generator Loss: 0.9272\n",
            "Epoch [9/10]                        Batch 800/1563 Discriminator Loss: 0.8797 Generator Loss: 0.7039\n",
            "Epoch [9/10]                        Batch 900/1563 Discriminator Loss: 0.5658 Generator Loss: 1.0179\n",
            "Epoch [9/10]                        Batch 1000/1563 Discriminator Loss: 0.5488 Generator Loss: 1.0008\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(num_epochs):\n",
        "    for i, batch in enumerate(dataloader):\n",
        "\n",
        "        real_images = batch[0].to(device)\n",
        "\n",
        "        valid = torch.ones(real_images.size(0), 1, device=device)\n",
        "        fake = torch.zeros(real_images.size(0), 1, device=device)\n",
        "\n",
        "        real_images = real_images.to(device)\n",
        "\n",
        "        optimizer_D.zero_grad()\n",
        "\n",
        "        z = torch.randn(real_images.size(0), latent_dim, device=device)\n",
        "\n",
        "        fake_images = generator(z)\n",
        "\n",
        "        real_loss = adversarial_loss(discriminator\\\n",
        "                                     (real_images), valid)\n",
        "        fake_loss = adversarial_loss(discriminator\\\n",
        "                                     (fake_images.detach()), fake)\n",
        "        d_loss = (real_loss + fake_loss) / 2\n",
        "\n",
        "        d_loss.backward()\n",
        "        optimizer_D.step()\n",
        "\n",
        "        optimizer_G.zero_grad()\n",
        "\n",
        "        gen_images = generator(z)\n",
        "\n",
        "        g_loss = adversarial_loss(discriminator(gen_images), valid)\n",
        "        g_loss.backward()\n",
        "        optimizer_G.step()\n",
        "\n",
        "        if (i + 1) % 100 == 0:\n",
        "            print(\n",
        "                f\"Epoch [{epoch+1}/{num_epochs}]\\\n",
        "                        Batch {i+1}/{len(dataloader)} \"\n",
        "                f\"Discriminator Loss: {d_loss.item():.4f} \"\n",
        "                f\"Generator Loss: {g_loss.item():.4f}\"\n",
        "            )\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        with torch.no_grad():\n",
        "            z = torch.randn(16, latent_dim, device=device)\n",
        "            generated = generator(z).detach().cpu()\n",
        "            grid = torchvision.utils.make_grid(generated,\\\n",
        "                                        nrow=4, normalize=True)\n",
        "            plt.imshow(np.transpose(grid, (1, 2, 0)))\n",
        "            plt.axis(\"off\")\n",
        "            plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73ktDWz6m67Q"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}