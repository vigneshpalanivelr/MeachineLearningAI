{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 526
        },
        "id": "peTshU_P938s",
        "outputId": "cbf2c98f-f679-46ca-a9f6-41f00a978d49"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/activations/leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/usr/local/lib/python3.12/dist-packages/keras/src/backend/tensorflow/trainer.py:83: UserWarning: The model does not have any trainable weights.\n",
            "  warnings.warn(\"The model does not have any trainable weights.\")\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3986117130.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0mfake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0md_loss_real\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0md_loss_fake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/_core/numeric.py\u001b[0m in \u001b[0;36mones\u001b[0;34m(shape, dtype, order, device, like)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m \u001b[0;34m@\u001b[0m\u001b[0mset_array_function_like_doc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mset_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'numpy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'C'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlike\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Reshape, Conv2DTranspose, LeakyReLU, Flatten, Conv2D\n",
        "from tensorflow.keras.models import Sequential\n",
        "import numpy as np\n",
        "\n",
        "# -------------------------\n",
        "# 1) BUILD GENERATOR (CNN)\n",
        "# -------------------------\n",
        "def build_generator():\n",
        "    model = Sequential([\n",
        "        Dense(7*7*256, activation=\"relu\", input_dim=100),\n",
        "        Reshape((7, 7, 256)),\n",
        "        Conv2DTranspose(128, (4,4), strides=2, padding=\"same\"),\n",
        "        LeakyReLU(alpha=0.2),\n",
        "        Conv2DTranspose(64, (4,4), strides=2, padding=\"same\"),\n",
        "        LeakyReLU(alpha=0.2),\n",
        "        Conv2DTranspose(1, (7,7), activation=\"tanh\", padding=\"same\")\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# -------------------------\n",
        "# 2) BUILD DISCRIMINATOR\n",
        "# -------------------------\n",
        "def build_discriminator():\n",
        "    model = Sequential([\n",
        "        Conv2D(64, (3,3), strides=2, padding=\"same\", input_shape=(28,28,1)),\n",
        "        LeakyReLU(0.2),\n",
        "        Flatten(),\n",
        "        Dense(1, activation=\"sigmoid\")\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# -------------------------\n",
        "# 3) TRAINING LOOP (SHORT)\n",
        "# -------------------------\n",
        "(X_train, _), _ = tf.keras.datasets.mnist.load_data()\n",
        "X_train = (X_train.astype(\"float32\") - 127.5) / 127.5\n",
        "X_train = np.expand_dims(X_train, axis=-1)\n",
        "\n",
        "generator = build_generator()\n",
        "discriminator = build_discriminator()\n",
        "discriminator.compile(optimizer=\"adam\", loss=\"binary_crossentropy\")\n",
        "\n",
        "gan = Sequential([generator, discriminator])\n",
        "discriminator.trainable = False\n",
        "gan.compile(optimizer=\"adam\", loss=\"binary_crossentropy\")\n",
        "\n",
        "# Train for a few steps for demo\n",
        "for step in range(2000):\n",
        "    # train discriminator\n",
        "    idx = np.random.randint(0, X_train.shape[0], 32)\n",
        "    real = X_train[idx]\n",
        "    noise = np.random.randn(32, 100)\n",
        "    fake = generator.predict(noise, verbose=0)\n",
        "\n",
        "    d_loss_real = discriminator.train_on_batch(real, np.ones((32,1)))\n",
        "    d_loss_fake = discriminator.train_on_batch(fake, np.zeros((32,1)))\n",
        "\n",
        "    # train GAN (generator wants discriminator to output 1)\n",
        "    noise = np.random.randn(32, 100)\n",
        "    gan.train_on_batch(noise, np.ones((32,1)))\n",
        "\n",
        "# Generate one sample\n",
        "noise = np.random.randn(1, 100)\n",
        "sample = generator.predict(noise, verbose=0)[0]\n",
        "print(\"Generated CNN-MNIST sample shape:\", sample.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import LSTM, Dense, RepeatVector, TimeDistributed\n",
        "from tensorflow.keras.models import Sequential\n",
        "import numpy as np\n",
        "\n",
        "# -------------------------\n",
        "# 1) PREPARE DATA\n",
        "# -------------------------\n",
        "(X_train, _), _ = tf.keras.datasets.mnist.load_data()\n",
        "X_train = X_train / 255.0  # normalize\n",
        "X_train = X_train.reshape(-1, 28, 28)  # sequence: 28 rows, 28 features\n",
        "\n",
        "# -------------------------\n",
        "# 2) BUILD RNN GENERATOR\n",
        "# -------------------------\n",
        "def build_rnn_generator():\n",
        "    model = Sequential([\n",
        "        Dense(128, activation=\"relu\", input_dim=100),\n",
        "        RepeatVector(28),   # output sequence length = 28 rows\n",
        "        LSTM(128, return_sequences=True),\n",
        "        TimeDistributed(Dense(28, activation=\"sigmoid\"))   # each row = 28 pixels\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# -------------------------\n",
        "# 3) TRAIN SHORT\n",
        "# -------------------------\n",
        "gen = build_rnn_generator()\n",
        "gen.compile(optimizer=\"adam\", loss=\"mse\")\n",
        "\n",
        "# Tiny training loop (demo)\n",
        "noise = np.random.randn(1000, 100)\n",
        "gen.fit(noise, X_train[:1000], epochs=5, batch_size=32)\n",
        "\n",
        "# Generate an MNIST-like image\n",
        "sample = gen.predict(np.random.randn(1, 100), verbose=0)[0]\n",
        "print(\"Generated RNN-MNIST sample shape:\", sample.shape)\n"
      ],
      "metadata": {
        "id": "v1XSqOvh9_wt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# ============================\n",
        "# 1. Load MNIST\n",
        "# ============================\n",
        "(x_train, _), _ = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# Normalize pixel values from [0,255] → [0,1]\n",
        "x_train = (x_train.astype(np.float32) / 255.0)\n",
        "\n",
        "# Flatten each image (28x28 → 784 vector)\n",
        "x_train = x_train.reshape(-1, 784)\n",
        "\n",
        "# Input/output dimension = number of pixels\n",
        "D = 784\n",
        "\n",
        "# Hidden layer width (typical MADE uses large layers)\n",
        "H = 512\n",
        "\n",
        "\n",
        "# ============================\n",
        "# 2. Mask creation functions\n",
        "# ============================\n",
        "\n",
        "def create_mask(input_size, hidden_sizes, output_size):\n",
        "    \"\"\"\n",
        "    Creates the MADE masks for:\n",
        "    - input → hidden1\n",
        "    - hidden1 → hidden2\n",
        "    - hidden_last → output\n",
        "\n",
        "    The masks enforce autoregressive ordering.\n",
        "    \"\"\"\n",
        "\n",
        "    degrees = []  # stores assigned \"indices\" for each layer\n",
        "\n",
        "    # Degrees for input layer = 1..784\n",
        "    # (Pixel 1 must come before pixel 2, etc.)\n",
        "    degrees.append(np.arange(1, input_size + 1))\n",
        "\n",
        "    # Degrees for hidden layers (random 1..D-1)\n",
        "    # Ensures different neurons learn different dependencies\n",
        "    for h in hidden_sizes:\n",
        "        degrees.append(np.random.randint(1, input_size, size=h))\n",
        "\n",
        "    # Degrees for output layer = 1..784 (same order as input)\n",
        "    degrees.append(np.arange(1, output_size + 1))\n",
        "\n",
        "    masks = []\n",
        "\n",
        "    # Build masks for each layer:\n",
        "    # Mask[l][i,j] = 1 if degree[i] ≤ degree[j]\n",
        "    # i = neuron in previous layer\n",
        "    # j = neuron in next layer\n",
        "    for l in range(len(hidden_sizes) + 1):\n",
        "        mask = (degrees[l][:, None] <= degrees[l + 1][None, :]).astype(np.float32)\n",
        "        masks.append(mask)\n",
        "\n",
        "    return masks, degrees\n",
        "\n",
        "\n",
        "\n",
        "# ============================\n",
        "# 3. Masked Dense Layer\n",
        "# ============================\n",
        "\n",
        "class MaskedDense(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    A Dense layer where weights are multiplied by a binary mask.\n",
        "    This forces autoregressive constraints.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, units, mask=None, activation=None):\n",
        "        super().__init__()\n",
        "        self.units = units\n",
        "        self.mask = mask  # the binary mask matrix\n",
        "        self.activation = tf.keras.activations.get(activation)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # Weight matrix W: (input_dim x units)\n",
        "        self.w = self.add_weight(\n",
        "            shape=(input_shape[-1], self.units),\n",
        "            initializer='glorot_uniform'\n",
        "        )\n",
        "\n",
        "        # Bias vector\n",
        "        self.b = self.add_weight(\n",
        "            shape=(self.units,),\n",
        "            initializer='zeros'\n",
        "        )\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Apply mask: zero out forbidden connections\n",
        "        masked_w = self.w * self.mask\n",
        "\n",
        "        # Standard dense forward pass\n",
        "        out = tf.matmul(inputs, masked_w) + self.b\n",
        "\n",
        "        # Apply activation (ReLU/sigmoid)\n",
        "        if self.activation:\n",
        "            return self.activation(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "# ============================\n",
        "# 4. Build MADE Model\n",
        "# ============================\n",
        "\n",
        "def build_made(input_size, hidden_sizes):\n",
        "    \"\"\"\n",
        "    Builds the MADE network:\n",
        "    Input → MaskedDense → MaskedDense → Output(MaskedDense)\n",
        "    \"\"\"\n",
        "\n",
        "    # Create masks for all layers\n",
        "    masks, degrees = create_mask(input_size, hidden_sizes, input_size)\n",
        "\n",
        "    inputs = tf.keras.Input(shape=(input_size,))\n",
        "    x = inputs\n",
        "\n",
        "    # Hidden layers with masks\n",
        "    for h, m in zip(hidden_sizes, masks[:-1]):\n",
        "        x = MaskedDense(h, mask=m, activation='relu')(x)\n",
        "\n",
        "    # Output layer with sigmoid (pixel probabilities)\n",
        "    outputs = MaskedDense(input_size, mask=masks[-1], activation='sigmoid')(x)\n",
        "\n",
        "    model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "    # Bernoulli likelihood for each pixel\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "# ============================\n",
        "# 5. Train MADE\n",
        "# ============================\n",
        "\n",
        "made = build_made(D, [H, H])  # 2 hidden layers, 512 each\n",
        "made.summary()\n",
        "\n",
        "# MADE learns to reconstruct the image pixel-by-pixel\n",
        "# but with autoregressive masking (no cheating)\n",
        "made.fit(\n",
        "    x_train,     # input image\n",
        "    x_train,     # target is same image\n",
        "    batch_size=256,\n",
        "    epochs=5\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# ============================\n",
        "# 6. Sampling from MADE\n",
        "# ============================\n",
        "\n",
        "def sample_from_made(model, D=784):\n",
        "    \"\"\"\n",
        "    Generates a new MNIST-like image using autoregressive sampling:\n",
        "    x1 first, then x2 using x1, then x3 using x1,x2, etc.\n",
        "    \"\"\"\n",
        "\n",
        "    sample = np.zeros((1, D), dtype=np.float32)\n",
        "\n",
        "    # Loop pixel-by-pixel (1..784)\n",
        "    for i in range(D):\n",
        "\n",
        "        # Predict all pixel probabilities given partially filled sample\n",
        "        out = model.predict(sample, verbose=0)\n",
        "\n",
        "        prob = out[0, i]  # probability of pixel i being 1\n",
        "\n",
        "        # Sample pixel i from Bernoulli(prob)\n",
        "        sample[0, i] = (np.random.rand() < prob)\n",
        "\n",
        "    # Convert vector → 28x28 image\n",
        "    return sample.reshape(28, 28)\n",
        "\n",
        "\n",
        "\n",
        "# Generate one MNIST-like sample\n",
        "generated = sample_from_made(made)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(generated, cmap='gray')\n",
        "plt.title(\"MADE Sample\")\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Qz5xjlCKFBsb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}