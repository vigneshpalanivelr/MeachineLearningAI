{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1: Feature Extraction via Dimensionality Reduction using Autoencoders\n",
    "\n",
    "## Problem Statement\n",
    "This assignment focuses on feature extraction via dimensionality reduction using variants of autoencoders with CIFAR10 and MNIST datasets.\n",
    "\n",
    "### Tasks Overview:\n",
    "1. **Task 1**: PCA-based feature extraction and classification (Standard vs Randomized PCA)\n",
    "2. **Task 2**: Single-layer linear autoencoder comparison with PCA\n",
    "3. **Task 3**: Deep convolutional autoencoder with reconstruction error analysis\n",
    "4. **Task 4**: MNIST 7-segment LED display classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA, IncrementalPCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, constraints, regularizers\n",
    "from tensorflow.keras.datasets import cifar10, mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "Load CIFAR10 dataset and convert to grayscale. Split into 70% training and 30% validation/test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CIFAR10 dataset\n",
    "# CIFAR10 contains 60,000 32x32 color images in 10 classes\n",
    "# Default split: 50,000 training + 10,000 test\n",
    "(X_train_color, y_train), (X_test_color, y_test) = cifar10.load_data()\n",
    "\n",
    "print(f\"Original CIFAR10 shapes:\")\n",
    "print(f\"Training: {X_train_color.shape}, Labels: {y_train.shape}\")\n",
    "print(f\"Test: {X_test_color.shape}, Labels: {y_test.shape}\")\n",
    "\n",
    "# Convert RGB images to grayscale as per assignment requirement\n",
    "# TensorFlow's rgb_to_grayscale uses ITU-R 601-2 luma transform\n",
    "# Formula: Gray = 0.299*R + 0.587*G + 0.114*B\n",
    "# This reduces dimensionality from 3 channels to 1 while preserving luminance information\n",
    "X_train_gray = tf.image.rgb_to_grayscale(X_train_color).numpy().squeeze()\n",
    "X_test_gray = tf.image.rgb_to_grayscale(X_test_color).numpy().squeeze()\n",
    "\n",
    "print(f\"\\nGrayscale shapes:\")\n",
    "print(f\"Training: {X_train_gray.shape}\")\n",
    "print(f\"Test: {X_test_gray.shape}\")\n",
    "\n",
    "# Combine and resplit data according to assignment requirement (70% train, 30% test)\n",
    "# Original split was 50k/10k, we need 42k/18k\n",
    "X_all = np.vstack([X_train_gray, X_test_gray])\n",
    "y_all = np.vstack([y_train, y_test])\n",
    "\n",
    "# Perform stratified split to maintain class distribution\n",
    "# stratify=y_all ensures each split has same proportion of each class\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_all, y_all, train_size=0.7, random_state=42, stratify=y_all\n",
    ")\n",
    "\n",
    "print(f\"\\nFinal 70/30 split:\")\n",
    "print(f\"Training: {X_train.shape}, Labels: {y_train.shape}\")\n",
    "print(f\"Test: {X_test.shape}, Labels: {y_test.shape}\")\n",
    "\n",
    "# Flatten images for traditional ML methods (PCA, Logistic Regression)\n",
    "# Convert from (N, 32, 32) to (N, 1024) for matrix operations\n",
    "# Keep original shape for CNN-based autoencoders later\n",
    "X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
    "X_test_flat = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "print(f\"\\nFlattened for PCA:\")\n",
    "print(f\"Training: {X_train_flat.shape}\")\n",
    "print(f\"Test: {X_test_flat.shape}\")\n",
    "\n",
    "# Flatten labels from (N, 1) to (N,) for sklearn compatibility\n",
    "y_train = y_train.flatten()\n",
    "y_test = y_test.flatten()\n",
    "\n",
    "# Define class names for visualization and reporting\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "               'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample grayscale images to verify conversion\n",
    "# Display first 10 images with their class labels\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i in range(10):\n",
    "    # Show grayscale image (single channel)\n",
    "    axes[i].imshow(X_train[i], cmap='gray')\n",
    "    axes[i].set_title(f\"{class_names[y_train[i]]}\")\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle('Sample Grayscale CIFAR10 Images')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: PCA-based Feature Extraction and Classification [2 marks]\n",
    "\n",
    "### Objectives:\n",
    "1. Perform standard PCA with 70% of training data\n",
    "2. Identify K eigenvectors with 95% total energy\n",
    "3. Train logistic regression classifier\n",
    "4. Draw ROC curves for test dataset\n",
    "5. Repeat with Randomized PCA and compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use 70% of training data for PCA as per assignment requirement\n",
    "# This further splits the training set for PCA computation\n",
    "n_samples_pca = int(0.7 * X_train_flat.shape[0])\n",
    "X_pca_subset = X_train_flat[:n_samples_pca]\n",
    "\n",
    "print(f\"PCA training subset: {X_pca_subset.shape}\")\n",
    "\n",
    "# IMPORTANT: Normalize data AFTER splitting to prevent data leakage\n",
    "# StandardScaler: centers data to mean=0 and scales to variance=1\n",
    "# This is crucial for PCA as it's sensitive to feature scales\n",
    "# fit_transform on training data learns mean and std\n",
    "# transform on test data applies the same training statistics\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_flat)\n",
    "X_test_scaled = scaler.transform(X_test_flat)\n",
    "X_pca_subset_scaled = X_train_scaled[:n_samples_pca]\n",
    "\n",
    "print(f\"Data normalized: mean={X_train_scaled.mean():.4f}, std={X_train_scaled.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard PCA with 95% Energy Retention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard PCA with 95% variance (energy) retention\n",
    "# n_components=0.95 means \"select minimum K components that explain 95% variance\"\n",
    "# PCA finds orthogonal directions (principal components) that maximize variance\n",
    "print(\"Performing Standard PCA...\")\n",
    "pca_standard = PCA(n_components=0.95, random_state=42)\n",
    "\n",
    "# fit_transform: computes eigenvectors and projects training data\n",
    "X_train_pca = pca_standard.fit_transform(X_train_scaled)\n",
    "\n",
    "# transform: projects test data using already computed eigenvectors\n",
    "X_test_pca = pca_standard.transform(X_test_scaled)\n",
    "\n",
    "# K = number of components needed to retain 95% of total variance\n",
    "K = pca_standard.n_components_\n",
    "print(f\"\\nNumber of components (K) for 95% variance: {K}\")\n",
    "print(f\"Explained variance ratio: {pca_standard.explained_variance_ratio_.sum():.4f}\")\n",
    "print(f\"PCA transformed training shape: {X_train_pca.shape}\")\n",
    "print(f\"PCA transformed test shape: {X_test_pca.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot cumulative explained variance to visualize PCA performance\n",
    "# Cumulative variance shows how much information is retained as we add components\n",
    "cumulative_variance = np.cumsum(pca_standard.explained_variance_ratio_)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "# Plot cumulative variance curve\n",
    "plt.plot(range(1, len(cumulative_variance)+1), cumulative_variance, 'b-', linewidth=2)\n",
    "# Mark 95% threshold (our target)\n",
    "plt.axhline(y=0.95, color='r', linestyle='--', label='95% threshold')\n",
    "# Mark K components needed\n",
    "plt.axvline(x=K, color='g', linestyle='--', label=f'K={K} components')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('Standard PCA: Cumulative Explained Variance')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Logistic Regression Classifier on PCA Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train logistic regression classifier on PCA-reduced features\n",
    "# multi_class='multinomial': uses softmax for 10-class classification\n",
    "# solver='lbfgs': Limited-memory BFGS optimizer (good for small datasets)\n",
    "# n_jobs=-1: uses all CPU cores for parallel processing\n",
    "print(\"Training Logistic Regression on Standard PCA features...\")\n",
    "lr_pca = LogisticRegression(max_iter=1000, random_state=42, multi_class='multinomial',\n",
    "                            solver='lbfgs', n_jobs=-1)\n",
    "lr_pca.fit(X_train_pca, y_train)\n",
    "\n",
    "# Evaluate model performance\n",
    "train_acc_pca = lr_pca.score(X_train_pca, y_train)\n",
    "test_acc_pca = lr_pca.score(X_test_pca, y_test)\n",
    "\n",
    "print(f\"\\nStandard PCA + Logistic Regression Results:\")\n",
    "print(f\"Training Accuracy: {train_acc_pca:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc_pca:.4f}\")\n",
    "\n",
    "# Get predictions for further analysis (ROC curves, confusion matrix)\n",
    "y_pred_pca = lr_pca.predict(X_test_pca)  # Class predictions\n",
    "y_pred_proba_pca = lr_pca.predict_proba(X_test_pca)  # Probability scores for each class\n",
    "\n",
    "print(\"\\nClassification Report (Standard PCA):\")\n",
    "print(classification_report(y_test, y_pred_pca, target_names=class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC Curves for Standard PCA (One-vs-Rest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for multi-class ROC curves using One-vs-Rest strategy\n",
    "# Binarize labels: convert single label (0-9) to 10 binary columns\n",
    "# Example: label 3 becomes [0,0,0,1,0,0,0,0,0,0]\n",
    "y_test_bin = label_binarize(y_test, classes=np.arange(10))\n",
    "n_classes = y_test_bin.shape[1]\n",
    "\n",
    "# Compute ROC curve and AUC (Area Under Curve) for each class\n",
    "# ROC curve plots True Positive Rate vs False Positive Rate\n",
    "# AUC closer to 1.0 indicates better classification\n",
    "fpr_pca = dict()  # False Positive Rate\n",
    "tpr_pca = dict()  # True Positive Rate\n",
    "roc_auc_pca = dict()  # Area Under Curve\n",
    "\n",
    "for i in range(n_classes):\n",
    "    # For each class, treat it as binary: this class vs all others\n",
    "    fpr_pca[i], tpr_pca[i], _ = roc_curve(y_test_bin[:, i], y_pred_proba_pca[:, i])\n",
    "    roc_auc_pca[i] = auc(fpr_pca[i], tpr_pca[i])\n",
    "\n",
    "# Plot ROC curves for all 10 classes\n",
    "plt.figure(figsize=(12, 8))\n",
    "colors = plt.cm.tab10(np.linspace(0, 1, 10))\n",
    "\n",
    "for i, color in enumerate(colors):\n",
    "    plt.plot(fpr_pca[i], tpr_pca[i], color=color, lw=2,\n",
    "             label=f'{class_names[i]} (AUC = {roc_auc_pca[i]:.3f})')\n",
    "\n",
    "# Diagonal line represents random classifier (AUC = 0.5)\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2, label='Random Classifier')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize=12)\n",
    "plt.ylabel('True Positive Rate', fontsize=12)\n",
    "plt.title('ROC Curves - Standard PCA + Logistic Regression', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc='lower right', fontsize=9)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nMean AUC (Standard PCA): {np.mean(list(roc_auc_pca.values())):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomized PCA Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomized PCA: Faster approximation using randomized SVD\n",
    "# Uses random projections for faster computation on large datasets\n",
    "# Particularly useful when K << n_features\n",
    "# Should give similar results to standard PCA with much faster computation\n",
    "print(f\"Performing Randomized PCA with K={K} components...\")\n",
    "pca_randomized = PCA(n_components=K, svd_solver='randomized', random_state=42)\n",
    "X_train_pca_rand = pca_randomized.fit_transform(X_train_scaled)\n",
    "X_test_pca_rand = pca_randomized.transform(X_test_scaled)\n",
    "\n",
    "print(f\"\\nRandomized PCA Results:\")\n",
    "print(f\"Number of components: {pca_randomized.n_components_}\")\n",
    "print(f\"Explained variance ratio: {pca_randomized.explained_variance_ratio_.sum():.4f}\")\n",
    "print(f\"Transformed training shape: {X_train_pca_rand.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train logistic regression on Randomized PCA features\n",
    "# Using same hyperparameters as standard PCA for fair comparison\n",
    "print(\"Training Logistic Regression on Randomized PCA features...\")\n",
    "lr_pca_rand = LogisticRegression(max_iter=1000, random_state=42, multi_class='multinomial',\n",
    "                                 solver='lbfgs', n_jobs=-1)\n",
    "lr_pca_rand.fit(X_train_pca_rand, y_train)\n",
    "\n",
    "# Evaluate model performance\n",
    "train_acc_pca_rand = lr_pca_rand.score(X_train_pca_rand, y_train)\n",
    "test_acc_pca_rand = lr_pca_rand.score(X_test_pca_rand, y_test)\n",
    "\n",
    "print(f\"\\nRandomized PCA + Logistic Regression Results:\")\n",
    "print(f\"Training Accuracy: {train_acc_pca_rand:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc_pca_rand:.4f}\")\n",
    "\n",
    "# Generate predictions for ROC analysis\n",
    "y_pred_pca_rand = lr_pca_rand.predict(X_test_pca_rand)\n",
    "y_pred_proba_pca_rand = lr_pca_rand.predict_proba(X_test_pca_rand)\n",
    "\n",
    "print(\"\\nClassification Report (Randomized PCA):\")\n",
    "print(classification_report(y_test, y_pred_pca_rand, target_names=class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC Curves for Randomized PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ROC curves for Randomized PCA classifier\n",
    "# Same methodology as standard PCA for direct comparison\n",
    "fpr_pca_rand = dict()\n",
    "tpr_pca_rand = dict()\n",
    "roc_auc_pca_rand = dict()\n",
    "\n",
    "for i in range(n_classes):\n",
    "    # One-vs-Rest ROC for each class\n",
    "    fpr_pca_rand[i], tpr_pca_rand[i], _ = roc_curve(y_test_bin[:, i], y_pred_proba_pca_rand[:, i])\n",
    "    roc_auc_pca_rand[i] = auc(fpr_pca_rand[i], tpr_pca_rand[i])\n",
    "\n",
    "# Plot ROC curves for all classes\n",
    "plt.figure(figsize=(12, 8))\n",
    "colors = plt.cm.tab10(np.linspace(0, 1, 10))\n",
    "\n",
    "for i, color in enumerate(colors):\n",
    "    plt.plot(fpr_pca_rand[i], tpr_pca_rand[i], color=color, lw=2,\n",
    "             label=f'{class_names[i]} (AUC = {roc_auc_pca_rand[i]:.3f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2, label='Random Classifier')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize=12)\n",
    "plt.ylabel('True Positive Rate', fontsize=12)\n",
    "plt.title('ROC Curves - Randomized PCA + Logistic Regression', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc='lower right', fontsize=9)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nMean AUC (Randomized PCA): {np.mean(list(roc_auc_pca_rand.values())):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Direct ROC curve comparison: Standard PCA vs Randomized PCA\n",
    "# Compare the average performance across all classes\n",
    "\n",
    "# Calculate mean ROC curve for Standard PCA\n",
    "# Interpolate all class ROC curves to common FPR points for averaging\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "tprs_std = []\n",
    "for i in range(n_classes):\n",
    "    tprs_std.append(np.interp(mean_fpr, fpr_pca[i], tpr_pca[i]))\n",
    "    tprs_std[-1][0] = 0.0\n",
    "mean_tpr_std = np.mean(tprs_std, axis=0)\n",
    "mean_tpr_std[-1] = 1.0\n",
    "mean_auc_std = auc(mean_fpr, mean_tpr_std)\n",
    "\n",
    "# Calculate mean ROC curve for Randomized PCA\n",
    "tprs_rand = []\n",
    "for i in range(n_classes):\n",
    "    tprs_rand.append(np.interp(mean_fpr, fpr_pca_rand[i], tpr_pca_rand[i]))\n",
    "    tprs_rand[-1][0] = 0.0\n",
    "mean_tpr_rand = np.mean(tprs_rand, axis=0)\n",
    "mean_tpr_rand[-1] = 1.0\n",
    "mean_auc_rand = auc(mean_fpr, mean_tpr_rand)\n",
    "\n",
    "# Plot comparison\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.plot(mean_fpr, mean_tpr_std,\n",
    "         label=f'Standard PCA (AUC = {mean_auc_std:.4f})',\n",
    "         color='blue', linewidth=2.5)\n",
    "plt.plot(mean_fpr, mean_tpr_rand,\n",
    "         label=f'Randomized PCA (AUC = {mean_auc_rand:.4f})',\n",
    "         color='red', linewidth=2.5, linestyle='--')\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2, alpha=0.3, label='Random Classifier')\n",
    "\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize=12)\n",
    "plt.ylabel('True Positive Rate', fontsize=12)\n",
    "plt.title('ROC Curve Comparison: Standard vs Randomized PCA\\n(Average across all classes)',\n",
    "         fontsize=13, fontweight='bold')\n",
    "plt.legend(loc=\"lower right\", fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary\n",
    "print(\"\\nROC Comparison Summary:\")\n",
    "print(f\"Standard PCA Mean AUC:    {mean_auc_std:.4f}\")\n",
    "print(f\"Randomized PCA Mean AUC:  {mean_auc_rand:.4f}\")\n",
    "print(f\"Difference:               {abs(mean_auc_std - mean_auc_rand):.4f}\")\n",
    "print(\"\\nConclusion: Both methods achieve nearly identical performance.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison: Standard vs Randomized PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Standard vs Randomized PCA performance\n",
    "# Both methods should give very similar results\n",
    "# Randomized PCA is faster but uses approximation\n",
    "comparison_data = {\n",
    "    'Metric': ['Components (K)', 'Explained Variance', 'Training Accuracy', 'Test Accuracy', 'Mean AUC'],\n",
    "    'Standard PCA': [\n",
    "        K,\n",
    "        f\"{pca_standard.explained_variance_ratio_.sum():.4f}\",\n",
    "        f\"{train_acc_pca:.4f}\",\n",
    "        f\"{test_acc_pca:.4f}\",\n",
    "        f\"{np.mean(list(roc_auc_pca.values())):.4f}\"\n",
    "    ],\n",
    "    'Randomized PCA': [\n",
    "        K,\n",
    "        f\"{pca_randomized.explained_variance_ratio_.sum():.4f}\",\n",
    "        f\"{train_acc_pca_rand:.4f}\",\n",
    "        f\"{test_acc_pca_rand:.4f}\",\n",
    "        f\"{np.mean(list(roc_auc_pca_rand.values())):.4f}\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "import pandas as pd\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPARISON: Standard PCA vs Randomized PCA\")\n",
    "print(\"=\"*60)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Single-Layer Linear Autoencoder vs PCA [2 marks]\n",
    "\n",
    "### Objectives:\n",
    "1. Train single-layer autoencoder with K encoder nodes\n",
    "2. Linear activation function\n",
    "3. Mean and variance normalized input\n",
    "4. Constraint: decoder weights = encoder weights transpose\n",
    "5. Unit magnitude weight vectors\n",
    "6. Compare eigenvectors from PCA with autoencoder weights\n",
    "7. Display both as grayscale images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Model for Tied Weights Linear Autoencoder\n",
    "# This implementation satisfies all assignment requirements for Task 2\n",
    "\n",
    "class TiedLinearAutoencoder(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Linear autoencoder with tied weights and unit norm constraint.\n",
    "\n",
    "    Architecture:\n",
    "        Encoder:  h = X @ W  (where W has unit-norm columns)\n",
    "        Decoder:  X' = h @ W^T\n",
    "\n",
    "    Key Properties (Assignment Requirements):\n",
    "        1. Decoder weights = Encoder weights transpose (tied weights)\n",
    "        2. Each weight vector has unit magnitude (UnitNorm constraint)\n",
    "        3. Linear activation throughout (no non-linearity)\n",
    "        4. No bias terms (data is pre-centered with StandardScaler)\n",
    "        5. Theoretically equivalent to PCA for centered data\n",
    "\n",
    "    Parameters:\n",
    "        input_dim: Dimensionality of input (1024 for 32x32 grayscale images)\n",
    "        latent_dim: Dimensionality of latent space (K from PCA)\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        super(TiedLinearAutoencoder, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        # ENCODER WEIGHT MATRIX W: Shape (input_dim, latent_dim)\n",
    "        # This is the ONLY trainable parameter in the model\n",
    "        #\n",
    "        # Constraint: UnitNorm(axis=0) enforces unit magnitude per column\n",
    "        # - axis=0 means normalization along rows (each column becomes unit vector)\n",
    "        # - Each column W[:, i] represents one latent dimension's basis vector\n",
    "        # - After constraint: ||W[:, i]||_2 = 1 for all i\n",
    "        #\n",
    "        # This constraint makes the autoencoder theoretically equivalent to PCA:\n",
    "        # - PCA eigenvectors also have unit norm\n",
    "        # - Both minimize reconstruction error via linear projection\n",
    "        self.W = self.add_weight(\n",
    "            shape=(input_dim, latent_dim),\n",
    "            initializer=\"glorot_uniform\",  # Xavier uniform initialization\n",
    "            constraint=tf.keras.constraints.UnitNorm(axis=0),  # Unit norm per column\n",
    "            trainable=True,\n",
    "            name=\"encoder_weights\"\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        Forward pass through the autoencoder.\n",
    "\n",
    "        Args:\n",
    "            inputs: Input data, shape (batch_size, input_dim)\n",
    "                    Assumed to be centered (mean=0) via StandardScaler\n",
    "\n",
    "        Returns:\n",
    "            reconstruction: Reconstructed data, shape (batch_size, input_dim)\n",
    "\n",
    "        Mathematics:\n",
    "            Encoder:  h = X @ W\n",
    "                      Maps input to latent space (1024 -> K dimensions)\n",
    "\n",
    "            Decoder:  X' = h @ W^T = (X @ W) @ W^T\n",
    "                      Reconstructs input from latent representation\n",
    "                      Uses TRANSPOSED encoder weights (tied weights)\n",
    "        \"\"\"\n",
    "        # ENCODER: Project input to latent space\n",
    "        # Linear transformation without bias (data is centered)\n",
    "        # Shape: (batch_size, input_dim) @ (input_dim, latent_dim) -> (batch_size, latent_dim)\n",
    "        hidden = tf.matmul(inputs, self.W)\n",
    "\n",
    "        # DECODER: Reconstruct input from latent representation\n",
    "        # Uses transposed encoder weights (tied weights constraint)\n",
    "        # Shape: (batch_size, latent_dim) @ (latent_dim, input_dim) -> (batch_size, input_dim)\n",
    "        reconstruction = tf.matmul(hidden, tf.transpose(self.W))\n",
    "\n",
    "        return reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and compile the tied weights linear autoencoder\n",
    "input_dim = X_train_flat.shape[1]  # 32*32 = 1024 pixels per grayscale image\n",
    "encoding_dim = K  # Use K from PCA (number of components for 95% variance retention)\n",
    "\n",
    "print(f\"Building Linear Autoencoder with {encoding_dim} latent dimensions...\")\n",
    "\n",
    "# Instantiate the custom autoencoder model\n",
    "# This creates the weight matrix W with shape (1024, K)\n",
    "autoencoder_linear = TiedLinearAutoencoder(input_dim, encoding_dim)\n",
    "\n",
    "# Compile the model with appropriate loss and metrics\n",
    "# Loss: MSE (Mean Squared Error) - measures reconstruction quality\n",
    "#   - Minimizing MSE is equivalent to maximizing explained variance\n",
    "#   - This is the SAME objective as PCA (hence theoretical equivalence)\n",
    "# Metrics: MAE (Mean Absolute Error) - alternative reconstruction metric\n",
    "autoencoder_linear.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "# Build the model by calling it once (Keras requirement for model summary)\n",
    "# This initializes all weights and establishes the computation graph\n",
    "_ = autoencoder_linear(X_train_scaled[:1])\n",
    "\n",
    "# Display model information\n",
    "print(\"\\nModel Architecture:\")\n",
    "print(f\"  Input dimension:  {input_dim}\")\n",
    "print(f\"  Latent dimension: {encoding_dim}\")\n",
    "print(f\"  Encoder weights:  {input_dim} x {encoding_dim} (unit norm columns)\")\n",
    "print(f\"  Decoder weights:  {encoding_dim} x {input_dim} (transpose of encoder)\")\n",
    "print(f\"  Biases:           None (data is centered via StandardScaler)\")\n",
    "print(f\"  Total parameters: {sum([tf.size(w).numpy() for w in autoencoder_linear.trainable_weights]):,}\")\n",
    "print(f\"\\nNote: No bias terms are needed because:\")\n",
    "print(f\"  1. Input data is standardized (mean=0, std=1)\")\n",
    "print(f\"  2. This ensures theoretical equivalence with PCA\")\n",
    "print(f\"  3. PCA also assumes centered data with no bias terms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the linear autoencoder\n",
    "# Training on normalized data (same preprocessing as PCA)\n",
    "# Objective: Minimize reconstruction error (MSE)\n",
    "print(\"Training Linear Autoencoder...\")\n",
    "\n",
    "history_linear = autoencoder_linear.fit(\n",
    "    X_train_scaled, X_train_scaled,  # Input = target for autoencoder\n",
    "    epochs=50,  # May need more epochs than CNN due to constraints\n",
    "    batch_size=256,\n",
    "    shuffle=True,  # Randomize training order\n",
    "    validation_data=(X_test_scaled, X_test_scaled),\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nTraining completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history to monitor convergence\n",
    "# Loss should decrease smoothly if training is stable\n",
    "# Validation loss close to training loss indicates no overfitting\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "# MSE Loss over epochs\n",
    "axes[0].plot(history_linear.history['loss'], label='Training Loss')\n",
    "axes[0].plot(history_linear.history['val_loss'], label='Validation Loss')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('MSE Loss')\n",
    "axes[0].set_title('Linear Autoencoder - Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# MAE over epochs (alternative metric)\n",
    "axes[1].plot(history_linear.history['mae'], label='Training MAE')\n",
    "axes[1].plot(history_linear.history['val_mae'], label='Validation MAE')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('MAE')\n",
    "axes[1].set_title('Linear Autoencoder - MAE')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract and Compare Eigenvectors (PCA) vs Weight Vectors (Autoencoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract eigenvectors and weight matrices for comparison\n",
    "# PCA components: directions of maximum variance\n",
    "# Autoencoder weights: learned feature detectors\n",
    "pca_components = pca_standard.components_  # Shape: (K, 1024)\n",
    "\n",
    "# Extract autoencoder weight matrix W from the model\n",
    "ae_weights = autoencoder_linear.W.numpy()  # Shape: (1024, K)\n",
    "ae_components = ae_weights.T  # Transpose to match PCA format: (K, 1024)\n",
    "\n",
    "print(f\"PCA components shape: {pca_components.shape}\")\n",
    "print(f\"Autoencoder weight matrix shape: {ae_components.shape}\")\n",
    "\n",
    "# Both can be reshaped to 32x32 for visualization\n",
    "img_shape = (32, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize PCA eigenvectors as grayscale images\n",
    "# Each eigenvector captures a pattern of variance in the data\n",
    "n_display = min(16, K)\n",
    "fig, axes = plt.subplots(4, 4, figsize=(12, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i in range(n_display):\n",
    "    # PCA components are stored as rows, reshape to 32x32 image\n",
    "    eigenvector = pca_components[i].reshape(img_shape)\n",
    "    axes[i].imshow(eigenvector, cmap='gray')\n",
    "    axes[i].set_title(f'PCA {i+1}', fontsize=10)\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle('PCA Eigenvectors (Top 16)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize autoencoder weight vectors as grayscale images\n",
    "# These should be similar to PCA eigenvectors (theoretical equivalence)\n",
    "fig, axes = plt.subplots(4, 4, figsize=(12, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i in range(n_display):\n",
    "    # Autoencoder weights transposed to match PCA format, reshape to 32x32\n",
    "    weight_vector = ae_components[i].reshape(img_shape)\n",
    "    axes[i].imshow(weight_vector, cmap='gray')\n",
    "    axes[i].set_title(f'AE {i+1}', fontsize=10)\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle('Autoencoder Weight Vectors (Top 16)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Side-by-side comparison: PCA vs Autoencoder\n",
    "# Direct visual comparison to see structural similarities\n",
    "n_compare = min(10, K)  # Show 10 pairs for clearer comparison\n",
    "\n",
    "fig, axes = plt.subplots(2, n_compare, figsize=(20, 4))\n",
    "\n",
    "# Top row: PCA eigenvectors\n",
    "for i in range(n_compare):\n",
    "    axes[0, i].imshow(pca_components[i].reshape(img_shape), cmap='gray')\n",
    "    axes[0, i].set_title(f'PCA {i+1}', fontsize=9)\n",
    "    axes[0, i].axis('off')\n",
    "\n",
    "# Bottom row: Autoencoder weights\n",
    "for i in range(n_compare):\n",
    "    axes[1, i].imshow(ae_components[i].reshape(img_shape), cmap='gray')\n",
    "    axes[1, i].set_title(f'AE {i+1}', fontsize=9)\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "# Add row labels on the left\n",
    "fig.text(0.02, 0.75, 'PCA', fontsize=12, fontweight='bold', va='center', rotation=90)\n",
    "fig.text(0.02, 0.25, 'Autoencoder', fontsize=12, fontweight='bold', va='center', rotation=90)\n",
    "\n",
    "plt.suptitle('Comparison: PCA Eigenvectors vs Autoencoder Weights',\n",
    "             fontsize=14, fontweight='bold')\n",
    "plt.tight_layout(rect=[0.03, 0, 1, 0.96])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commentary on PCA vs Autoencoder Comparison\n",
    "\n",
    "**Observations:**\n",
    "\n",
    "1. **Structural Similarity**: Both PCA eigenvectors and autoencoder weight vectors capture similar low-frequency patterns and principal directions in the data. This is expected since both methods perform linear dimensionality reduction optimizing for reconstruction.\n",
    "\n",
    "2. **Theoretical Equivalence**: For a linear autoencoder with MSE loss and tied weights (decoder = encoder transpose), the learned weight vectors span the same subspace as the top K PCA eigenvectors. However, they may differ in:\n",
    "   - **Ordering**: PCA components are sorted by explained variance, while autoencoder weights have no inherent order\n",
    "   - **Sign**: Eigenvectors can be multiplied by -1 without changing the solution\n",
    "   - **Orthogonality**: PCA guarantees orthogonal components, autoencoder does not\n",
    "\n",
    "3. **Visual Patterns**: \n",
    "   - Early components capture global structure (edges, orientations)\n",
    "   - Later components capture finer details and noise\n",
    "   - Both show grayscale patterns representing feature detectors learned from data\n",
    "\n",
    "4. **Practical Differences**:\n",
    "   - PCA is deterministic and finds global optimum\n",
    "   - Autoencoder optimization is iterative (gradient descent) and may converge to local optima\n",
    "   - PCA requires computing full covariance matrix (memory intensive for large data)\n",
    "   - Autoencoders can be trained with mini-batch SGD (scalable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Deep Convolutional Autoencoders [4 marks]\n",
    "\n",
    "### Objectives:\n",
    "1. Design deep convolutional autoencoder with latent dimension \u2248 K\n",
    "2. Calculate reconstruction error on test set\n",
    "3. Compare with single hidden layer K-node autoencoder (sigmoid)\n",
    "4. Build 3-hidden-layer variant with K nodes distributed equally\n",
    "5. Compare reconstruction errors across all variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for convolutional autoencoders\n",
    "# CNNs expect 4D input: (batch_size, height, width, channels)\n",
    "# Normalize to [0, 1] range for sigmoid output activation compatibility\n",
    "X_train_cnn = X_train.reshape(-1, 32, 32, 1) / 255.0  # Add channel dimension\n",
    "X_test_cnn = X_test.reshape(-1, 32, 32, 1) / 255.0\n",
    "\n",
    "print(f\"CNN input shapes:\")\n",
    "print(f\"Training: {X_train_cnn.shape}\")\n",
    "print(f\"Test: {X_test_cnn.shape}\")\n",
    "print(f\"Value range: [{X_train_cnn.min():.3f}, {X_train_cnn.max():.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: Deep Convolutional Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Convolutional Autoencoder architecture\n",
    "# Uses convolutional layers to learn hierarchical spatial features\n",
    "def build_deep_conv_autoencoder(latent_dim):\n",
    "    \"\"\"\n",
    "    Architecture Overview:\n",
    "        ENCODER:  32x32x1 -> 16x16x32 -> 8x8x64 -> 4x4x128 -> 2048 -> K\n",
    "        DECODER:  K -> 2048 -> 4x4x128 -> 8x8x128 -> 16x16x64 -> 32x32x32 -> 32x32x1\n",
    "    \"\"\"\n",
    "\n",
    "    # ========================================================================\n",
    "    # ENCODER: Progressive downsampling (shrinking spatial dimensions)\n",
    "    # ========================================================================\n",
    "    encoder_input = layers.Input(shape=(32, 32, 1))\n",
    "\n",
    "    # --- LAYER 1: Extract low-level features (edges, textures) ---\n",
    "    # Conv2D: Applies 32 filters of size 3x3 to detect patterns\n",
    "    #   - filters=32: learns 32 different feature detectors\n",
    "    #   - kernel_size=(3,3): each filter looks at 3x3 pixel neighborhoods\n",
    "    #   - activation='relu': introduces non-linearity, helps learn complex patterns\n",
    "    #   - padding='same': maintains spatial dimensions (32x32 stays 32x32)\n",
    "    x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(encoder_input)\n",
    "\n",
    "    # MaxPooling2D: Reduces spatial dimensions by taking maximum value in 2x2 windows\n",
    "    #   - pool_size=(2,2): divides height and width by 2\n",
    "    #   - padding='same': handles odd dimensions gracefully\n",
    "    #   - Result: 32x32x32 -> 16x16x32 (spatial shrinking begins)\n",
    "    x = layers.MaxPooling2D((2, 2), padding='same')(x)  # 32x32 -> 16x16\n",
    "\n",
    "    # --- LAYER 2: Extract mid-level features (shapes, patterns) ---\n",
    "    # More filters (64) to capture increased complexity at lower resolution\n",
    "    x = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = layers.MaxPooling2D((2, 2), padding='same')(x)  # 16x16 -> 8x8\n",
    "\n",
    "    # --- LAYER 3: Extract high-level features (object parts) ---\n",
    "    # Even more filters (128) for abstract representations\n",
    "    x = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = layers.MaxPooling2D((2, 2), padding='same')(x)  # 8x8 -> 4x4\n",
    "\n",
    "    # ========================================================================\n",
    "    # LATENT SPACE: Bottleneck representation (maximum compression)\n",
    "    # ========================================================================\n",
    "    # Flatten: Converts 3D spatial data to 1D vector\n",
    "    #   - Input: 4x4x128 spatial feature maps\n",
    "    #   - Output: 2048-dimensional vector (4 * 4 * 128 = 2048)\n",
    "    #   - This destroys spatial structure but preserves all information\n",
    "    x = layers.Flatten()(x)  # 4x4x128 = 2048 features\n",
    "\n",
    "    # Dense: Compresses to K-dimensional latent representation\n",
    "    #   - 2048 -> latent_dim (K): Maximum information compression\n",
    "    #   - This K-dim vector encodes the entire 32x32 image\n",
    "    #   - ReLU activation maintains non-negative activations\n",
    "    latent = layers.Dense(latent_dim, activation='relu', name='latent')(x)\n",
    "\n",
    "    # ========================================================================\n",
    "    # DECODER: Progressive upsampling (expanding to reconstruct input)\n",
    "    # ========================================================================\n",
    "    # Dense: Expand from K dimensions back to spatial representation size\n",
    "    #   - latent_dim (K) -> 2048: First expansion step\n",
    "    #   - Prepares to reshape back into 4x4x128 spatial format\n",
    "    x = layers.Dense(4 * 4 * 128, activation='relu')(latent)\n",
    "\n",
    "    # Reshape: Convert 1D vector back to 3D spatial format\n",
    "    #   - 2048 -> 4x4x128: Restores spatial structure\n",
    "    #   - Starting point for spatial upsampling\n",
    "    x = layers.Reshape((4, 4, 128))(x)  # Reshape back to spatial format\n",
    "\n",
    "    # --- Upsampling path (mirror of encoder, expands spatial dimensions) ---\n",
    "\n",
    "    # Conv2DTranspose: Learns to upsample while refining features\n",
    "    #   - Also called \"deconvolution\" or \"transposed convolution\"\n",
    "    #   - strides=2: doubles spatial dimensions (opposite of pooling)\n",
    "    #   - Learns HOW to upsample (unlike simple interpolation)\n",
    "    #   - padding='same': ensures output size is exactly stride * input_size\n",
    "\n",
    "    # Layer 1 decoder: 4x4x128 -> 8x8x128\n",
    "    #   - 128 filters: maintains feature richness while upsampling\n",
    "    #   - strides=2: 4x4 -> 8x8 (doubles height and width)\n",
    "    x = layers.Conv2DTranspose(128, (3, 3), strides=2, activation='relu', padding='same')(x)\n",
    "\n",
    "    # Layer 2 decoder: 8x8x128 -> 16x16x64\n",
    "    #   - 64 filters: reduces feature count as we approach output\n",
    "    #   - strides=2: 8x8 -> 16x16 (doubles again)\n",
    "    x = layers.Conv2DTranspose(64, (3, 3), strides=2, activation='relu', padding='same')(x)\n",
    "\n",
    "    # Layer 3 decoder: 16x16x64 -> 32x32x32\n",
    "    #   - 32 filters: further reduces feature count\n",
    "    #   - strides=2: 16x16 -> 32x32 (final spatial expansion)\n",
    "    x = layers.Conv2DTranspose(32, (3, 3), strides=2, activation='relu', padding='same')(x)\n",
    "\n",
    "    # ========================================================================\n",
    "    # OUTPUT LAYER: Reconstruct single-channel grayscale image\n",
    "    # ========================================================================\n",
    "    # Final Conv2D: 32x32x32 -> 32x32x1\n",
    "    #   - filters=1: reduces to single channel (grayscale)\n",
    "    #   - activation='sigmoid': outputs values in [0, 1] range\n",
    "    #   - padding='same': maintains 32x32 spatial dimensions\n",
    "    #   - Sigmoid ensures output matches normalized input range\n",
    "    decoder_output = layers.Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "    autoencoder = models.Model(encoder_input, decoder_output, name='deep_conv_autoencoder')\n",
    "    return autoencoder\n",
    "\n",
    "# Build model with latent dimension = K (from PCA)\n",
    "conv_ae = build_deep_conv_autoencoder(latent_dim=K)\n",
    "conv_ae.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "print(f\"\\nDeep Convolutional Autoencoder (Latent Dim = {K})\")\n",
    "conv_ae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train deep convolutional autoencoder\n",
    "# This should learn hierarchical spatial features\n",
    "# Expected to outperform simple linear/fully-connected autoencoders\n",
    "print(\"Training Deep Convolutional Autoencoder...\")\n",
    "\n",
    "history_conv = conv_ae.fit(\n",
    "    X_train_cnn, X_train_cnn,  # Autoencoder: input = target\n",
    "    epochs=30,  # CNNs typically need fewer epochs due to strong inductive bias\n",
    "    batch_size=128,  # Smaller batch for better gradient estimates\n",
    "    shuffle=True,\n",
    "    validation_data=(X_test_cnn, X_test_cnn),\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nTraining completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate reconstruction error on test set\n",
    "# Lower MSE indicates better reconstruction quality\n",
    "# Use test set for fair comparison (unseen data)\n",
    "X_test_reconstructed_conv = conv_ae.predict(X_test_cnn)\n",
    "reconstruction_error_conv = np.mean((X_test_cnn - X_test_reconstructed_conv) ** 2)\n",
    "\n",
    "print(f\"\\nDeep Convolutional Autoencoder:\")\n",
    "print(f\"Test Reconstruction Error (MSE): {reconstruction_error_conv:.6f}\")\n",
    "print(f\"Test Reconstruction Error (RMSE): {np.sqrt(reconstruction_error_conv):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: Single Hidden Layer Autoencoder (Sigmoid Activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single hidden layer autoencoder with sigmoid activation\n",
    "# Simpler architecture than convolutional - uses fully connected layers\n",
    "# Sigmoid activation introduces non-linearity (unlike linear autoencoder in Task 2)\n",
    "def build_single_layer_ae(latent_dim):\n",
    "    \"\"\"\n",
    "    Architecture Overview:\n",
    "        ENCODER:  32x32x1 -> [Flatten] -> 1024 -> K (sigmoid)\n",
    "        DECODER:  K -> 1024 (linear) -> [Reshape] -> 32x32x1\n",
    "\n",
    "    Key difference from Conv AE:\n",
    "        - No spatial structure preservation (flatten immediately)\n",
    "        - Single compression step (1024 -> K)\n",
    "        - Fully connected layers (every input connects to every output)\n",
    "    \"\"\"\n",
    "\n",
    "    # ========================================================================\n",
    "    # ENCODER: Single-step compression with sigmoid non-linearity\n",
    "    # ========================================================================\n",
    "    encoder_input = layers.Input(shape=(32, 32, 1))\n",
    "\n",
    "    # Flatten: Convert 2D image to 1D vector\n",
    "    #   - Input: 32x32x1 spatial image\n",
    "    #   - Output: 1024-dimensional vector (32 * 32 * 1 = 1024)\n",
    "    #   - Destroys spatial structure immediately (unlike Conv AE)\n",
    "    #   - Every pixel becomes an independent feature\n",
    "    x = layers.Flatten()(encoder_input)  # 32x32x1 -> 1024\n",
    "\n",
    "    # Dense layer with sigmoid: Non-linear compression to latent space\n",
    "    #   - Input: 1024 features\n",
    "    #   - Output: latent_dim (K) features\n",
    "    #   - activation='sigmoid': squashes values to [0, 1] range\n",
    "    #   - Creates non-linear bottleneck (unlike linear autoencoder in Task 2)\n",
    "    #   - Each of K neurons learns a weighted combination of all 1024 pixels\n",
    "    #   - Parameters: 1024 * K weights + K biases\n",
    "    latent = layers.Dense(latent_dim, activation='sigmoid', name='latent')(x)\n",
    "\n",
    "    # ========================================================================\n",
    "    # DECODER: Single-step expansion with linear activation\n",
    "    # ========================================================================\n",
    "    # Dense layer with linear activation: Reconstruct pixel values\n",
    "    #   - Input: latent_dim (K) features\n",
    "    #   - Output: 1024 pixel values\n",
    "    #   - activation='linear': no constraint on output range\n",
    "    #   - Allows reconstruction of full pixel intensity range\n",
    "    #   - Parameters: K * 1024 weights + 1024 biases\n",
    "    x = layers.Dense(32 * 32, activation='linear')(latent)\n",
    "\n",
    "    # Reshape: Convert 1D vector back to 2D image\n",
    "    #   - Input: 1024-dimensional vector\n",
    "    #   - Output: 32x32x1 spatial image\n",
    "    #   - Restores spatial structure for visualization and comparison\n",
    "    decoder_output = layers.Reshape((32, 32, 1))(x)\n",
    "\n",
    "    autoencoder = models.Model(encoder_input, decoder_output, name='single_layer_ae')\n",
    "    return autoencoder\n",
    "\n",
    "single_ae = build_single_layer_ae(latent_dim=K)\n",
    "single_ae.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "print(f\"\\nSingle Hidden Layer Autoencoder (K={K} nodes, Sigmoid)\")\n",
    "single_ae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train single layer autoencoder\n",
    "# Compare with conv autoencoder to see benefit of spatial structure\n",
    "print(\"Training Single Hidden Layer Autoencoder...\")\n",
    "\n",
    "history_single = single_ae.fit(\n",
    "    X_train_cnn, X_train_cnn,\n",
    "    epochs=30,  # Same as conv AE for fair comparison\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    "    validation_data=(X_test_cnn, X_test_cnn),\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nTraining completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate reconstruction error for single layer autoencoder\n",
    "# Expected to be worse than conv AE but better than linear AE\n",
    "X_test_reconstructed_single = single_ae.predict(X_test_cnn)\n",
    "reconstruction_error_single = np.mean((X_test_cnn - X_test_reconstructed_single) ** 2)\n",
    "\n",
    "print(f\"\\nSingle Hidden Layer Autoencoder:\")\n",
    "print(f\"Test Reconstruction Error (MSE): {reconstruction_error_single:.6f}\")\n",
    "print(f\"Test Reconstruction Error (RMSE): {np.sqrt(reconstruction_error_single):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3: Three Hidden Layer Autoencoder (K nodes distributed equally)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Three hidden layer autoencoder with K nodes distributed equally\n",
    "# Tests if distributing capacity across layers improves performance\n",
    "# Architecture: gradually compress then expand (hourglass/symmetric shape)\n",
    "def build_three_layer_ae(total_nodes):\n",
    "    \"\"\"\n",
    "    Architecture Overview:\n",
    "        ENCODER:  32x32x1 -> [Flatten] -> 1024 -> K*3 -> K*2 -> K (sigmoid at each layer)\n",
    "        DECODER:  K -> K*2 -> K*3 -> 1024 (linear) -> [Reshape] -> 32x32x1\n",
    "\n",
    "    Key difference from Single Layer AE:\n",
    "        - Gradual compression (3 steps vs 1 step)\n",
    "        - Hierarchical feature learning (like Conv AE but without spatial structure)\n",
    "        - Total capacity distributed across 3 bottleneck layers\n",
    "    \"\"\"\n",
    "    nodes_per_layer = total_nodes // 3  # Distribute K nodes equally\n",
    "\n",
    "    # ========================================================================\n",
    "    # ENCODER: Multi-step compression (gradual bottleneck)\n",
    "    # ========================================================================\n",
    "    encoder_input = layers.Input(shape=(32, 32, 1))\n",
    "\n",
    "    # Flatten: Convert 2D image to 1D vector\n",
    "    #   - Input: 32x32x1 spatial image\n",
    "    #   - Output: 1024-dimensional vector\n",
    "    x = layers.Flatten()(encoder_input)  # 32x32x1 -> 1024\n",
    "\n",
    "    # Layer 1: First compression step (1024 -> K*3)\n",
    "    #   - Mild compression: reduces to ~3x the final bottleneck size\n",
    "    #   - activation='sigmoid': non-linear transformation [0, 1]\n",
    "    #   - Learns first level of abstract features\n",
    "    #   - Example: If K=160, this layer has 480 nodes (160*3)\n",
    "    x = layers.Dense(nodes_per_layer * 3, activation='sigmoid')(x)  # First compression\n",
    "\n",
    "    # Layer 2: Second compression step (K*3 -> K*2)\n",
    "    #   - Further compression: reduces to ~2x the final bottleneck size\n",
    "    #   - activation='sigmoid': continues non-linear feature extraction\n",
    "    #   - Learns second level of more abstract features\n",
    "    #   - Example: If K=160, this layer has 320 nodes (160*2)\n",
    "    x = layers.Dense(nodes_per_layer * 2, activation='sigmoid')(x)  # Further compress\n",
    "\n",
    "    # Layer 3: Final compression step (K*2 -> K)\n",
    "    #   - Maximum compression: reaches the bottleneck\n",
    "    #   - activation='sigmoid': final non-linear encoding\n",
    "    #   - Creates the K-dimensional latent representation\n",
    "    #   - Example: If K=160, this layer has 160 nodes (160*1)\n",
    "    latent = layers.Dense(nodes_per_layer, activation='sigmoid', name='latent')(x)  # Bottleneck\n",
    "\n",
    "    # ========================================================================\n",
    "    # DECODER: Multi-step expansion (gradual reconstruction)\n",
    "    # ========================================================================\n",
    "    # Mirror structure of encoder (symmetric hourglass shape)\n",
    "\n",
    "    # Layer 1: First expansion step (K -> K*2)\n",
    "    #   - Begin reconstruction: doubles the representation size\n",
    "    #   - activation='sigmoid': maintains non-linearity\n",
    "    #   - Starts expanding abstract features back toward pixel space\n",
    "    x = layers.Dense(nodes_per_layer * 2, activation='sigmoid')(latent)\n",
    "\n",
    "    # Layer 2: Second expansion step (K*2 -> K*3)\n",
    "    #   - Continue expansion: 1.5x increase in size\n",
    "    #   - activation='sigmoid': continues feature refinement\n",
    "    #   - Further expands toward full pixel representation\n",
    "    x = layers.Dense(nodes_per_layer * 3, activation='sigmoid')(x)\n",
    "\n",
    "    # Layer 3: Final expansion step (K*3 -> 1024)\n",
    "    #   - Full expansion: back to original 1024 pixel values\n",
    "    #   - activation='linear': no constraint on output range (allows full reconstruction)\n",
    "    #   - Directly outputs pixel intensities\n",
    "    x = layers.Dense(32 * 32, activation='linear')(x)  # Linear at final layer\n",
    "\n",
    "    # Reshape: Convert 1D vector back to 2D image\n",
    "    #   - Input: 1024-dimensional vector\n",
    "    #   - Output: 32x32x1 spatial image\n",
    "    decoder_output = layers.Reshape((32, 32, 1))(x)\n",
    "\n",
    "    autoencoder = models.Model(encoder_input, decoder_output, name='three_layer_ae')\n",
    "    return autoencoder, nodes_per_layer\n",
    "\n",
    "three_layer_ae, nodes_per_layer = build_three_layer_ae(total_nodes=K)\n",
    "three_layer_ae.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "print(f\"\\nThree Hidden Layer Autoencoder (\u2248{nodes_per_layer} nodes per layer)\")\n",
    "print(f\"Architecture: 1024 -> {nodes_per_layer*3} -> {nodes_per_layer*2} -> {nodes_per_layer} -> {nodes_per_layer*2} -> {nodes_per_layer*3} -> 1024\")\n",
    "three_layer_ae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train three layer autoencoder\n",
    "# Deeper network may learn better hierarchical features\n",
    "print(\"Training Three Hidden Layer Autoencoder...\")\n",
    "\n",
    "history_three = three_layer_ae.fit(\n",
    "    X_train_cnn, X_train_cnn,\n",
    "    epochs=30,\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    "    validation_data=(X_test_cnn, X_test_cnn),\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nTraining completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate reconstruction error for three layer autoencoder\n",
    "# Compare: Does distributing nodes help vs single layer?\n",
    "X_test_reconstructed_three = three_layer_ae.predict(X_test_cnn)\n",
    "reconstruction_error_three = np.mean((X_test_cnn - X_test_reconstructed_three) ** 2)\n",
    "\n",
    "print(f\"\\nThree Hidden Layer Autoencoder:\")\n",
    "print(f\"Test Reconstruction Error (MSE): {reconstruction_error_three:.6f}\")\n",
    "print(f\"Test Reconstruction Error (RMSE): {np.sqrt(reconstruction_error_three):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Reconstructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize original vs reconstructed images for all 3 autoencoders\n",
    "# This allows qualitative comparison of reconstruction quality\n",
    "# Look for: sharpness, color accuracy, preservation of details\n",
    "n_samples = 5\n",
    "indices = np.random.choice(len(X_test_cnn), n_samples, replace=False)\n",
    "\n",
    "fig, axes = plt.subplots(4, n_samples, figsize=(15, 10))\n",
    "\n",
    "for i, idx in enumerate(indices):\n",
    "    # Original image (ground truth)\n",
    "    axes[0, i].imshow(X_test_cnn[idx].squeeze(), cmap='gray')\n",
    "    axes[0, i].set_title('Original')\n",
    "    axes[0, i].axis('off')\n",
    "\n",
    "    # Deep Conv AE reconstruction\n",
    "    axes[1, i].imshow(X_test_reconstructed_conv[idx].squeeze(), cmap='gray')\n",
    "    axes[1, i].set_title('Deep Conv AE')\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "    # Single Layer AE reconstruction\n",
    "    axes[2, i].imshow(X_test_reconstructed_single[idx].squeeze(), cmap='gray')\n",
    "    axes[2, i].set_title('Single Layer AE')\n",
    "    axes[2, i].axis('off')\n",
    "\n",
    "    # Three Layer AE reconstruction\n",
    "    axes[3, i].imshow(X_test_reconstructed_three[idx].squeeze(), cmap='gray')\n",
    "    axes[3, i].set_title('3-Layer AE')\n",
    "    axes[3, i].axis('off')\n",
    "\n",
    "plt.suptitle('Reconstruction Comparison', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of Reconstruction Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantitative comparison of all three autoencoders\n",
    "# Lower MSE/RMSE indicates better reconstruction\n",
    "# Deep conv AE should have lowest error due to spatial inductive bias\n",
    "comparison_ae = {\n",
    "    'Model': [\n",
    "        'Deep Convolutional AE',\n",
    "        'Single Layer AE (Sigmoid)',\n",
    "        '3-Layer AE (Sigmoid)'\n",
    "    ],\n",
    "    'Architecture': [\n",
    "        f'Conv layers \u2192 {K} latent',\n",
    "        f'{K} nodes, Sigmoid',\n",
    "        f'{nodes_per_layer}\u2192{nodes_per_layer*2}\u2192{nodes_per_layer*3} nodes'\n",
    "    ],\n",
    "    'MSE': [\n",
    "        f\"{reconstruction_error_conv:.6f}\",\n",
    "        f\"{reconstruction_error_single:.6f}\",\n",
    "        f\"{reconstruction_error_three:.6f}\"\n",
    "    ],\n",
    "    'RMSE': [\n",
    "        f\"{np.sqrt(reconstruction_error_conv):.6f}\",\n",
    "        f\"{np.sqrt(reconstruction_error_single):.6f}\",\n",
    "        f\"{np.sqrt(reconstruction_error_three):.6f}\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "comparison_ae_df = pd.DataFrame(comparison_ae)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPARISON: Autoencoder Reconstruction Errors (Test Set)\")\n",
    "print(\"=\"*80)\n",
    "print(comparison_ae_df.to_string(index=False))\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar plot for visual comparison of reconstruction errors\n",
    "# Makes it easy to see relative performance at a glance\n",
    "models = ['Deep Conv AE', 'Single Layer AE', '3-Layer AE']\n",
    "mse_values = [reconstruction_error_conv, reconstruction_error_single, reconstruction_error_three]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(models, mse_values, color=['#2E86AB', '#A23B72', '#F18F01'])\n",
    "plt.ylabel('Mean Squared Error (MSE)', fontsize=12)\n",
    "plt.title('Reconstruction Error Comparison (Test Set)', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels on top of bars for exact comparison\n",
    "for bar, val in zip(bars, mse_values):\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{val:.6f}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4: MNIST 7-Segment LED Display Classifier [3 marks]\n",
    "\n",
    "### Objectives:\n",
    "1. Train deep convolutional autoencoder on MNIST\n",
    "2. Extract latent features\n",
    "3. Create 7-segment LED mapping for digits (7 outputs)\n",
    "4. Train MLP classifier on extracted features\n",
    "5. Generate confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST dataset - handwritten digits 0-9\n",
    "# MNIST: 28x28 grayscale images, 60k training + 10k test\n",
    "(X_train_mnist, y_train_mnist), (X_test_mnist, y_test_mnist) = mnist.load_data()\n",
    "\n",
    "# Normalize to [0, 1] range for neural network training\n",
    "# Add channel dimension for CNN compatibility\n",
    "X_train_mnist = X_train_mnist.astype('float32') / 255.0\n",
    "X_test_mnist = X_test_mnist.astype('float32') / 255.0\n",
    "X_train_mnist = X_train_mnist.reshape(-1, 28, 28, 1)\n",
    "X_test_mnist = X_test_mnist.reshape(-1, 28, 28, 1)\n",
    "\n",
    "print(f\"MNIST Dataset:\")\n",
    "print(f\"Training: {X_train_mnist.shape}, Labels: {y_train_mnist.shape}\")\n",
    "print(f\"Test: {X_test_mnist.shape}, Labels: {y_test_mnist.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample MNIST images to understand the data\n",
    "# Display first 10 images (one of each digit ideally)\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i in range(10):\n",
    "    axes[i].imshow(X_train_mnist[i].squeeze(), cmap='gray')\n",
    "    axes[i].set_title(f\"Digit: {y_train_mnist[i]}\")\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle('Sample MNIST Digits')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and Train Deep Convolutional Autoencoder for MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Convolutional Autoencoder for MNIST (28x28 images)",
    "# Similar architecture to CIFAR10 but adapted for smaller images",
    "# Encoder and decoder are separated for easier feature extraction later",
    "def build_mnist_conv_autoencoder(latent_dim=64):",
    "    \"\"\"",
    "    Architecture Overview:",
    "        ENCODER:  28x28x1 -> 14x14x32 -> 7x7x64 -> 3136 -> 64",
    "        DECODER:  64 -> 3136 -> 7x7x64 -> 14x14x64 -> 28x28x32 -> 28x28x1",
    "    ",
    "    Key difference from CIFAR10 autoencoder:",
    "        - Smaller input (28x28 vs 32x32)",
    "        - Fewer downsampling steps (2 vs 3)",
    "        - Separated encoder/decoder for feature extraction",
    "    \"\"\"",
    "    ",
    "    # ========================================================================",
    "    # ENCODER: Extract features from digit images",
    "    # ========================================================================",
    "    encoder_input = layers.Input(shape=(28, 28, 1))",
    "    ",
    "    # Layer 1: Extract low-level features (edges, strokes)",
    "    #   Conv2D: 32 filters learn basic digit patterns",
    "    #   - activation='relu': non-linearity for pattern detection",
    "    #   - padding='same': maintains 28x28 spatial dimensions",
    "    x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(encoder_input)",
    "    ",
    "    # MaxPooling2D: Spatial downsampling",
    "    #   - pool_size=(2,2): reduces dimensions by half",
    "    #   - Result: 28x28 -> 14x14 (first compression)",
    "    x = layers.MaxPooling2D((2, 2), padding='same')(x)  # 28x28 -> 14x14",
    "    ",
    "    # Layer 2: Extract higher-level features (digit shapes, curves)",
    "    #   64 filters: increased capacity for complex patterns",
    "    x = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x)",
    "    ",
    "    # MaxPooling2D: Further spatial downsampling",
    "    #   Result: 14x14 -> 7x7 (second compression)",
    "    x = layers.MaxPooling2D((2, 2), padding='same')(x)  # 14x14 -> 7x7",
    "    ",
    "    # Flatten: Convert spatial features to vector",
    "    #   - Input: 7x7x64 spatial feature maps",
    "    #   - Output: 3136-dimensional vector (7 * 7 * 64 = 3136)",
    "    x = layers.Flatten()(x)",
    "    ",
    "    # Dense: Compress to latent representation",
    "    #   - 3136 -> 64: Final bottleneck",
    "    #   - This 64-dim vector encodes digit identity",
    "    #   - Will be used as input to 7-segment classifier",
    "    latent = layers.Dense(latent_dim, activation='relu', name='latent')(x)",
    "    ",
    "    # Create encoder model for feature extraction",
    "    #   - Separate model allows easy feature extraction",
    "    #   - Input: 28x28x1 image, Output: 64-dim latent vector",
    "    encoder = models.Model(encoder_input, latent, name='encoder')",
    "    ",
    "    # ========================================================================",
    "    # DECODER: Reconstruct images from latent features",
    "    # ========================================================================",
    "    decoder_input = layers.Input(shape=(latent_dim,))",
    "    ",
    "    # Dense: Expand from latent space to spatial representation size",
    "    #   - 64 -> 3136: First expansion",
    "    #   - Prepares for reshaping to 7x7x64",
    "    x = layers.Dense(7 * 7 * 64, activation='relu')(decoder_input)",
    "    ",
    "    # Reshape: Convert 1D vector to 3D spatial format",
    "    #   - 3136 -> 7x7x64: Restore spatial structure",
    "    x = layers.Reshape((7, 7, 64))(x)",
    "    ",
    "    # Conv2DTranspose: Learnable upsampling",
    "    #   - strides=2: doubles spatial dimensions (7x7 -> 14x14)",
    "    #   - 64 filters: maintains feature richness",
    "    x = layers.Conv2DTranspose(64, (3, 3), strides=2, activation='relu', padding='same')(x)  # 7x7 -> 14x14",
    "    ",
    "    # Conv2DTranspose: Second upsampling step",
    "    #   - strides=2: doubles again (14x14 -> 28x28)",
    "    #   - 32 filters: reduces feature count toward output",
    "    x = layers.Conv2DTranspose(32, (3, 3), strides=2, activation='relu', padding='same')(x)  # 14x14 -> 28x28",
    "    ",
    "    # Output layer: Reconstruct single-channel grayscale image",
    "    #   - filters=1: single channel (grayscale)",
    "    #   - activation='sigmoid': outputs [0, 1] range",
    "    #   - padding='same': maintains 28x28 dimensions",
    "    decoder_output = layers.Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)",
    "    ",
    "    # Create decoder model",
    "    decoder = models.Model(decoder_input, decoder_output, name='decoder')",
    "    ",
    "    # Full autoencoder: combines encoder and decoder",
    "    #   - Input: 28x28x1 image",
    "    #   - Output: 28x28x1 reconstructed image",
    "    #   - Training: unsupervised reconstruction task",
    "    autoencoder = models.Model(encoder_input, decoder(encoder(encoder_input)),",
    "                               name='mnist_autoencoder')",
    "    ",
    "    return autoencoder, encoder, decoder",
    "",
    "mnist_ae, mnist_encoder, mnist_decoder = build_mnist_conv_autoencoder(latent_dim=64)",
    "mnist_ae.compile(optimizer='adam', loss='mse', metrics=['mae'])",
    "",
    "print(\"\\nMNIST Convolutional Autoencoder:\")",
    "mnist_ae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train MNIST autoencoder\n",
    "# Goal: Learn compact 64-dim representation of digits\n",
    "# These features will be used for 7-segment classification\n",
    "print(\"Training MNIST Convolutional Autoencoder...\")\n",
    "\n",
    "history_mnist = mnist_ae.fit(\n",
    "    X_train_mnist, X_train_mnist,  # Unsupervised learning\n",
    "    epochs=20,  # Fewer epochs - MNIST is easier than CIFAR10\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    "    validation_data=(X_test_mnist, X_test_mnist),\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nTraining completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize MNIST reconstructions to verify autoencoder quality\n",
    "# Good reconstructions indicate useful learned features\n",
    "n_samples = 10\n",
    "X_mnist_reconstructed = mnist_ae.predict(X_test_mnist[:n_samples])\n",
    "\n",
    "fig, axes = plt.subplots(2, n_samples, figsize=(15, 3))\n",
    "\n",
    "for i in range(n_samples):\n",
    "    # Original digit image\n",
    "    axes[0, i].imshow(X_test_mnist[i].squeeze(), cmap='gray')\n",
    "    axes[0, i].set_title(f'Digit: {y_test_mnist[i]}')\n",
    "    axes[0, i].axis('off')\n",
    "\n",
    "    # Reconstructed image\n",
    "    axes[1, i].imshow(X_mnist_reconstructed[i].squeeze(), cmap='gray')\n",
    "    axes[1, i].set_title('Reconstructed')\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "plt.suptitle('MNIST: Original vs Reconstructed', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create 7-Segment LED Mapping\n",
    "\n",
    "7-segment display layout:\n",
    "```\n",
    "     0\n",
    "   -----\n",
    "  |     |\n",
    "1 |     | 2\n",
    "  |  3  |\n",
    "   -----\n",
    "  |     |\n",
    "4 |     | 5\n",
    "  |     |\n",
    "   -----\n",
    "     6\n",
    "```\n",
    "\n",
    "Segments: [top, top-left, top-right, middle, bottom-left, bottom-right, bottom]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7-segment LED mapping for digits 0-9\n",
    "# Each digit is represented by which segments are ON (1) or OFF (0)\n",
    "# Format: [top, top-left, top-right, middle, bottom-left, bottom-right, bottom]\n",
    "#\n",
    "# Visual reference:\n",
    "#      top(0)\n",
    "#   ---------\n",
    "#   |       |\n",
    "# TL|       |TR\n",
    "# (1)       (2)\n",
    "#   | mid(3)|\n",
    "#   ---------\n",
    "#   |       |\n",
    "# BL|       |BR\n",
    "# (4)       (5)\n",
    "#   | bot(6)|\n",
    "#   ---------\n",
    "seven_segment_map = {\n",
    "    0: [1, 1, 1, 0, 1, 1, 1],  # 0: all except middle\n",
    "    1: [0, 0, 1, 0, 0, 1, 0],  # 1: right side only\n",
    "    2: [1, 0, 1, 1, 1, 0, 1],  # 2: classic \"2\" shape\n",
    "    3: [1, 0, 1, 1, 0, 1, 1],  # 3: classic \"3\" shape\n",
    "    4: [0, 1, 1, 1, 0, 1, 0],  # 4: top-left, middle, right\n",
    "    5: [1, 1, 0, 1, 0, 1, 1],  # 5: classic \"S\" shape\n",
    "    6: [1, 1, 0, 1, 1, 1, 1],  # 6: all except top-right\n",
    "    7: [1, 0, 1, 0, 0, 1, 0],  # 7: top and right side\n",
    "    8: [1, 1, 1, 1, 1, 1, 1],  # 8: all segments on\n",
    "    9: [1, 1, 1, 1, 0, 1, 1],  # 9: all except bottom-left\n",
    "}\n",
    "\n",
    "# Convert digit labels to 7-segment binary representations\n",
    "# This transforms classification problem to multi-label binary classification\n",
    "y_train_7seg = np.array([seven_segment_map[digit] for digit in y_train_mnist])\n",
    "y_test_7seg = np.array([seven_segment_map[digit] for digit in y_test_mnist])\n",
    "\n",
    "print(f\"7-Segment Labels:\")\n",
    "print(f\"Training: {y_train_7seg.shape}\")  # (60000, 7)\n",
    "print(f\"Test: {y_test_7seg.shape}\")        # (10000, 7)\n",
    "\n",
    "# Display mapping for verification\n",
    "print(\"\\n7-Segment Mapping:\")\n",
    "for digit, segments in seven_segment_map.items():\n",
    "    print(f\"Digit {digit}: {segments}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to draw 7-segment LED display\n",
    "# Used for visualizing predicted and ground truth segment patterns\n",
    "def draw_7segment(segments, ax, title):\n",
    "    \"\"\"\n",
    "    Draw a 7-segment LED display representation\n",
    "\n",
    "    Args:\n",
    "        segments: List of 7 binary values (1=ON, 0=OFF)\n",
    "        ax: Matplotlib axis to draw on\n",
    "        title: Title for the plot\n",
    "    \"\"\"\n",
    "    ax.set_xlim(0, 3)\n",
    "    ax.set_ylim(0, 5)\n",
    "    ax.axis('off')\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "    # Define line segments for each LED segment\n",
    "    # [top, top-left, top-right, middle, bottom-left, bottom-right, bottom]\n",
    "    segment_lines = [\n",
    "        [(0.5, 4.5), (2.5, 4.5)],  # top horizontal\n",
    "        [(0.5, 4.5), (0.5, 2.5)],  # top-left vertical\n",
    "        [(2.5, 4.5), (2.5, 2.5)],  # top-right vertical\n",
    "        [(0.5, 2.5), (2.5, 2.5)],  # middle horizontal\n",
    "        [(0.5, 2.5), (0.5, 0.5)],  # bottom-left vertical\n",
    "        [(2.5, 2.5), (2.5, 0.5)],  # bottom-right vertical\n",
    "        [(0.5, 0.5), (2.5, 0.5)],  # bottom horizontal\n",
    "    ]\n",
    "\n",
    "    # Draw each segment based on ON/OFF state\n",
    "    for seg, line in zip(segments, segment_lines):\n",
    "        color = 'black' if seg == 1 else 'lightgray'  # ON = black, OFF = gray\n",
    "        linewidth = 8 if seg == 1 else 2  # Thicker when ON\n",
    "        ax.plot([line[0][0], line[1][0]], [line[0][1], line[1][1]],\n",
    "                color=color, linewidth=linewidth)\n",
    "\n",
    "    ax.set_title(title, fontsize=10)\n",
    "\n",
    "# Visualize 7-segment representation for all 10 digits\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 6))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for digit in range(10):\n",
    "    draw_7segment(seven_segment_map[digit], axes[digit], f'Digit {digit}')\n",
    "\n",
    "plt.suptitle('7-Segment LED Display Representations', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Latent Features and Train MLP Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract latent features from trained autoencoder\n",
    "# These 64-dim features will be input to the 7-segment classifier\n",
    "# Using encoder (not full autoencoder) for feature extraction\n",
    "print(\"Extracting latent features from MNIST autoencoder...\")\n",
    "X_train_latent = mnist_encoder.predict(X_train_mnist)\n",
    "X_test_latent = mnist_encoder.predict(X_test_mnist)\n",
    "\n",
    "print(f\"Latent features shape:\")\n",
    "print(f\"Training: {X_train_latent.shape}\")  # (60000, 64)\n",
    "print(f\"Test: {X_test_latent.shape}\")        # (10000, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build MLP classifier to predict 7-segment LED representation",
    "# Input: Latent features from autoencoder (64-dim)",
    "# Output: 7 binary values (one for each LED segment)",
    "def build_7segment_classifier(input_dim):",
    "    \"\"\"",
    "    Architecture Overview:",
    "        INPUT:  64-dim latent features (from MNIST autoencoder)",
    "        HIDDEN: 64 -> 128 -> 64 (with dropout regularization)",
    "        OUTPUT: 7 binary predictions (one per LED segment)",
    "    ",
    "    Key design choices:",
    "        - Multi-label classification (7 independent outputs)",
    "        - Sigmoid output (each segment probability [0, 1])",
    "        - Dropout for regularization (prevent overfitting)",
    "        - Binary crossentropy loss (suitable for multi-label)",
    "    \"\"\"",
    "    model = models.Sequential([",
    "        layers.Input(shape=(input_dim,)),",
    "        ",
    "        # HIDDEN LAYER 1: Learn complex patterns from latent features",
    "        #   - Input: 64-dim latent features (digit representations)",
    "        #   - Output: 128 neurons",
    "        #   - activation='relu': non-linear transformation",
    "        #   - Purpose: Expand representation to capture digit-to-segment mappings",
    "        layers.Dense(128, activation='relu'),",
    "        ",
    "        # Dropout: Regularization to prevent overfitting",
    "        #   - rate=0.3: randomly drops 30% of neurons during training",
    "        #   - Prevents co-adaptation of neurons",
    "        #   - Improves generalization to test set",
    "        layers.Dropout(0.3),",
    "        ",
    "        # HIDDEN LAYER 2: Further refinement",
    "        #   - Input: 128 neurons",
    "        #   - Output: 64 neurons",
    "        #   - Compresses back to prepare for output layer",
    "        layers.Dense(64, activation='relu'),",
    "        layers.Dropout(0.3),",
    "        ",
    "        # OUTPUT LAYER: 7 independent binary predictions",
    "        #   - Input: 64 neurons",
    "        #   - Output: 7 neurons (one per LED segment)",
    "        #   - activation='sigmoid': each output in [0, 1] range",
    "        #   - Each output represents probability of segment being ON",
    "        #   ",
    "        # Segment mapping:",
    "        #   Output[0] = top segment",
    "        #   Output[1] = top-left segment",
    "        #   Output[2] = top-right segment",
    "        #   Output[3] = middle segment",
    "        #   Output[4] = bottom-left segment",
    "        #   Output[5] = bottom-right segment",
    "        #   Output[6] = bottom segment",
    "        layers.Dense(7, activation='sigmoid')  # 7 outputs for 7 segments",
    "    ], name='7segment_classifier')",
    "    ",
    "    return model",
    "",
    "classifier_7seg = build_7segment_classifier(input_dim=X_train_latent.shape[1])",
    "",
    "# Compile with appropriate loss and metrics",
    "# Binary crossentropy: treats each of 7 outputs as independent binary classification",
    "#   - Loss calculated separately for each segment",
    "#   - Suitable for multi-label classification (segments can be ON/OFF independently)",
    "# Accuracy: measures how often predicted segment matches true segment (across all 7)",
    "classifier_7seg.compile(",
    "    optimizer='adam',",
    "    loss='binary_crossentropy',  # Binary loss for each segment",
    "    metrics=['accuracy']  # Accuracy across all segments",
    ")",
    "",
    "print(\"\\n7-Segment LED Classifier:\")",
    "classifier_7seg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train 7-segment classifier on extracted latent features\n",
    "# Goal: Map digit features to 7 binary segment outputs\n",
    "# This is a novel intermediate representation task\n",
    "print(\"Training 7-Segment LED Classifier...\")\n",
    "\n",
    "history_7seg = classifier_7seg.fit(\n",
    "    X_train_latent, y_train_7seg,  # Input: features, Output: 7 segments\n",
    "    epochs=20,\n",
    "    batch_size=128,\n",
    "    validation_data=(X_test_latent, y_test_7seg),\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nTraining completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history for 7-segment classifier\n",
    "# Monitor convergence and check for overfitting\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "# Binary crossentropy loss (lower is better)\n",
    "axes[0].plot(history_7seg.history['loss'], label='Training Loss')\n",
    "axes[0].plot(history_7seg.history['val_loss'], label='Validation Loss')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Binary Crossentropy Loss')\n",
    "axes[0].set_title('7-Segment Classifier - Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy across all 7 segments (higher is better)\n",
    "axes[1].plot(history_7seg.history['accuracy'], label='Training Accuracy')\n",
    "axes[1].plot(history_7seg.history['val_accuracy'], label='Validation Accuracy')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('7-Segment Classifier - Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Predictions and Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 7-segment predictions for test set\n",
    "# Convert probabilities to binary (threshold at 0.5)\n",
    "y_pred_7seg_prob = classifier_7seg.predict(X_test_latent)\n",
    "y_pred_7seg = (y_pred_7seg_prob > 0.5).astype(int)  # Binarize predictions\n",
    "\n",
    "print(f\"Prediction shape: {y_pred_7seg.shape}\")  # (10000, 7)\n",
    "print(f\"\\nSample predictions (first 5):\")\n",
    "for i in range(5):\n",
    "    print(f\"True digit: {y_test_mnist[i]}, Predicted segments: {y_pred_7seg[i]}, True segments: {y_test_7seg[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 7-segment predictions back to digit labels for evaluation\n",
    "def segments_to_digit(segments):\n",
    "    \"\"\"\n",
    "    Convert 7-segment binary representation back to digit (0-9)\n",
    "\n",
    "    Args:\n",
    "        segments: Array of 7 binary values [top, top-left, top-right, middle,\n",
    "                  bottom-left, bottom-right, bottom]\n",
    "\n",
    "    Returns:\n",
    "        digit (0-9) if pattern matches, -1 if unknown pattern\n",
    "    \"\"\"\n",
    "    segments_tuple = tuple(segments)\n",
    "    for digit, seg_map in seven_segment_map.items():\n",
    "        if tuple(seg_map) == segments_tuple:\n",
    "            return digit\n",
    "    return -1  # Unknown pattern (prediction doesn't match any valid digit)\n",
    "\n",
    "# Convert all predictions back to digit labels\n",
    "y_pred_digits = np.array([segments_to_digit(seg) for seg in y_pred_7seg])\n",
    "\n",
    "# Calculate overall accuracy\n",
    "correct_predictions = np.sum(y_pred_digits == y_test_mnist)\n",
    "total_predictions = len(y_test_mnist)\n",
    "accuracy = correct_predictions / total_predictions\n",
    "\n",
    "print(f\"\\n7-Segment LED Classifier Results:\")\n",
    "print(f\"Correct predictions: {correct_predictions} / {total_predictions}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Unknown patterns (-1): {np.sum(y_pred_digits == -1)}\")\n",
    "print(f\"Note: Unknown patterns occur when predicted segment combination doesn't match any digit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and visualize confusion matrix\n",
    "# Shows how often each digit is classified as each other digit\n",
    "# Diagonal elements = correct classifications\n",
    "# Off-diagonal elements = misclassifications\n",
    "cm = confusion_matrix(y_test_mnist, y_pred_digits)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=list(range(10)) + ['Unknown'],\n",
    "            yticklabels=list(range(10)),\n",
    "            cbar_kws={'label': 'Count'})\n",
    "plt.xlabel('Predicted Digit', fontsize=12)\n",
    "plt.ylabel('True Digit', fontsize=12)\n",
    "plt.title('Confusion Matrix - 7-Segment LED Classifier', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Detailed classification report with precision, recall, F1-score per class\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_mnist, y_pred_digits,\n",
    "                          target_names=[str(i) for i in range(10)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions: Original image \u2192 Predicted 7-seg \u2192 True 7-seg\n",
    "# This shows the complete pipeline: image \u2192 features \u2192 segments\n",
    "n_samples = 10\n",
    "indices = np.random.choice(len(X_test_mnist), n_samples, replace=False)\n",
    "\n",
    "fig, axes = plt.subplots(3, n_samples, figsize=(20, 6))\n",
    "\n",
    "for i, idx in enumerate(indices):\n",
    "    # Row 1: Original MNIST image\n",
    "    axes[0, i].imshow(X_test_mnist[idx].squeeze(), cmap='gray')\n",
    "    axes[0, i].set_title(f'True: {y_test_mnist[idx]}')\n",
    "    axes[0, i].axis('off')\n",
    "\n",
    "    # Row 2: Predicted 7-segment display\n",
    "    draw_7segment(y_pred_7seg[idx], axes[1, i], 'Predicted')\n",
    "\n",
    "    # Row 3: Ground truth 7-segment display\n",
    "    draw_7segment(y_test_7seg[idx], axes[2, i], f'True')\n",
    "\n",
    "# Add row labels\n",
    "axes[0, 0].set_ylabel('Original', fontsize=12)\n",
    "axes[1, 0].set_ylabel('Predicted', fontsize=12)\n",
    "axes[2, 0].set_ylabel('Ground Truth', fontsize=12)\n",
    "\n",
    "plt.suptitle('7-Segment LED Classifier: Predictions vs Ground Truth',\n",
    "             fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary and Conclusions\n",
    "\n",
    "## Task 1: PCA-based Classification\n",
    "- Identified K components retaining 95% variance\n",
    "- Both Standard and Randomized PCA achieved similar performance\n",
    "- Logistic regression on PCA features provided baseline classification\n",
    "\n",
    "## Task 2: Linear Autoencoder vs PCA\n",
    "- Single-layer linear autoencoder with tied weights learns similar features to PCA\n",
    "- Weight vectors capture principal directions in data\n",
    "- Visual comparison shows structural similarity between eigenvectors and learned weights\n",
    "\n",
    "## Task 3: Deep Convolutional Autoencoders\n",
    "- Deep convolutional architecture significantly outperforms shallow autoencoders\n",
    "- Hierarchical feature learning enables better reconstruction\n",
    "- Distributing nodes across multiple layers (3-layer) provides intermediate performance\n",
    "\n",
    "## Task 4: MNIST 7-Segment LED Classifier\n",
    "- Successfully trained autoencoder to extract MNIST features\n",
    "- MLP classifier learned to map latent features to 7-segment representations\n",
    "- Confusion matrix reveals digit-specific performance patterns\n",
    "- Novel task demonstrating practical application of learned representations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}